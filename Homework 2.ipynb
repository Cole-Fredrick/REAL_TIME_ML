{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc8ccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28df34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "#########Problem 1#########\n",
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39934aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>semi-furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
       "0  13300000  7420         4          2        3      yes        no       no   \n",
       "1  12250000  8960         4          4        4      yes        no       no   \n",
       "2  12250000  9960         3          2        2      yes        no      yes   \n",
       "3  12215000  7500         4          2        2      yes        no      yes   \n",
       "4  11410000  7420         4          1        2      yes       yes      yes   \n",
       "\n",
       "  hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
       "0              no             yes        2      yes        furnished  \n",
       "1              no             yes        3       no        furnished  \n",
       "2              no              no        2      yes   semi-furnished  \n",
       "3              no             yes        3      yes        furnished  \n",
       "4              no             yes        2       no        furnished  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "housing = pd.DataFrame(pd.read_csv(\"Housing.csv\"))\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8932bf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>parking</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>12250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11410000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   area  bedrooms  bathrooms  stories  parking     price\n",
       "0  7420         4          2        3        2  13300000\n",
       "1  8960         4          4        4        3  12250000\n",
       "2  9960         3          2        2        2  12250000\n",
       "3  7500         4          2        2        3  12215000\n",
       "4  7420         4          1        2        2  11410000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price'] \n",
    "Newtrain = housing[num_vars] \n",
    "Newtrain.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f2a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Price Predicition\n",
    "t_area = torch.tensor(Newtrain['area']) \n",
    "t_bedrooms = torch.tensor(Newtrain['bedrooms'])\n",
    "t_bathrooms = torch.tensor(Newtrain['bathrooms'])\n",
    "t_stories = torch.tensor(Newtrain['stories'])\n",
    "t_parking = torch.tensor(Newtrain['parking'])\n",
    "t_price = torch.tensor(Newtrain['price'])\n",
    "\n",
    "#Normalized #Split Post Normalized Values\n",
    "t_areaN = t_area / max(Newtrain['area'])\n",
    "t_bedroomsN = t_bedrooms / max(Newtrain['bedrooms'])\n",
    "t_bathroomsN = t_bathrooms / max(Newtrain['bathrooms'])\n",
    "t_storiesN = t_stories / max(Newtrain['stories'])\n",
    "t_parkingN = t_parking / max(Newtrain['parking'])\n",
    "t_priceN = t_price / max(Newtrain['price'])\n",
    "\n",
    "#Split ^\n",
    "#Mask Creation 80/20 Split\n",
    "mask = np.random.rand(len(t_areaN)) <= 0.8\n",
    "\n",
    "#AreaSets\n",
    "t_area_train = t_areaN[mask]\n",
    "t_area_valid = t_areaN[~mask]\n",
    "\n",
    "#BedroomSets\n",
    "t_bedrooms_train = t_bedroomsN[mask]\n",
    "t_bedrooms_valid = t_bedroomsN[~mask]\n",
    "\n",
    "#BathroomSets\n",
    "t_bathrooms_train = t_bathroomsN[mask]\n",
    "t_bathrooms_valid = t_bathroomsN[~mask]\n",
    "\n",
    "#StoriesSets\n",
    "t_stories_train = t_storiesN[mask]\n",
    "t_stories_valid = t_storiesN[~mask]\n",
    "\n",
    "#ParkingSets\n",
    "t_parking_train = t_parkingN[mask]\n",
    "t_parking_valid = t_parkingN[~mask]\n",
    "\n",
    "#PriceSets\n",
    "t_price_train = t_priceN[mask]\n",
    "t_price_valid = t_priceN[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23636492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bringing the normalized features back together and making it a tensor.\n",
    "training_features = [t_area_train, t_bedrooms_train, t_bathrooms_train, t_stories_train, t_parking_train]\n",
    "training_features = torch.stack(training_features)\n",
    "training_features = np.transpose(training_features)\n",
    "\n",
    "validation_features = [t_area_valid, t_bedrooms_valid, t_bathrooms_valid, t_stories_valid, t_parking_valid]\n",
    "validation_features = torch.stack(validation_features)\n",
    "validation_features = np.transpose(validation_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cef5a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4580, 0.6667, 0.5000, 0.7500, 0.6667],\n",
       "        [0.5531, 0.6667, 1.0000, 1.0000, 1.0000],\n",
       "        [0.6148, 0.5000, 0.5000, 0.5000, 0.6667],\n",
       "        ...,\n",
       "        [0.1481, 0.5000, 0.2500, 0.2500, 0.0000],\n",
       "        [0.1796, 0.5000, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2377, 0.5000, 0.2500, 0.5000, 0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03dc46d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.8333, 0.7500, 0.5000, 0.0000],\n",
       "        [0.8148, 0.5000, 0.2500, 0.5000, 0.6667],\n",
       "        [0.3704, 0.6667, 0.7500, 0.5000, 0.6667],\n",
       "        [0.5247, 0.5000, 0.5000, 1.0000, 0.6667],\n",
       "        [0.3704, 0.5000, 0.5000, 1.0000, 0.0000],\n",
       "        [0.3679, 0.5000, 0.7500, 0.5000, 0.3333],\n",
       "        [0.5556, 0.6667, 0.5000, 1.0000, 0.6667],\n",
       "        [0.3704, 0.6667, 0.5000, 1.0000, 0.3333],\n",
       "        [0.5556, 0.6667, 0.5000, 1.0000, 0.3333],\n",
       "        [0.3704, 0.5000, 0.5000, 0.5000, 0.3333],\n",
       "        [0.3852, 0.6667, 0.5000, 0.5000, 0.3333],\n",
       "        [0.5481, 0.5000, 0.5000, 0.5000, 0.3333],\n",
       "        [0.7463, 0.6667, 0.5000, 0.5000, 0.6667],\n",
       "        [0.3704, 0.6667, 0.5000, 1.0000, 0.0000],\n",
       "        [0.3099, 0.5000, 0.2500, 1.0000, 0.0000],\n",
       "        [0.4012, 0.5000, 0.5000, 0.7500, 0.0000],\n",
       "        [0.2444, 0.5000, 0.2500, 0.2500, 0.6667],\n",
       "        [0.5296, 0.8333, 0.7500, 0.5000, 0.6667],\n",
       "        [0.3704, 0.6667, 0.5000, 1.0000, 0.3333],\n",
       "        [0.5168, 0.5000, 0.2500, 0.7500, 0.6667],\n",
       "        [0.2654, 1.0000, 0.5000, 0.5000, 0.0000],\n",
       "        [0.4028, 0.5000, 0.5000, 1.0000, 0.3333],\n",
       "        [0.4012, 0.5000, 0.5000, 0.7500, 0.0000],\n",
       "        [0.2864, 0.6667, 0.2500, 0.5000, 0.3333],\n",
       "        [0.3086, 0.5000, 0.2500, 0.7500, 0.0000],\n",
       "        [0.3580, 0.5000, 0.5000, 1.0000, 0.0000],\n",
       "        [0.2901, 0.6667, 0.2500, 0.5000, 0.3333],\n",
       "        [0.6481, 0.3333, 0.2500, 0.2500, 0.3333],\n",
       "        [0.3765, 0.5000, 0.5000, 0.2500, 0.6667],\n",
       "        [0.4259, 0.5000, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2840, 0.3333, 0.5000, 0.2500, 0.6667],\n",
       "        [0.3395, 0.5000, 0.5000, 0.2500, 0.0000],\n",
       "        [0.2346, 0.5000, 0.2500, 0.5000, 0.3333],\n",
       "        [0.5259, 0.5000, 0.2500, 0.2500, 0.6667],\n",
       "        [0.3735, 0.5000, 0.2500, 0.2500, 0.0000],\n",
       "        [0.4373, 0.5000, 0.2500, 0.2500, 0.6667],\n",
       "        [0.1852, 0.5000, 0.5000, 0.5000, 0.0000],\n",
       "        [0.1852, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.6605, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.5031, 0.5000, 0.5000, 0.2500, 0.0000],\n",
       "        [0.2790, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.2528, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.2111, 0.6667, 0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.2500, 0.5000, 0.0000],\n",
       "        [0.1772, 0.3333, 0.2500, 0.5000, 0.0000],\n",
       "        [0.3093, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.2370, 0.5000, 0.2500, 0.5000, 0.3333],\n",
       "        [0.3284, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.5185, 0.6667, 0.2500, 1.0000, 1.0000],\n",
       "        [0.6086, 0.5000, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2449, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.4796, 0.3333, 0.2500, 0.2500, 0.3333],\n",
       "        [0.3395, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.3111, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.1611, 0.6667, 0.7500, 0.5000, 0.0000],\n",
       "        [0.1435, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.2173, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.1324, 0.5000, 0.2500, 0.7500, 0.3333],\n",
       "        [0.2130, 0.5000, 0.2500, 0.5000, 0.3333],\n",
       "        [0.2498, 0.5000, 0.2500, 0.5000, 0.3333],\n",
       "        [0.2859, 0.6667, 0.2500, 0.5000, 0.0000],\n",
       "        [0.2667, 0.5000, 0.2500, 0.5000, 0.6667],\n",
       "        [0.2031, 0.3333, 0.2500, 0.2500, 0.3333],\n",
       "        [0.4988, 0.5000, 0.2500, 0.2500, 0.6667],\n",
       "        [0.2222, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2494, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2500, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2241, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2642, 0.3333, 0.2500, 0.2500, 0.6667],\n",
       "        [0.1759, 0.5000, 0.5000, 0.5000, 0.0000],\n",
       "        [0.2173, 0.5000, 0.2500, 0.2500, 0.6667],\n",
       "        [0.1944, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.2253, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.3642, 0.3333, 0.2500, 0.2500, 0.3333],\n",
       "        [0.4537, 0.3333, 0.2500, 0.2500, 0.3333],\n",
       "        [0.2168, 0.3333, 0.2500, 0.2500, 0.3333],\n",
       "        [0.3630, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2469, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.1966, 0.3333, 0.2500, 0.2500, 0.6667],\n",
       "        [0.2377, 0.5000, 0.2500, 0.2500, 0.0000],\n",
       "        [0.1611, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.3062, 0.6667, 0.2500, 0.7500, 0.0000],\n",
       "        [0.1966, 0.3333, 0.2500, 0.2500, 0.6667],\n",
       "        [0.3741, 0.5000, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2341, 0.6667, 0.2500, 0.5000, 0.0000],\n",
       "        [0.2160, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2528, 0.3333, 0.2500, 0.2500, 0.6667],\n",
       "        [0.1019, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.2778, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.2160, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.4969, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2686, 0.6667, 0.2500, 0.5000, 0.3333],\n",
       "        [0.3704, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.3210, 0.6667, 0.2500, 0.7500, 0.0000],\n",
       "        [0.2444, 0.5000, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2428, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.1966, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.1796, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2222, 0.3333, 0.5000, 0.5000, 0.3333],\n",
       "        [0.2716, 0.5000, 0.2500, 0.5000, 0.0000],\n",
       "        [0.4753, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2244, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.1852, 0.3333, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2074, 0.3333, 0.2500, 0.2500, 0.3333],\n",
       "        [0.2235, 0.3333, 0.2500, 0.2500, 0.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae531257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=8, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=8, out_features=16, bias=True)\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequential Model Setup \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "hw2_seq_model = nn.Sequential(\n",
    "                nn.Linear(5,8), # (X,Y) X = Input Features Y = Output Features\n",
    "                nn.Tanh(),  # Neuron Activiation Function\n",
    "                nn.Linear(8,16), # Hidden Layer \n",
    "                nn.Tanh(),\n",
    "                nn.Linear(16,1)) # End Layer (Y,N) # Output Features as Input, N - Price Output\n",
    "hw2_seq_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b87fdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer Initialization\n",
    "OPTIM = optim.SGD(hw2_seq_model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d33f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop Function Declaration\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p_train = model(t_u_train)\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "        \n",
    "        t_p_val = model(t_u_val)\n",
    "        loss_val = loss_fn(t_p_val, t_c_val)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train.item():0.4f},\"f\" Validation loss {loss_val.item():.4f}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c924978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.1987, Validation loss 0.1911\n",
      "Epoch 100, Training loss 0.0762, Validation loss 0.0711\n",
      "Epoch 200, Training loss 0.0383, Validation loss 0.0344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colef\\.conda\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([440])) that is different to the input size (torch.Size([440, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\colef\\.conda\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([105])) that is different to the input size (torch.Size([105, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "#Training Loop Intialization\n",
    "training_loop(\n",
    "    n_epochs = 200,\n",
    "    optimizer = OPTIM,\n",
    "    model = hw2_seq_model,\n",
    "    loss_fn = nn.MSELoss(),\n",
    "    t_u_train = training_features,\n",
    "    t_u_val = validation_features,\n",
    "    t_c_train = t_price_train,\n",
    "    t_c_val = t_price_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91fc1dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Tanh is treated as a zero-op.\n",
      "Sequential(\n",
      "  0.0 M, 100.000% Params, 0.0 GMac, 100.000% MACs, \n",
      "  (0): Linear(0.0 M, 22.967% Params, 0.0 GMac, 22.967% MACs, in_features=5, out_features=8, bias=True)\n",
      "  (1): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (2): Linear(0.0 M, 68.900% Params, 0.0 GMac, 68.900% MACs, in_features=8, out_features=16, bias=True)\n",
      "  (3): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (4): Linear(0.0 M, 8.134% Params, 0.0 GMac, 8.134% MACs, in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           209     \n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(0):\n",
    "  net = hw2_seq_model\n",
    "  macs, params = get_model_complexity_info(net, (1, 5), as_strings=True, print_per_layer_stat=True, \n",
    "verbose=True)\n",
    "  print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "  print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43d892cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "#########Problem 1 Part 2#########\n",
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "140fc0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=8, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=8, out_features=16, bias=True)\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (5): Tanh()\n",
       "  (6): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (7): Tanh()\n",
       "  (8): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequential Model Setup \n",
    "hw2_seq_model2 = nn.Sequential(\n",
    "                nn.Linear(5,8), # (X,Y) X = Input Features Y = Output Features\n",
    "                nn.Tanh(),  # Neuron Activiation Function\n",
    "                nn.Linear(8,16), # Hidden Layer \n",
    "                nn.Tanh(),\n",
    "                nn.Linear(16, 32),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(32,64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64,1)) # End Layer (Y,N) # Output Features as Input, N - Price Output\n",
    "hw2_seq_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8adfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer Initialization\n",
    "OPTIM2 = optim.SGD(hw2_seq_model2.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4355bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop Function Declaration\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p_train = model(t_u_train)\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "        \n",
    "        t_p_val = model(t_u_val)\n",
    "        loss_val = loss_fn(t_p_val, t_c_val)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train.item():0.4f},\"f\" Validation loss {loss_val.item():.4f}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b0ad646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.2281, Validation loss 0.2189\n",
      "Epoch 100, Training loss 0.0593, Validation loss 0.0540\n",
      "Epoch 200, Training loss 0.0271, Validation loss 0.0233\n"
     ]
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 200,\n",
    "    optimizer = OPTIM2,\n",
    "    model = hw2_seq_model2,\n",
    "    loss_fn = nn.MSELoss(),\n",
    "    t_u_train = training_features,\n",
    "    t_u_val = validation_features,\n",
    "    t_c_train = t_price_train,\n",
    "    t_c_val = t_price_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67948273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Tanh is treated as a zero-op.\n",
      "Sequential(\n",
      "  0.003 M, 100.000% Params, 0.0 GMac, 100.000% MACs, \n",
      "  (0): Linear(0.0 M, 1.648% Params, 0.0 GMac, 1.648% MACs, in_features=5, out_features=8, bias=True)\n",
      "  (1): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (2): Linear(0.0 M, 4.943% Params, 0.0 GMac, 4.943% MACs, in_features=8, out_features=16, bias=True)\n",
      "  (3): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (4): Linear(0.001 M, 18.675% Params, 0.0 GMac, 18.675% MACs, in_features=16, out_features=32, bias=True)\n",
      "  (5): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (6): Linear(0.002 M, 72.503% Params, 0.0 GMac, 72.503% MACs, in_features=32, out_features=64, bias=True)\n",
      "  (7): Tanh(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "  (8): Linear(0.0 M, 2.231% Params, 0.0 GMac, 2.231% MACs, in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           2.91 k  \n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(0):\n",
    "  net = hw2_seq_model2\n",
    "  macs, params = get_model_complexity_info(net, (1, 5), as_strings=True, print_per_layer_stat=True, \n",
    "verbose=True)\n",
    "  print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "  print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eb87582",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "#########Problem 2 Part 1#########\n",
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "699a393a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=(0.4915, 0.4823, 0.4468), std=(0.247, 0.2435, 0.2616))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47dfa044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a129d3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "data_path = 'C:/Users/colef/Documents/MachineLearningCIFAR10'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87fa2900",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_model = nn.Sequential(\n",
    "              nn.Linear(3072,512),\n",
    "              nn.Tanh(),\n",
    "              nn.Linear(512,10),\n",
    "              nn.LogSoftmax(dim=1))\n",
    "CIFAR_model.to(device)\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d84b7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(CIFAR_model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a538980",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "n_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "992112ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.614596\n",
      "Epoch: 1, Loss: 2.117877\n",
      "Epoch: 2, Loss: 1.805727\n",
      "Epoch: 3, Loss: 1.566769\n",
      "Epoch: 4, Loss: 1.955997\n",
      "Epoch: 5, Loss: 1.614391\n",
      "Epoch: 6, Loss: 1.915290\n",
      "Epoch: 7, Loss: 1.498046\n",
      "Epoch: 8, Loss: 2.015746\n",
      "Epoch: 9, Loss: 1.725687\n",
      "Epoch: 10, Loss: 2.360999\n",
      "Epoch: 11, Loss: 2.358185\n",
      "Epoch: 12, Loss: 1.637207\n",
      "Epoch: 13, Loss: 2.048340\n",
      "Epoch: 14, Loss: 1.133327\n",
      "Epoch: 15, Loss: 1.956957\n",
      "Epoch: 16, Loss: 1.942017\n",
      "Epoch: 17, Loss: 1.783881\n",
      "Epoch: 18, Loss: 1.290211\n",
      "Epoch: 19, Loss: 1.652921\n",
      "Epoch: 20, Loss: 1.640243\n",
      "Epoch: 21, Loss: 1.627741\n",
      "Epoch: 22, Loss: 1.876759\n",
      "Epoch: 23, Loss: 1.864081\n",
      "Epoch: 24, Loss: 1.928416\n",
      "Epoch: 25, Loss: 1.620163\n",
      "Epoch: 26, Loss: 1.380888\n",
      "Epoch: 27, Loss: 1.782860\n",
      "Epoch: 28, Loss: 1.259953\n",
      "Epoch: 29, Loss: 1.362238\n",
      "Epoch: 30, Loss: 1.502514\n",
      "Epoch: 31, Loss: 2.192045\n",
      "Epoch: 32, Loss: 1.560259\n",
      "Epoch: 33, Loss: 1.853304\n",
      "Epoch: 34, Loss: 1.586539\n",
      "Epoch: 35, Loss: 1.254144\n",
      "Epoch: 36, Loss: 1.741871\n",
      "Epoch: 37, Loss: 1.325702\n",
      "Epoch: 38, Loss: 1.738043\n",
      "Epoch: 39, Loss: 1.266086\n",
      "Epoch: 40, Loss: 1.645734\n",
      "Epoch: 41, Loss: 1.450358\n",
      "Epoch: 42, Loss: 1.307877\n",
      "Epoch: 43, Loss: 1.617405\n",
      "Epoch: 44, Loss: 1.664931\n",
      "Epoch: 45, Loss: 1.406992\n",
      "Epoch: 46, Loss: 1.305362\n",
      "Epoch: 47, Loss: 1.443697\n",
      "Epoch: 48, Loss: 1.039608\n",
      "Epoch: 49, Loss: 1.501491\n",
      "Epoch: 50, Loss: 1.890990\n",
      "Epoch: 51, Loss: 1.287718\n",
      "Epoch: 52, Loss: 1.663718\n",
      "Epoch: 53, Loss: 1.395910\n",
      "Epoch: 54, Loss: 1.588787\n",
      "Epoch: 55, Loss: 1.881870\n",
      "Epoch: 56, Loss: 1.341839\n",
      "Epoch: 57, Loss: 1.434057\n",
      "Epoch: 58, Loss: 1.888163\n",
      "Epoch: 59, Loss: 1.415896\n",
      "Epoch: 60, Loss: 1.028937\n",
      "Epoch: 61, Loss: 1.548436\n",
      "Epoch: 62, Loss: 1.652131\n",
      "Epoch: 63, Loss: 1.180719\n",
      "Epoch: 64, Loss: 1.666666\n",
      "Epoch: 65, Loss: 0.836803\n",
      "Epoch: 66, Loss: 1.664413\n",
      "Epoch: 67, Loss: 1.363144\n",
      "Epoch: 68, Loss: 1.626243\n",
      "Epoch: 69, Loss: 1.349079\n",
      "Epoch: 70, Loss: 1.498026\n",
      "Epoch: 71, Loss: 2.082631\n",
      "Epoch: 72, Loss: 1.419869\n",
      "Epoch: 73, Loss: 1.705236\n",
      "Epoch: 74, Loss: 1.099649\n",
      "Epoch: 75, Loss: 1.471673\n",
      "Epoch: 76, Loss: 1.371526\n",
      "Epoch: 77, Loss: 1.033352\n",
      "Epoch: 78, Loss: 1.671112\n",
      "Epoch: 79, Loss: 1.414946\n",
      "Epoch: 80, Loss: 1.263606\n",
      "Epoch: 81, Loss: 1.180766\n",
      "Epoch: 82, Loss: 1.192403\n",
      "Epoch: 83, Loss: 1.364001\n",
      "Epoch: 84, Loss: 0.994005\n",
      "Epoch: 85, Loss: 1.177496\n",
      "Epoch: 86, Loss: 1.248232\n",
      "Epoch: 87, Loss: 1.325188\n",
      "Epoch: 88, Loss: 1.345271\n",
      "Epoch: 89, Loss: 2.107562\n",
      "Epoch: 90, Loss: 1.020965\n",
      "Epoch: 91, Loss: 1.091975\n",
      "Epoch: 92, Loss: 1.050703\n",
      "Epoch: 93, Loss: 1.239935\n",
      "Epoch: 94, Loss: 1.611100\n",
      "Epoch: 95, Loss: 1.356407\n",
      "Epoch: 96, Loss: 1.000598\n",
      "Epoch: 97, Loss: 1.227593\n",
      "Epoch: 98, Loss: 1.488547\n",
      "Epoch: 99, Loss: 1.265643\n",
      "Epoch: 100, Loss: 1.127580\n",
      "Epoch: 101, Loss: 1.409868\n",
      "Epoch: 102, Loss: 0.971009\n",
      "Epoch: 103, Loss: 1.015985\n",
      "Epoch: 104, Loss: 0.927519\n",
      "Epoch: 105, Loss: 1.434499\n",
      "Epoch: 106, Loss: 1.666801\n",
      "Epoch: 107, Loss: 1.112867\n",
      "Epoch: 108, Loss: 1.284398\n",
      "Epoch: 109, Loss: 1.494799\n",
      "Epoch: 110, Loss: 1.124750\n",
      "Epoch: 111, Loss: 1.006911\n",
      "Epoch: 112, Loss: 0.816376\n",
      "Epoch: 113, Loss: 0.866056\n",
      "Epoch: 114, Loss: 1.167405\n",
      "Epoch: 115, Loss: 1.439982\n",
      "Epoch: 116, Loss: 0.762444\n",
      "Epoch: 117, Loss: 1.384260\n",
      "Epoch: 118, Loss: 0.884988\n",
      "Epoch: 119, Loss: 1.378784\n",
      "Epoch: 120, Loss: 1.417576\n",
      "Epoch: 121, Loss: 1.414192\n",
      "Epoch: 122, Loss: 1.097863\n",
      "Epoch: 123, Loss: 1.638709\n",
      "Epoch: 124, Loss: 0.680431\n",
      "Epoch: 125, Loss: 0.957592\n",
      "Epoch: 126, Loss: 1.856713\n",
      "Epoch: 127, Loss: 0.870527\n",
      "Epoch: 128, Loss: 0.981983\n",
      "Epoch: 129, Loss: 1.118630\n",
      "Epoch: 130, Loss: 1.358169\n",
      "Epoch: 131, Loss: 0.647787\n",
      "Epoch: 132, Loss: 0.779308\n",
      "Epoch: 133, Loss: 1.264042\n",
      "Epoch: 134, Loss: 1.294460\n",
      "Epoch: 135, Loss: 1.051437\n",
      "Epoch: 136, Loss: 1.083959\n",
      "Epoch: 137, Loss: 1.074937\n",
      "Epoch: 138, Loss: 0.920297\n",
      "Epoch: 139, Loss: 1.390181\n",
      "Epoch: 140, Loss: 0.887459\n",
      "Epoch: 141, Loss: 1.082510\n",
      "Epoch: 142, Loss: 0.925836\n",
      "Epoch: 143, Loss: 1.154851\n",
      "Epoch: 144, Loss: 0.906680\n",
      "Epoch: 145, Loss: 0.921653\n",
      "Epoch: 146, Loss: 0.813153\n",
      "Epoch: 147, Loss: 0.787924\n",
      "Epoch: 148, Loss: 0.685961\n",
      "Epoch: 149, Loss: 1.331076\n",
      "Epoch: 150, Loss: 1.002737\n",
      "Epoch: 151, Loss: 0.627568\n",
      "Epoch: 152, Loss: 1.277034\n",
      "Epoch: 153, Loss: 1.420953\n",
      "Epoch: 154, Loss: 0.995455\n",
      "Epoch: 155, Loss: 1.348443\n",
      "Epoch: 156, Loss: 0.895549\n",
      "Epoch: 157, Loss: 1.656294\n",
      "Epoch: 158, Loss: 1.115876\n",
      "Epoch: 159, Loss: 1.059489\n",
      "Epoch: 160, Loss: 1.032500\n",
      "Epoch: 161, Loss: 1.085383\n",
      "Epoch: 162, Loss: 1.141419\n",
      "Epoch: 163, Loss: 1.004444\n",
      "Epoch: 164, Loss: 0.904090\n",
      "Epoch: 165, Loss: 1.022142\n",
      "Epoch: 166, Loss: 0.791546\n",
      "Epoch: 167, Loss: 0.645467\n",
      "Epoch: 168, Loss: 0.723727\n",
      "Epoch: 169, Loss: 1.207871\n",
      "Epoch: 170, Loss: 0.689134\n",
      "Epoch: 171, Loss: 1.078483\n",
      "Epoch: 172, Loss: 0.718062\n",
      "Epoch: 173, Loss: 1.206998\n",
      "Epoch: 174, Loss: 1.200880\n",
      "Epoch: 175, Loss: 0.880216\n",
      "Epoch: 176, Loss: 0.793720\n",
      "Epoch: 177, Loss: 1.220284\n",
      "Epoch: 178, Loss: 0.599784\n",
      "Epoch: 179, Loss: 0.692532\n",
      "Epoch: 180, Loss: 0.885158\n",
      "Epoch: 181, Loss: 0.967249\n",
      "Epoch: 182, Loss: 0.973406\n",
      "Epoch: 183, Loss: 0.907066\n",
      "Epoch: 184, Loss: 0.937013\n",
      "Epoch: 185, Loss: 0.800431\n",
      "Epoch: 186, Loss: 0.788980\n",
      "Epoch: 187, Loss: 0.700120\n",
      "Epoch: 188, Loss: 0.772914\n",
      "Epoch: 189, Loss: 0.917843\n",
      "Epoch: 190, Loss: 1.106165\n",
      "Epoch: 191, Loss: 1.017878\n",
      "Epoch: 192, Loss: 0.967599\n",
      "Epoch: 193, Loss: 0.895059\n",
      "Epoch: 194, Loss: 0.967395\n",
      "Epoch: 195, Loss: 0.897588\n",
      "Epoch: 196, Loss: 0.776086\n",
      "Epoch: 197, Loss: 0.596955\n",
      "Epoch: 198, Loss: 0.719619\n",
      "Epoch: 199, Loss: 0.659360\n",
      "Epoch: 200, Loss: 0.902159\n",
      "Epoch: 201, Loss: 0.892285\n",
      "Epoch: 202, Loss: 0.902582\n",
      "Epoch: 203, Loss: 0.837783\n",
      "Epoch: 204, Loss: 0.759985\n",
      "Epoch: 205, Loss: 0.846098\n",
      "Epoch: 206, Loss: 0.677953\n",
      "Epoch: 207, Loss: 0.977099\n",
      "Epoch: 208, Loss: 0.994848\n",
      "Epoch: 209, Loss: 0.732483\n",
      "Epoch: 210, Loss: 0.829982\n",
      "Epoch: 211, Loss: 0.728455\n",
      "Epoch: 212, Loss: 0.526481\n",
      "Epoch: 213, Loss: 0.893693\n",
      "Epoch: 214, Loss: 0.528558\n",
      "Epoch: 215, Loss: 0.595540\n",
      "Epoch: 216, Loss: 0.488169\n",
      "Epoch: 217, Loss: 0.827972\n",
      "Epoch: 218, Loss: 0.757691\n",
      "Epoch: 219, Loss: 0.595877\n",
      "Epoch: 220, Loss: 0.785887\n",
      "Epoch: 221, Loss: 0.729980\n",
      "Epoch: 222, Loss: 0.462501\n",
      "Epoch: 223, Loss: 0.700010\n",
      "Epoch: 224, Loss: 0.883743\n",
      "Epoch: 225, Loss: 0.717134\n",
      "Epoch: 226, Loss: 0.729269\n",
      "Epoch: 227, Loss: 0.842005\n",
      "Epoch: 228, Loss: 0.791650\n",
      "Epoch: 229, Loss: 0.913690\n",
      "Epoch: 230, Loss: 0.803066\n",
      "Epoch: 231, Loss: 0.669634\n",
      "Epoch: 232, Loss: 0.673033\n",
      "Epoch: 233, Loss: 0.854398\n",
      "Epoch: 234, Loss: 0.618033\n",
      "Epoch: 235, Loss: 0.770446\n",
      "Epoch: 236, Loss: 0.827756\n",
      "Epoch: 237, Loss: 0.813128\n",
      "Epoch: 238, Loss: 0.612715\n",
      "Epoch: 239, Loss: 0.863527\n",
      "Epoch: 240, Loss: 0.472747\n",
      "Epoch: 241, Loss: 0.974322\n",
      "Epoch: 242, Loss: 0.886075\n",
      "Epoch: 243, Loss: 0.825966\n",
      "Epoch: 244, Loss: 0.695138\n",
      "Epoch: 245, Loss: 0.625007\n",
      "Epoch: 246, Loss: 0.525264\n",
      "Epoch: 247, Loss: 0.516474\n",
      "Epoch: 248, Loss: 0.342867\n",
      "Epoch: 249, Loss: 0.707740\n",
      "Epoch: 250, Loss: 0.859957\n",
      "Epoch: 251, Loss: 0.795091\n",
      "Epoch: 252, Loss: 0.581176\n",
      "Epoch: 253, Loss: 0.542437\n",
      "Epoch: 254, Loss: 0.550262\n",
      "Epoch: 255, Loss: 0.472947\n",
      "Epoch: 256, Loss: 0.753305\n",
      "Epoch: 257, Loss: 0.566343\n",
      "Epoch: 258, Loss: 0.510378\n",
      "Epoch: 259, Loss: 0.566169\n",
      "Epoch: 260, Loss: 0.813402\n",
      "Epoch: 261, Loss: 0.628387\n",
      "Epoch: 262, Loss: 0.310889\n",
      "Epoch: 263, Loss: 0.442451\n",
      "Epoch: 264, Loss: 0.726260\n",
      "Epoch: 265, Loss: 0.793642\n",
      "Epoch: 266, Loss: 0.782562\n",
      "Epoch: 267, Loss: 0.382412\n",
      "Epoch: 268, Loss: 0.575460\n",
      "Epoch: 269, Loss: 0.627783\n",
      "Epoch: 270, Loss: 0.391055\n",
      "Epoch: 271, Loss: 0.839618\n",
      "Epoch: 272, Loss: 0.743348\n",
      "Epoch: 273, Loss: 0.750401\n",
      "Epoch: 274, Loss: 0.708776\n",
      "Epoch: 275, Loss: 0.439158\n",
      "Epoch: 276, Loss: 0.688931\n",
      "Epoch: 277, Loss: 0.296734\n",
      "Epoch: 278, Loss: 0.657197\n",
      "Epoch: 279, Loss: 0.521093\n",
      "Epoch: 280, Loss: 0.590492\n",
      "Epoch: 281, Loss: 0.475949\n",
      "Epoch: 282, Loss: 0.389321\n",
      "Epoch: 283, Loss: 0.310151\n",
      "Epoch: 284, Loss: 0.704203\n",
      "Epoch: 285, Loss: 0.569361\n",
      "Epoch: 286, Loss: 0.543905\n",
      "Epoch: 287, Loss: 0.524311\n",
      "Epoch: 288, Loss: 0.450766\n",
      "Epoch: 289, Loss: 0.256111\n",
      "Epoch: 290, Loss: 0.386260\n",
      "Epoch: 291, Loss: 0.528591\n",
      "Epoch: 292, Loss: 0.982011\n",
      "Epoch: 293, Loss: 0.355865\n",
      "Epoch: 294, Loss: 0.422137\n",
      "Epoch: 295, Loss: 0.529941\n",
      "Epoch: 296, Loss: 0.525961\n",
      "Epoch: 297, Loss: 0.400360\n",
      "Epoch: 298, Loss: 0.386926\n",
      "Epoch: 299, Loss: 0.403210\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = CIFAR_model(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "605f72a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: %f 0.4855\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = CIFAR_model(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "print(\"Accuracy: %f\", (correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b471c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "#########Problem 2 Part 2#########\n",
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faad2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_model2 = nn.Sequential(\n",
    "              nn.Linear(3072,512),\n",
    "              nn.Tanh(),\n",
    "              nn.Linear(512,256),\n",
    "              nn.Tanh(),\n",
    "              nn.Linear(256,128),\n",
    "              nn.Tanh(),\n",
    "              nn.Linear(128,10),\n",
    "              nn.LogSoftmax(dim=1))\n",
    "CIFAR_model2.to(device)\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3b2033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(CIFAR_model2.parameters(),lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce3c0be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "n_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d08e0c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.169474\n",
      "Epoch: 1, Loss: 1.832735\n",
      "Epoch: 2, Loss: 1.932111\n",
      "Epoch: 3, Loss: 1.646647\n",
      "Epoch: 4, Loss: 2.064003\n",
      "Epoch: 5, Loss: 1.475062\n",
      "Epoch: 6, Loss: 1.326771\n",
      "Epoch: 7, Loss: 1.258244\n",
      "Epoch: 8, Loss: 1.374021\n",
      "Epoch: 9, Loss: 1.543547\n",
      "Epoch: 10, Loss: 1.605304\n",
      "Epoch: 11, Loss: 1.248654\n",
      "Epoch: 12, Loss: 1.517236\n",
      "Epoch: 13, Loss: 1.652164\n",
      "Epoch: 14, Loss: 1.247386\n",
      "Epoch: 15, Loss: 0.845520\n",
      "Epoch: 16, Loss: 0.879568\n",
      "Epoch: 17, Loss: 1.217232\n",
      "Epoch: 18, Loss: 1.432421\n",
      "Epoch: 19, Loss: 1.257779\n",
      "Epoch: 20, Loss: 0.790998\n",
      "Epoch: 21, Loss: 0.764496\n",
      "Epoch: 22, Loss: 0.950711\n",
      "Epoch: 23, Loss: 0.819069\n",
      "Epoch: 24, Loss: 1.061929\n",
      "Epoch: 25, Loss: 0.718289\n",
      "Epoch: 26, Loss: 0.389545\n",
      "Epoch: 27, Loss: 0.413426\n",
      "Epoch: 28, Loss: 0.448888\n",
      "Epoch: 29, Loss: 0.308737\n",
      "Epoch: 30, Loss: 0.781674\n",
      "Epoch: 31, Loss: 0.430049\n",
      "Epoch: 32, Loss: 0.850836\n",
      "Epoch: 33, Loss: 0.309553\n",
      "Epoch: 34, Loss: 0.638327\n",
      "Epoch: 35, Loss: 0.361515\n",
      "Epoch: 36, Loss: 0.324023\n",
      "Epoch: 37, Loss: 0.562612\n",
      "Epoch: 38, Loss: 0.195025\n",
      "Epoch: 39, Loss: 0.269289\n",
      "Epoch: 40, Loss: 0.325503\n",
      "Epoch: 41, Loss: 0.149030\n",
      "Epoch: 42, Loss: 0.450375\n",
      "Epoch: 43, Loss: 0.041237\n",
      "Epoch: 44, Loss: 0.061650\n",
      "Epoch: 45, Loss: 0.131295\n",
      "Epoch: 46, Loss: 0.202313\n",
      "Epoch: 47, Loss: 0.065841\n",
      "Epoch: 48, Loss: 0.030285\n",
      "Epoch: 49, Loss: 0.060257\n",
      "Epoch: 50, Loss: 0.065945\n",
      "Epoch: 51, Loss: 0.041945\n",
      "Epoch: 52, Loss: 0.071422\n",
      "Epoch: 53, Loss: 0.063625\n",
      "Epoch: 54, Loss: 0.054684\n",
      "Epoch: 55, Loss: 0.070166\n",
      "Epoch: 56, Loss: 0.027627\n",
      "Epoch: 57, Loss: 0.009930\n",
      "Epoch: 58, Loss: 0.024035\n",
      "Epoch: 59, Loss: 0.072519\n",
      "Epoch: 60, Loss: 0.031312\n",
      "Epoch: 61, Loss: 0.014138\n",
      "Epoch: 62, Loss: 0.064691\n",
      "Epoch: 63, Loss: 0.022045\n",
      "Epoch: 64, Loss: 0.004162\n",
      "Epoch: 65, Loss: 0.015082\n",
      "Epoch: 66, Loss: 0.008681\n",
      "Epoch: 67, Loss: 0.001982\n",
      "Epoch: 68, Loss: 0.005677\n",
      "Epoch: 69, Loss: 0.006809\n",
      "Epoch: 70, Loss: 0.001468\n",
      "Epoch: 71, Loss: 0.003303\n",
      "Epoch: 72, Loss: 0.012478\n",
      "Epoch: 73, Loss: 0.003741\n",
      "Epoch: 74, Loss: 0.003839\n",
      "Epoch: 75, Loss: 0.003550\n",
      "Epoch: 76, Loss: 0.003837\n",
      "Epoch: 77, Loss: 0.004644\n",
      "Epoch: 78, Loss: 0.002258\n",
      "Epoch: 79, Loss: 0.002294\n",
      "Epoch: 80, Loss: 0.007769\n",
      "Epoch: 81, Loss: 0.003308\n",
      "Epoch: 82, Loss: 0.003887\n",
      "Epoch: 83, Loss: 0.002454\n",
      "Epoch: 84, Loss: 0.005107\n",
      "Epoch: 85, Loss: 0.001438\n",
      "Epoch: 86, Loss: 0.001449\n",
      "Epoch: 87, Loss: 0.002120\n",
      "Epoch: 88, Loss: 0.002194\n",
      "Epoch: 89, Loss: 0.002200\n",
      "Epoch: 90, Loss: 0.002183\n",
      "Epoch: 91, Loss: 0.001505\n",
      "Epoch: 92, Loss: 0.001739\n",
      "Epoch: 93, Loss: 0.002119\n",
      "Epoch: 94, Loss: 0.001832\n",
      "Epoch: 95, Loss: 0.001288\n",
      "Epoch: 96, Loss: 0.001522\n",
      "Epoch: 97, Loss: 0.001756\n",
      "Epoch: 98, Loss: 0.008009\n",
      "Epoch: 99, Loss: 0.001499\n",
      "Epoch: 100, Loss: 0.001723\n",
      "Epoch: 101, Loss: 0.002963\n",
      "Epoch: 102, Loss: 0.002826\n",
      "Epoch: 103, Loss: 0.001608\n",
      "Epoch: 104, Loss: 0.001826\n",
      "Epoch: 105, Loss: 0.002729\n",
      "Epoch: 106, Loss: 0.001122\n",
      "Epoch: 107, Loss: 0.001127\n",
      "Epoch: 108, Loss: 0.001575\n",
      "Epoch: 109, Loss: 0.001970\n",
      "Epoch: 110, Loss: 0.002033\n",
      "Epoch: 111, Loss: 0.001434\n",
      "Epoch: 112, Loss: 0.001380\n",
      "Epoch: 113, Loss: 0.001549\n",
      "Epoch: 114, Loss: 0.000782\n",
      "Epoch: 115, Loss: 0.001065\n",
      "Epoch: 116, Loss: 0.001252\n",
      "Epoch: 117, Loss: 0.001394\n",
      "Epoch: 118, Loss: 0.000960\n",
      "Epoch: 119, Loss: 0.000756\n",
      "Epoch: 120, Loss: 0.001600\n",
      "Epoch: 121, Loss: 0.001872\n",
      "Epoch: 122, Loss: 0.000875\n",
      "Epoch: 123, Loss: 0.000865\n",
      "Epoch: 124, Loss: 0.001410\n",
      "Epoch: 125, Loss: 0.001431\n",
      "Epoch: 126, Loss: 0.000834\n",
      "Epoch: 127, Loss: 0.000761\n",
      "Epoch: 128, Loss: 0.001183\n",
      "Epoch: 129, Loss: 0.000943\n",
      "Epoch: 130, Loss: 0.001095\n",
      "Epoch: 131, Loss: 0.000899\n",
      "Epoch: 132, Loss: 0.001154\n",
      "Epoch: 133, Loss: 0.001173\n",
      "Epoch: 134, Loss: 0.000985\n",
      "Epoch: 135, Loss: 0.001215\n",
      "Epoch: 136, Loss: 0.000679\n",
      "Epoch: 137, Loss: 0.000731\n",
      "Epoch: 138, Loss: 0.000994\n",
      "Epoch: 139, Loss: 0.000988\n",
      "Epoch: 140, Loss: 0.001288\n",
      "Epoch: 141, Loss: 0.001165\n",
      "Epoch: 142, Loss: 0.000807\n",
      "Epoch: 143, Loss: 0.001151\n",
      "Epoch: 144, Loss: 0.000871\n",
      "Epoch: 145, Loss: 0.001177\n",
      "Epoch: 146, Loss: 0.001649\n",
      "Epoch: 147, Loss: 0.000898\n",
      "Epoch: 148, Loss: 0.000535\n",
      "Epoch: 149, Loss: 0.000954\n",
      "Epoch: 150, Loss: 0.000715\n",
      "Epoch: 151, Loss: 0.000922\n",
      "Epoch: 152, Loss: 0.000642\n",
      "Epoch: 153, Loss: 0.000559\n",
      "Epoch: 154, Loss: 0.000879\n",
      "Epoch: 155, Loss: 0.000752\n",
      "Epoch: 156, Loss: 0.000980\n",
      "Epoch: 157, Loss: 0.000828\n",
      "Epoch: 158, Loss: 0.000499\n",
      "Epoch: 159, Loss: 0.000332\n",
      "Epoch: 160, Loss: 0.000960\n",
      "Epoch: 161, Loss: 0.000905\n",
      "Epoch: 162, Loss: 0.000603\n",
      "Epoch: 163, Loss: 0.000542\n",
      "Epoch: 164, Loss: 0.000704\n",
      "Epoch: 165, Loss: 0.001304\n",
      "Epoch: 166, Loss: 0.000786\n",
      "Epoch: 167, Loss: 0.000743\n",
      "Epoch: 168, Loss: 0.000610\n",
      "Epoch: 169, Loss: 0.000786\n",
      "Epoch: 170, Loss: 0.000763\n",
      "Epoch: 171, Loss: 0.000523\n",
      "Epoch: 172, Loss: 0.000826\n",
      "Epoch: 173, Loss: 0.000567\n",
      "Epoch: 174, Loss: 0.004241\n",
      "Epoch: 175, Loss: 0.000604\n",
      "Epoch: 176, Loss: 0.000864\n",
      "Epoch: 177, Loss: 0.001187\n",
      "Epoch: 178, Loss: 0.000668\n",
      "Epoch: 179, Loss: 0.000712\n",
      "Epoch: 180, Loss: 0.000642\n",
      "Epoch: 181, Loss: 0.000820\n",
      "Epoch: 182, Loss: 0.000918\n",
      "Epoch: 183, Loss: 0.000708\n",
      "Epoch: 184, Loss: 0.000491\n",
      "Epoch: 185, Loss: 0.000993\n",
      "Epoch: 186, Loss: 0.000989\n",
      "Epoch: 187, Loss: 0.000368\n",
      "Epoch: 188, Loss: 0.000826\n",
      "Epoch: 189, Loss: 0.000686\n",
      "Epoch: 190, Loss: 0.000388\n",
      "Epoch: 191, Loss: 0.000768\n",
      "Epoch: 192, Loss: 0.000261\n",
      "Epoch: 193, Loss: 0.000813\n",
      "Epoch: 194, Loss: 0.000495\n",
      "Epoch: 195, Loss: 0.000621\n",
      "Epoch: 196, Loss: 0.000568\n",
      "Epoch: 197, Loss: 0.000386\n",
      "Epoch: 198, Loss: 0.000524\n",
      "Epoch: 199, Loss: 0.000257\n",
      "Epoch: 200, Loss: 0.000730\n",
      "Epoch: 201, Loss: 0.000717\n",
      "Epoch: 202, Loss: 0.000526\n",
      "Epoch: 203, Loss: 0.000708\n",
      "Epoch: 204, Loss: 0.000430\n",
      "Epoch: 205, Loss: 0.000574\n",
      "Epoch: 206, Loss: 0.000759\n",
      "Epoch: 207, Loss: 0.000303\n",
      "Epoch: 208, Loss: 0.001147\n",
      "Epoch: 209, Loss: 0.000835\n",
      "Epoch: 210, Loss: 0.000594\n",
      "Epoch: 211, Loss: 0.000436\n",
      "Epoch: 212, Loss: 0.000511\n",
      "Epoch: 213, Loss: 0.001058\n",
      "Epoch: 214, Loss: 0.000329\n",
      "Epoch: 215, Loss: 0.000375\n",
      "Epoch: 216, Loss: 0.000628\n",
      "Epoch: 217, Loss: 0.000520\n",
      "Epoch: 218, Loss: 0.000569\n",
      "Epoch: 219, Loss: 0.000588\n",
      "Epoch: 220, Loss: 0.000379\n",
      "Epoch: 221, Loss: 0.000454\n",
      "Epoch: 222, Loss: 0.000490\n",
      "Epoch: 223, Loss: 0.000507\n",
      "Epoch: 224, Loss: 0.000784\n",
      "Epoch: 225, Loss: 0.000450\n",
      "Epoch: 226, Loss: 0.000675\n",
      "Epoch: 227, Loss: 0.000357\n",
      "Epoch: 228, Loss: 0.000471\n",
      "Epoch: 229, Loss: 0.000595\n",
      "Epoch: 230, Loss: 0.000410\n",
      "Epoch: 231, Loss: 0.000285\n",
      "Epoch: 232, Loss: 0.000339\n",
      "Epoch: 233, Loss: 0.000940\n",
      "Epoch: 234, Loss: 0.000250\n",
      "Epoch: 235, Loss: 0.000731\n",
      "Epoch: 236, Loss: 0.000432\n",
      "Epoch: 237, Loss: 0.000475\n",
      "Epoch: 238, Loss: 0.000580\n",
      "Epoch: 239, Loss: 0.000337\n",
      "Epoch: 240, Loss: 0.000379\n",
      "Epoch: 241, Loss: 0.000405\n",
      "Epoch: 242, Loss: 0.000504\n",
      "Epoch: 243, Loss: 0.000439\n",
      "Epoch: 244, Loss: 0.000431\n",
      "Epoch: 245, Loss: 0.000725\n",
      "Epoch: 246, Loss: 0.000337\n",
      "Epoch: 247, Loss: 0.000503\n",
      "Epoch: 248, Loss: 0.000578\n",
      "Epoch: 249, Loss: 0.000361\n",
      "Epoch: 250, Loss: 0.000637\n",
      "Epoch: 251, Loss: 0.000226\n",
      "Epoch: 252, Loss: 0.000546\n",
      "Epoch: 253, Loss: 0.000554\n",
      "Epoch: 254, Loss: 0.000692\n",
      "Epoch: 255, Loss: 0.000381\n",
      "Epoch: 256, Loss: 0.000648\n",
      "Epoch: 257, Loss: 0.000347\n",
      "Epoch: 258, Loss: 0.000520\n",
      "Epoch: 259, Loss: 0.000408\n",
      "Epoch: 260, Loss: 0.000207\n",
      "Epoch: 261, Loss: 0.000490\n",
      "Epoch: 262, Loss: 0.000341\n",
      "Epoch: 263, Loss: 0.000468\n",
      "Epoch: 264, Loss: 0.000404\n",
      "Epoch: 265, Loss: 0.000403\n",
      "Epoch: 266, Loss: 0.000285\n",
      "Epoch: 267, Loss: 0.000412\n",
      "Epoch: 268, Loss: 0.000537\n",
      "Epoch: 269, Loss: 0.000364\n",
      "Epoch: 270, Loss: 0.000392\n",
      "Epoch: 271, Loss: 0.000501\n",
      "Epoch: 272, Loss: 0.000278\n",
      "Epoch: 273, Loss: 0.000414\n",
      "Epoch: 274, Loss: 0.000291\n",
      "Epoch: 275, Loss: 0.000232\n",
      "Epoch: 276, Loss: 0.000310\n",
      "Epoch: 277, Loss: 0.000362\n",
      "Epoch: 278, Loss: 0.000284\n",
      "Epoch: 279, Loss: 0.000384\n",
      "Epoch: 280, Loss: 0.000311\n",
      "Epoch: 281, Loss: 0.000249\n",
      "Epoch: 282, Loss: 0.000522\n",
      "Epoch: 283, Loss: 0.000453\n",
      "Epoch: 284, Loss: 0.000183\n",
      "Epoch: 285, Loss: 0.000301\n",
      "Epoch: 286, Loss: 0.000587\n",
      "Epoch: 287, Loss: 0.000375\n",
      "Epoch: 288, Loss: 0.000162\n",
      "Epoch: 289, Loss: 0.000331\n",
      "Epoch: 290, Loss: 0.000358\n",
      "Epoch: 291, Loss: 0.000235\n",
      "Epoch: 292, Loss: 0.000480\n",
      "Epoch: 293, Loss: 0.000390\n",
      "Epoch: 294, Loss: 0.000450\n",
      "Epoch: 295, Loss: 0.000210\n",
      "Epoch: 296, Loss: 0.000460\n",
      "Epoch: 297, Loss: 0.000160\n",
      "Epoch: 298, Loss: 0.000252\n",
      "Epoch: 299, Loss: 0.000542\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = CIFAR_model2(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2abc35f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: %f 0.4525\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = CIFAR_model2(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "print(\"Accuracy: %f\", (correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ddf7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
