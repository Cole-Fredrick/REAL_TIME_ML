{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9b7e4ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9b7e4ab",
        "outputId": "2dab0dd8-c39f-4a62-8ee1-ebfbe2af92bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8bba4faf90>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0199bb96",
      "metadata": {
        "id": "0199bb96"
      },
      "outputs": [],
      "source": [
        "##############################################################################################################################\n",
        "#########Problem 1#########\n",
        "##############################################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adc21268",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adc21268",
        "outputId": "ebf0b918-a695-4383-9876-75547e384135"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5a9214f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "55fda10dd22042d08466bb4ee0e516b9",
            "46ef21000ec94c45856f39dceb4b8aef",
            "1b2c4106dd88477cbbd39ac3654de724",
            "b33fe9614f534ef28c002a2b09ebbb78",
            "c524a45ee05846eb9018ae78add1b0cf",
            "9f6544e89cbd444bbb843d26ca18a045",
            "c3bca4c09b6e4ed0a2d519bea7550e42",
            "372c6a058c3a484c9af8acbbe5738862",
            "7ce5a74ea39d4c469f54d0bfb8746023",
            "9c18b4b364924a5d913a19afa3f54413",
            "e88e584e07af4187a81e1c4973d99b79"
          ]
        },
        "id": "b5a9214f",
        "outputId": "69a7dab6-efd5-40ac-b660-a3f560ae6783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55fda10dd22042d08466bb4ee0e516b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "data_path = '../data-unversioned/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))\n",
        "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfd0d29f",
      "metadata": {
        "id": "bfd0d29f"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8 * 8 * 8) # <1>\n",
        "        out = self.act3(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c208fee",
      "metadata": {
        "id": "9c208fee"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      imgs = imgs.to(device=device)\n",
        "      labels = labels.to(device=device)\n",
        "      outputs = model(imgs)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss_train += loss.item()\n",
        "    print('{} Epoch {}, Training loss {}'.format(\n",
        "    datetime.datetime.now(), epoch,\n",
        "    loss_train / len(train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, train_loader, val_loader):\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "          imgs, labels = imgs.to(device), labels.to(device)\n",
        "          batchsize = imgs.shape[0]\n",
        "          outputs = model(imgs)\n",
        "          _, predicted = torch.max(outputs, dim=1)\n",
        "          total += labels.shape[0]\n",
        "          correct += int((predicted == labels).sum())\n",
        "    print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"
      ],
      "metadata": {
        "id": "MfUQIIfYmKdm"
      },
      "id": "MfUQIIfYmKdm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75714be4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75714be4",
        "outputId": "20a172a8-5cb8-4d8c-94cd-83cb658f4a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-29 02:55:28.511502 Epoch 1, Training loss 2.098292909467312\n",
            "2022-03-29 02:55:44.957787 Epoch 2, Training loss 1.785247363855162\n",
            "2022-03-29 02:56:01.498808 Epoch 3, Training loss 1.6172291255363114\n",
            "2022-03-29 02:56:18.106240 Epoch 4, Training loss 1.5265314772610774\n",
            "2022-03-29 02:56:34.565004 Epoch 5, Training loss 1.460617965901904\n",
            "2022-03-29 02:56:51.125967 Epoch 6, Training loss 1.3960864796967762\n",
            "2022-03-29 02:57:07.374479 Epoch 7, Training loss 1.3315046275668132\n",
            "2022-03-29 02:57:23.676630 Epoch 8, Training loss 1.2779924499866602\n",
            "2022-03-29 02:57:40.115119 Epoch 9, Training loss 1.2345515409546435\n",
            "2022-03-29 02:57:56.530338 Epoch 10, Training loss 1.1979004371044275\n",
            "2022-03-29 02:58:13.287508 Epoch 11, Training loss 1.1683111435464582\n",
            "2022-03-29 02:58:30.434091 Epoch 12, Training loss 1.1412167190895666\n",
            "2022-03-29 02:58:47.213326 Epoch 13, Training loss 1.1183172301258273\n",
            "2022-03-29 02:59:05.539046 Epoch 14, Training loss 1.0987535007774372\n",
            "2022-03-29 02:59:23.375201 Epoch 15, Training loss 1.0813645399592418\n",
            "2022-03-29 02:59:39.743912 Epoch 16, Training loss 1.064727510104094\n",
            "2022-03-29 02:59:56.237525 Epoch 17, Training loss 1.0497497331608288\n",
            "2022-03-29 03:00:12.552136 Epoch 18, Training loss 1.0367035318518538\n",
            "2022-03-29 03:00:28.910718 Epoch 19, Training loss 1.0225555322054403\n",
            "2022-03-29 03:00:45.144734 Epoch 20, Training loss 1.0104577020001229\n",
            "2022-03-29 03:01:01.272248 Epoch 21, Training loss 0.9984980390962127\n",
            "2022-03-29 03:01:17.389896 Epoch 22, Training loss 0.9870203161026205\n",
            "2022-03-29 03:01:33.446773 Epoch 23, Training loss 0.9763542122548193\n",
            "2022-03-29 03:01:49.535846 Epoch 24, Training loss 0.9685943522264281\n",
            "2022-03-29 03:02:05.608171 Epoch 25, Training loss 0.9599722945476736\n",
            "2022-03-29 03:02:21.853665 Epoch 26, Training loss 0.9513908764133063\n",
            "2022-03-29 03:02:37.972432 Epoch 27, Training loss 0.9430117405893857\n",
            "2022-03-29 03:02:53.965290 Epoch 28, Training loss 0.9361549005331591\n",
            "2022-03-29 03:03:10.093494 Epoch 29, Training loss 0.9279146470377208\n",
            "2022-03-29 03:03:26.541258 Epoch 30, Training loss 0.9211312612456739\n",
            "2022-03-29 03:03:42.834169 Epoch 31, Training loss 0.9158164428932892\n",
            "2022-03-29 03:03:59.097944 Epoch 32, Training loss 0.9086433960043866\n",
            "2022-03-29 03:04:15.396773 Epoch 33, Training loss 0.9024460647264709\n",
            "2022-03-29 03:04:31.594407 Epoch 34, Training loss 0.8983353927464741\n",
            "2022-03-29 03:04:48.076073 Epoch 35, Training loss 0.8920338855070227\n",
            "2022-03-29 03:05:04.449025 Epoch 36, Training loss 0.8878011528183433\n",
            "2022-03-29 03:05:20.634813 Epoch 37, Training loss 0.8808134042698404\n",
            "2022-03-29 03:05:36.799275 Epoch 38, Training loss 0.8751728677231333\n",
            "2022-03-29 03:05:52.888188 Epoch 39, Training loss 0.8715454747762217\n",
            "2022-03-29 03:06:09.337100 Epoch 40, Training loss 0.8678278495436129\n",
            "2022-03-29 03:06:26.099361 Epoch 41, Training loss 0.8627779941882014\n",
            "2022-03-29 03:06:42.244368 Epoch 42, Training loss 0.8587816173920546\n",
            "2022-03-29 03:06:58.322610 Epoch 43, Training loss 0.8532133981623613\n",
            "2022-03-29 03:07:14.402082 Epoch 44, Training loss 0.8491877326956185\n",
            "2022-03-29 03:07:30.517585 Epoch 45, Training loss 0.8461499280484436\n",
            "2022-03-29 03:07:46.731280 Epoch 46, Training loss 0.8419598073453245\n",
            "2022-03-29 03:08:03.126595 Epoch 47, Training loss 0.8397845892650088\n",
            "2022-03-29 03:08:19.691139 Epoch 48, Training loss 0.833157032118429\n",
            "2022-03-29 03:08:36.184989 Epoch 49, Training loss 0.830636063042809\n",
            "2022-03-29 03:08:52.691442 Epoch 50, Training loss 0.8281766366394584\n",
            "2022-03-29 03:09:08.840545 Epoch 51, Training loss 0.8235505400868632\n",
            "2022-03-29 03:09:25.040153 Epoch 52, Training loss 0.8206282400185495\n",
            "2022-03-29 03:09:41.121176 Epoch 53, Training loss 0.81739061819318\n",
            "2022-03-29 03:09:57.273842 Epoch 54, Training loss 0.8134695308668839\n",
            "2022-03-29 03:10:13.408780 Epoch 55, Training loss 0.8097742314228926\n",
            "2022-03-29 03:10:29.530875 Epoch 56, Training loss 0.8083909700822343\n",
            "2022-03-29 03:10:45.591301 Epoch 57, Training loss 0.8045051425238094\n",
            "2022-03-29 03:11:01.537537 Epoch 58, Training loss 0.8031258754565588\n",
            "2022-03-29 03:11:17.721693 Epoch 59, Training loss 0.798725777155603\n",
            "2022-03-29 03:11:33.830301 Epoch 60, Training loss 0.7961614345345656\n",
            "2022-03-29 03:11:50.044405 Epoch 61, Training loss 0.7920944979199973\n",
            "2022-03-29 03:12:06.192294 Epoch 62, Training loss 0.7891762294351597\n",
            "2022-03-29 03:12:22.324718 Epoch 63, Training loss 0.7876228921477447\n",
            "2022-03-29 03:12:38.386841 Epoch 64, Training loss 0.7827830054723394\n",
            "2022-03-29 03:12:54.430643 Epoch 65, Training loss 0.7805662662781718\n",
            "2022-03-29 03:13:10.491849 Epoch 66, Training loss 0.779191897272149\n",
            "2022-03-29 03:13:26.584049 Epoch 67, Training loss 0.7763667689717334\n",
            "2022-03-29 03:13:42.757740 Epoch 68, Training loss 0.7745497886024778\n",
            "2022-03-29 03:13:58.939717 Epoch 69, Training loss 0.7702815239996557\n",
            "2022-03-29 03:14:15.142775 Epoch 70, Training loss 0.7677711191613351\n",
            "2022-03-29 03:14:31.360705 Epoch 71, Training loss 0.7674367167913091\n",
            "2022-03-29 03:14:47.484668 Epoch 72, Training loss 0.7642256206716113\n",
            "2022-03-29 03:15:03.656266 Epoch 73, Training loss 0.7625424952991783\n",
            "2022-03-29 03:15:19.730432 Epoch 74, Training loss 0.7589570281984251\n",
            "2022-03-29 03:15:35.778328 Epoch 75, Training loss 0.7567437870042099\n",
            "2022-03-29 03:15:51.842845 Epoch 76, Training loss 0.7556332957637889\n",
            "2022-03-29 03:16:07.886566 Epoch 77, Training loss 0.7522985399379145\n",
            "2022-03-29 03:16:23.948217 Epoch 78, Training loss 0.7496131711908619\n",
            "2022-03-29 03:16:39.991489 Epoch 79, Training loss 0.7471572187398096\n",
            "2022-03-29 03:16:56.041072 Epoch 80, Training loss 0.7449337906773438\n",
            "2022-03-29 03:17:11.943535 Epoch 81, Training loss 0.742473365667531\n",
            "2022-03-29 03:17:27.812429 Epoch 82, Training loss 0.7424075911417032\n",
            "2022-03-29 03:17:43.701024 Epoch 83, Training loss 0.7392076789723028\n",
            "2022-03-29 03:17:59.676583 Epoch 84, Training loss 0.7369455094532589\n",
            "2022-03-29 03:18:15.622429 Epoch 85, Training loss 0.7350164632053326\n",
            "2022-03-29 03:18:31.623503 Epoch 86, Training loss 0.7323505903220238\n",
            "2022-03-29 03:18:47.622012 Epoch 87, Training loss 0.731219464288953\n",
            "2022-03-29 03:19:03.532690 Epoch 88, Training loss 0.7283291466476972\n",
            "2022-03-29 03:19:19.574703 Epoch 89, Training loss 0.7261771639365979\n",
            "2022-03-29 03:19:35.572594 Epoch 90, Training loss 0.7251748076027922\n",
            "2022-03-29 03:19:51.595220 Epoch 91, Training loss 0.7225163981432805\n",
            "2022-03-29 03:20:07.699194 Epoch 92, Training loss 0.7206250161999632\n",
            "2022-03-29 03:20:23.932318 Epoch 93, Training loss 0.7192150644786522\n",
            "2022-03-29 03:20:39.982688 Epoch 94, Training loss 0.716696811606512\n",
            "2022-03-29 03:20:56.259141 Epoch 95, Training loss 0.7169016753621114\n",
            "2022-03-29 03:21:12.465735 Epoch 96, Training loss 0.7133899548703142\n",
            "2022-03-29 03:21:28.628094 Epoch 97, Training loss 0.7120571545017954\n",
            "2022-03-29 03:21:44.818767 Epoch 98, Training loss 0.7093997961267486\n",
            "2022-03-29 03:22:00.992022 Epoch 99, Training loss 0.7083234695141273\n",
            "2022-03-29 03:22:17.167209 Epoch 100, Training loss 0.7069524261728882\n",
            "2022-03-29 03:22:33.415375 Epoch 101, Training loss 0.704463880652052\n",
            "2022-03-29 03:22:49.652089 Epoch 102, Training loss 0.702961806522306\n",
            "2022-03-29 03:23:05.894280 Epoch 103, Training loss 0.7021463707737301\n",
            "2022-03-29 03:23:22.133187 Epoch 104, Training loss 0.6982278471331462\n",
            "2022-03-29 03:23:38.386034 Epoch 105, Training loss 0.6973925447829848\n",
            "2022-03-29 03:23:54.706446 Epoch 106, Training loss 0.695757226489694\n",
            "2022-03-29 03:24:11.019403 Epoch 107, Training loss 0.6966190300405483\n",
            "2022-03-29 03:24:27.263199 Epoch 108, Training loss 0.6935375241748513\n",
            "2022-03-29 03:24:43.608963 Epoch 109, Training loss 0.6918945741622954\n",
            "2022-03-29 03:24:59.984855 Epoch 110, Training loss 0.6908204024252684\n",
            "2022-03-29 03:25:16.390987 Epoch 111, Training loss 0.6880581767281608\n",
            "2022-03-29 03:25:32.806778 Epoch 112, Training loss 0.6884850618403281\n",
            "2022-03-29 03:25:49.115315 Epoch 113, Training loss 0.6862342139262982\n",
            "2022-03-29 03:26:05.470580 Epoch 114, Training loss 0.6833256302815874\n",
            "2022-03-29 03:26:21.884349 Epoch 115, Training loss 0.6833944377082083\n",
            "2022-03-29 03:26:38.262657 Epoch 116, Training loss 0.6817905578924262\n",
            "2022-03-29 03:26:54.852197 Epoch 117, Training loss 0.6803156804017094\n",
            "2022-03-29 03:27:11.249771 Epoch 118, Training loss 0.6785744370325751\n",
            "2022-03-29 03:27:27.584306 Epoch 119, Training loss 0.6768169121821518\n",
            "2022-03-29 03:27:43.979032 Epoch 120, Training loss 0.676035494755601\n",
            "2022-03-29 03:28:00.404693 Epoch 121, Training loss 0.672240778079728\n",
            "2022-03-29 03:28:16.786537 Epoch 122, Training loss 0.6738973774797167\n",
            "2022-03-29 03:28:32.949087 Epoch 123, Training loss 0.6711863099080523\n",
            "2022-03-29 03:28:49.162716 Epoch 124, Training loss 0.6698969430325891\n",
            "2022-03-29 03:29:05.403201 Epoch 125, Training loss 0.6681691270578852\n",
            "2022-03-29 03:29:21.702528 Epoch 126, Training loss 0.6673993117669049\n",
            "2022-03-29 03:29:38.085400 Epoch 127, Training loss 0.6634987436444558\n",
            "2022-03-29 03:29:54.411553 Epoch 128, Training loss 0.6661889236372756\n",
            "2022-03-29 03:30:10.772222 Epoch 129, Training loss 0.6635072588768152\n",
            "2022-03-29 03:30:27.104590 Epoch 130, Training loss 0.6614431406149779\n",
            "2022-03-29 03:30:43.473272 Epoch 131, Training loss 0.660283566778883\n",
            "2022-03-29 03:30:59.801192 Epoch 132, Training loss 0.660198241357913\n",
            "2022-03-29 03:31:16.136785 Epoch 133, Training loss 0.6599525939244444\n",
            "2022-03-29 03:31:32.443337 Epoch 134, Training loss 0.6585558965764082\n",
            "2022-03-29 03:31:48.725654 Epoch 135, Training loss 0.6552081508252322\n",
            "2022-03-29 03:32:05.119104 Epoch 136, Training loss 0.6551416105855151\n",
            "2022-03-29 03:32:21.466180 Epoch 137, Training loss 0.6542735538824135\n",
            "2022-03-29 03:32:37.726943 Epoch 138, Training loss 0.6519401679410959\n",
            "2022-03-29 03:32:54.043873 Epoch 139, Training loss 0.6510785673280506\n",
            "2022-03-29 03:33:10.313799 Epoch 140, Training loss 0.649881843182132\n",
            "2022-03-29 03:33:26.662131 Epoch 141, Training loss 0.6478022770274936\n",
            "2022-03-29 03:33:42.928730 Epoch 142, Training loss 0.6479237203479118\n",
            "2022-03-29 03:33:59.174322 Epoch 143, Training loss 0.6477943838328657\n",
            "2022-03-29 03:34:15.503639 Epoch 144, Training loss 0.6471176811724978\n",
            "2022-03-29 03:34:31.855183 Epoch 145, Training loss 0.6452394402240549\n",
            "2022-03-29 03:34:48.043390 Epoch 146, Training loss 0.6447957874945057\n",
            "2022-03-29 03:35:04.402604 Epoch 147, Training loss 0.6411082331862901\n",
            "2022-03-29 03:35:20.758419 Epoch 148, Training loss 0.6404653158791535\n",
            "2022-03-29 03:35:37.266283 Epoch 149, Training loss 0.6404137876256347\n",
            "2022-03-29 03:35:53.715032 Epoch 150, Training loss 0.6381602049864772\n",
            "2022-03-29 03:36:09.998775 Epoch 151, Training loss 0.6364582180595764\n",
            "2022-03-29 03:36:26.309118 Epoch 152, Training loss 0.6371242385309981\n",
            "2022-03-29 03:36:42.560236 Epoch 153, Training loss 0.6359944764686667\n",
            "2022-03-29 03:36:58.876718 Epoch 154, Training loss 0.6354844647905101\n",
            "2022-03-29 03:37:15.106266 Epoch 155, Training loss 0.6345104941397982\n",
            "2022-03-29 03:37:31.398395 Epoch 156, Training loss 0.6334240452179214\n",
            "2022-03-29 03:37:47.629036 Epoch 157, Training loss 0.6315438544277645\n",
            "2022-03-29 03:38:03.743361 Epoch 158, Training loss 0.6302390700334783\n",
            "2022-03-29 03:38:20.098181 Epoch 159, Training loss 0.6299181064147779\n",
            "2022-03-29 03:38:36.438430 Epoch 160, Training loss 0.6297976002287682\n",
            "2022-03-29 03:38:52.677255 Epoch 161, Training loss 0.628501235142998\n",
            "2022-03-29 03:39:09.079665 Epoch 162, Training loss 0.6275027025386196\n",
            "2022-03-29 03:39:25.345492 Epoch 163, Training loss 0.6259408874813553\n",
            "2022-03-29 03:39:41.628661 Epoch 164, Training loss 0.6250030369404942\n",
            "2022-03-29 03:39:58.309286 Epoch 165, Training loss 0.6230544558418986\n",
            "2022-03-29 03:40:14.841232 Epoch 166, Training loss 0.6215358815915749\n",
            "2022-03-29 03:40:31.406809 Epoch 167, Training loss 0.6207286810783474\n",
            "2022-03-29 03:40:47.702196 Epoch 168, Training loss 0.6209018302466863\n",
            "2022-03-29 03:41:04.028877 Epoch 169, Training loss 0.6204132209043673\n",
            "2022-03-29 03:41:20.311409 Epoch 170, Training loss 0.6198986939075962\n",
            "2022-03-29 03:41:36.585813 Epoch 171, Training loss 0.6173397844938366\n",
            "2022-03-29 03:41:52.895537 Epoch 172, Training loss 0.6177224553835666\n",
            "2022-03-29 03:42:09.118998 Epoch 173, Training loss 0.6172530971600881\n",
            "2022-03-29 03:42:25.491747 Epoch 174, Training loss 0.6180217169663486\n",
            "2022-03-29 03:42:41.746891 Epoch 175, Training loss 0.6145943024045671\n",
            "2022-03-29 03:42:58.031072 Epoch 176, Training loss 0.6143064281672163\n",
            "2022-03-29 03:43:14.383184 Epoch 177, Training loss 0.6124090248209131\n",
            "2022-03-29 03:43:30.850397 Epoch 178, Training loss 0.6116840524213089\n",
            "2022-03-29 03:43:47.359972 Epoch 179, Training loss 0.6121559850776287\n",
            "2022-03-29 03:44:03.758293 Epoch 180, Training loss 0.6103364800476967\n",
            "2022-03-29 03:44:20.145420 Epoch 181, Training loss 0.6104684246470556\n",
            "2022-03-29 03:44:36.511899 Epoch 182, Training loss 0.6101199682716214\n",
            "2022-03-29 03:44:52.846720 Epoch 183, Training loss 0.6076380944694094\n",
            "2022-03-29 03:45:09.123660 Epoch 184, Training loss 0.6048304301012507\n",
            "2022-03-29 03:45:25.326196 Epoch 185, Training loss 0.6058011298518047\n",
            "2022-03-29 03:45:41.653638 Epoch 186, Training loss 0.6053872532246972\n",
            "2022-03-29 03:45:57.992787 Epoch 187, Training loss 0.6037892896653442\n",
            "2022-03-29 03:46:14.174101 Epoch 188, Training loss 0.6044694313688961\n",
            "2022-03-29 03:46:30.382026 Epoch 189, Training loss 0.6030095000096294\n",
            "2022-03-29 03:46:46.631875 Epoch 190, Training loss 0.6021149679065665\n",
            "2022-03-29 03:47:02.906864 Epoch 191, Training loss 0.6028771602055606\n",
            "2022-03-29 03:47:19.174717 Epoch 192, Training loss 0.6013981091701771\n",
            "2022-03-29 03:47:35.340549 Epoch 193, Training loss 0.5986585592293678\n",
            "2022-03-29 03:47:51.532116 Epoch 194, Training loss 0.5975984423361775\n",
            "2022-03-29 03:48:07.744541 Epoch 195, Training loss 0.5978435220011055\n",
            "2022-03-29 03:48:24.069693 Epoch 196, Training loss 0.5993723526520802\n",
            "2022-03-29 03:48:40.306052 Epoch 197, Training loss 0.5968530825946642\n",
            "2022-03-29 03:48:56.514294 Epoch 198, Training loss 0.594723846182189\n",
            "2022-03-29 03:49:12.749465 Epoch 199, Training loss 0.5936594675950078\n",
            "2022-03-29 03:49:28.921471 Epoch 200, Training loss 0.5956517864218758\n",
            "2022-03-29 03:49:45.224975 Epoch 201, Training loss 0.5949522890436375\n",
            "2022-03-29 03:50:01.528610 Epoch 202, Training loss 0.5931381398759534\n",
            "2022-03-29 03:50:17.724110 Epoch 203, Training loss 0.5918398264728849\n",
            "2022-03-29 03:50:33.882056 Epoch 204, Training loss 0.5918220973685574\n",
            "2022-03-29 03:50:50.084213 Epoch 205, Training loss 0.5917728973928925\n",
            "2022-03-29 03:51:06.290383 Epoch 206, Training loss 0.5889557534662049\n",
            "2022-03-29 03:51:22.564670 Epoch 207, Training loss 0.589007810024959\n",
            "2022-03-29 03:51:38.693319 Epoch 208, Training loss 0.5887828133523921\n",
            "2022-03-29 03:51:54.824733 Epoch 209, Training loss 0.5879510962368583\n",
            "2022-03-29 03:52:10.964859 Epoch 210, Training loss 0.5869513181469325\n",
            "2022-03-29 03:52:27.074681 Epoch 211, Training loss 0.5873901837164789\n",
            "2022-03-29 03:52:43.185970 Epoch 212, Training loss 0.5860331119097713\n",
            "2022-03-29 03:52:59.356420 Epoch 213, Training loss 0.5850086811253482\n",
            "2022-03-29 03:53:15.547781 Epoch 214, Training loss 0.5845975238267723\n",
            "2022-03-29 03:53:31.674232 Epoch 215, Training loss 0.5843934837890707\n",
            "2022-03-29 03:53:47.843441 Epoch 216, Training loss 0.5823584169035068\n",
            "2022-03-29 03:54:04.004052 Epoch 217, Training loss 0.5803724115385729\n",
            "2022-03-29 03:54:20.237305 Epoch 218, Training loss 0.5823202370225317\n",
            "2022-03-29 03:54:36.351149 Epoch 219, Training loss 0.5810132163107548\n",
            "2022-03-29 03:54:52.567086 Epoch 220, Training loss 0.5801094160284228\n",
            "2022-03-29 03:55:08.710967 Epoch 221, Training loss 0.5817040011782171\n",
            "2022-03-29 03:55:25.033534 Epoch 222, Training loss 0.5797656889614242\n",
            "2022-03-29 03:55:41.486116 Epoch 223, Training loss 0.5784043604531861\n",
            "2022-03-29 03:55:57.990513 Epoch 224, Training loss 0.5766531289995783\n",
            "2022-03-29 03:56:14.075856 Epoch 225, Training loss 0.5766737146107742\n",
            "2022-03-29 03:56:30.148259 Epoch 226, Training loss 0.5764557833180708\n",
            "2022-03-29 03:56:46.350353 Epoch 227, Training loss 0.5766026846054569\n",
            "2022-03-29 03:57:02.578112 Epoch 228, Training loss 0.5764595542455573\n",
            "2022-03-29 03:57:19.140830 Epoch 229, Training loss 0.5737152095036129\n",
            "2022-03-29 03:57:35.605358 Epoch 230, Training loss 0.5730849945407999\n",
            "2022-03-29 03:57:51.961391 Epoch 231, Training loss 0.5734220488983042\n",
            "2022-03-29 03:58:08.317941 Epoch 232, Training loss 0.5722536023544229\n",
            "2022-03-29 03:58:24.648248 Epoch 233, Training loss 0.5721614770877087\n",
            "2022-03-29 03:58:40.832469 Epoch 234, Training loss 0.572029745296749\n",
            "2022-03-29 03:58:56.954776 Epoch 235, Training loss 0.5708935175405438\n",
            "2022-03-29 03:59:13.134791 Epoch 236, Training loss 0.5707397812696369\n",
            "2022-03-29 03:59:29.454271 Epoch 237, Training loss 0.5694762440898534\n",
            "2022-03-29 03:59:45.620578 Epoch 238, Training loss 0.5697324940996706\n",
            "2022-03-29 04:00:02.014775 Epoch 239, Training loss 0.5680903096485626\n",
            "2022-03-29 04:00:18.692885 Epoch 240, Training loss 0.5681627970141219\n",
            "2022-03-29 04:00:34.912221 Epoch 241, Training loss 0.5686214571947332\n",
            "2022-03-29 04:00:51.005272 Epoch 242, Training loss 0.5662955959587146\n",
            "2022-03-29 04:01:07.268099 Epoch 243, Training loss 0.5646733320734995\n",
            "2022-03-29 04:01:23.499576 Epoch 244, Training loss 0.5651377477609288\n",
            "2022-03-29 04:01:39.619141 Epoch 245, Training loss 0.5652412935672209\n",
            "2022-03-29 04:01:55.898391 Epoch 246, Training loss 0.5643784483832777\n",
            "2022-03-29 04:02:12.083803 Epoch 247, Training loss 0.5635640635286145\n",
            "2022-03-29 04:02:28.274516 Epoch 248, Training loss 0.5635654547673357\n",
            "2022-03-29 04:02:44.566671 Epoch 249, Training loss 0.5618987839545131\n",
            "2022-03-29 04:03:00.848153 Epoch 250, Training loss 0.5629629966852915\n",
            "2022-03-29 04:03:17.288958 Epoch 251, Training loss 0.5617856079583887\n",
            "2022-03-29 04:03:33.593402 Epoch 252, Training loss 0.5608622217193588\n",
            "2022-03-29 04:03:49.947362 Epoch 253, Training loss 0.5597834090899934\n",
            "2022-03-29 04:04:06.310315 Epoch 254, Training loss 0.5595557119916467\n",
            "2022-03-29 04:04:22.541591 Epoch 255, Training loss 0.5595403174152764\n",
            "2022-03-29 04:04:38.669207 Epoch 256, Training loss 0.5597032622608078\n",
            "2022-03-29 04:04:54.924373 Epoch 257, Training loss 0.5584822267370151\n",
            "2022-03-29 04:05:11.088412 Epoch 258, Training loss 0.5578903530717201\n",
            "2022-03-29 04:05:27.254112 Epoch 259, Training loss 0.557936127304726\n",
            "2022-03-29 04:05:43.420444 Epoch 260, Training loss 0.5555135716334023\n",
            "2022-03-29 04:05:59.557690 Epoch 261, Training loss 0.5560582029011548\n",
            "2022-03-29 04:06:15.683631 Epoch 262, Training loss 0.5546902722257483\n",
            "2022-03-29 04:06:31.754618 Epoch 263, Training loss 0.5547028680134307\n",
            "2022-03-29 04:06:47.836845 Epoch 264, Training loss 0.553103031023689\n",
            "2022-03-29 04:07:03.915274 Epoch 265, Training loss 0.5528947881344334\n",
            "2022-03-29 04:07:20.183815 Epoch 266, Training loss 0.5543682788262891\n",
            "2022-03-29 04:07:36.547501 Epoch 267, Training loss 0.5534831987470007\n",
            "2022-03-29 04:07:52.927904 Epoch 268, Training loss 0.5520947824811082\n",
            "2022-03-29 04:08:09.255743 Epoch 269, Training loss 0.5510141109795217\n",
            "2022-03-29 04:08:25.553511 Epoch 270, Training loss 0.5500095382980679\n",
            "2022-03-29 04:08:41.786300 Epoch 271, Training loss 0.5495821528727441\n",
            "2022-03-29 04:08:58.075327 Epoch 272, Training loss 0.551873640636044\n",
            "2022-03-29 04:09:14.359201 Epoch 273, Training loss 0.5505801034743524\n",
            "2022-03-29 04:09:30.615193 Epoch 274, Training loss 0.5503251237408889\n",
            "2022-03-29 04:09:46.839757 Epoch 275, Training loss 0.5486739711535861\n",
            "2022-03-29 04:10:03.033443 Epoch 276, Training loss 0.5490220770659044\n",
            "2022-03-29 04:10:19.251615 Epoch 277, Training loss 0.5494598450944247\n",
            "2022-03-29 04:10:35.440554 Epoch 278, Training loss 0.5473735247121747\n",
            "2022-03-29 04:10:51.664006 Epoch 279, Training loss 0.5473000758977802\n",
            "2022-03-29 04:11:07.831234 Epoch 280, Training loss 0.5482738291287361\n",
            "2022-03-29 04:11:24.008225 Epoch 281, Training loss 0.5453469856544528\n",
            "2022-03-29 04:11:40.282370 Epoch 282, Training loss 0.5445881673061025\n",
            "2022-03-29 04:11:56.449175 Epoch 283, Training loss 0.5465167615648425\n",
            "2022-03-29 04:12:12.567345 Epoch 284, Training loss 0.5442030627251891\n",
            "2022-03-29 04:12:28.703909 Epoch 285, Training loss 0.5437659494331121\n",
            "2022-03-29 04:12:44.917162 Epoch 286, Training loss 0.5444902880950961\n",
            "2022-03-29 04:13:01.187396 Epoch 287, Training loss 0.5440796395320722\n",
            "2022-03-29 04:13:17.472282 Epoch 288, Training loss 0.5430201316809715\n",
            "2022-03-29 04:13:33.619026 Epoch 289, Training loss 0.541213220952417\n",
            "2022-03-29 04:13:49.892348 Epoch 290, Training loss 0.5417560293241535\n",
            "2022-03-29 04:14:06.166228 Epoch 291, Training loss 0.5411269783097155\n",
            "2022-03-29 04:14:22.302557 Epoch 292, Training loss 0.539753924969517\n",
            "2022-03-29 04:14:38.433490 Epoch 293, Training loss 0.5407832288147544\n",
            "2022-03-29 04:14:54.629069 Epoch 294, Training loss 0.5403732714598136\n",
            "2022-03-29 04:15:10.771279 Epoch 295, Training loss 0.5401983678417133\n",
            "2022-03-29 04:15:27.033748 Epoch 296, Training loss 0.5401988684978631\n",
            "2022-03-29 04:15:43.261740 Epoch 297, Training loss 0.5371984465004843\n",
            "2022-03-29 04:15:59.394683 Epoch 298, Training loss 0.53649991882198\n",
            "2022-03-29 04:16:15.584403 Epoch 299, Training loss 0.5378842502451309\n",
            "2022-03-29 04:16:31.724349 Epoch 300, Training loss 0.5379038864122633\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)  # <1>\n",
        "\n",
        "model = Net().to(device=device)  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8_AkhNX5qBt",
        "outputId": "d4382751-662f-4a04-cd98-40d4e849805b"
      },
      "id": "H8_AkhNX5qBt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.79\n",
            "Accuracy val: 0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################################################################\n",
        "#########Problem 1 PART B #############\n",
        "##############################################################################################################################"
      ],
      "metadata": {
        "id": "ScveHuiTnaXp"
      },
      "id": "ScveHuiTnaXp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9476994",
      "metadata": {
        "id": "b9476994"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = nn.Conv2d(8, 3, kernel_size=3, padding=1)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8 * 8 * 8) # <1>\n",
        "        out = self.act3(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                        shuffle=True)\n",
        "model = Net().to(device=device)  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtL98diwo6te",
        "outputId": "0e7d4f1b-270e-4199-a097-77878bef0046"
      },
      "id": "VtL98diwo6te",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-29 04:44:05.192207 Epoch 1, Training loss 2.0476194139941573\n",
            "2022-03-29 04:44:21.416536 Epoch 2, Training loss 1.7932819879573325\n",
            "2022-03-29 04:44:37.600597 Epoch 3, Training loss 1.6093341150247227\n",
            "2022-03-29 04:44:53.748188 Epoch 4, Training loss 1.504915327519712\n",
            "2022-03-29 04:45:09.785505 Epoch 5, Training loss 1.4332176494171551\n",
            "2022-03-29 04:45:25.983206 Epoch 6, Training loss 1.3671491766524742\n",
            "2022-03-29 04:45:41.928765 Epoch 7, Training loss 1.310898762987093\n",
            "2022-03-29 04:45:58.000601 Epoch 8, Training loss 1.262955849128001\n",
            "2022-03-29 04:46:14.110734 Epoch 9, Training loss 1.2249980403486724\n",
            "2022-03-29 04:46:30.077334 Epoch 10, Training loss 1.1931186087448578\n",
            "2022-03-29 04:46:46.138664 Epoch 11, Training loss 1.1658514782290934\n",
            "2022-03-29 04:47:02.169020 Epoch 12, Training loss 1.1414087233336077\n",
            "2022-03-29 04:47:18.183312 Epoch 13, Training loss 1.1186294909328451\n",
            "2022-03-29 04:47:34.181483 Epoch 14, Training loss 1.1012593538255033\n",
            "2022-03-29 04:47:50.238129 Epoch 15, Training loss 1.0868839213762747\n",
            "2022-03-29 04:48:06.337656 Epoch 16, Training loss 1.0701049291111928\n",
            "2022-03-29 04:48:22.522027 Epoch 17, Training loss 1.056854472745715\n",
            "2022-03-29 04:48:38.616286 Epoch 18, Training loss 1.0434525705816802\n",
            "2022-03-29 04:48:54.695818 Epoch 19, Training loss 1.0318034239437268\n",
            "2022-03-29 04:49:10.536841 Epoch 20, Training loss 1.0214847234813758\n",
            "2022-03-29 04:49:26.704349 Epoch 21, Training loss 1.0116219730175975\n",
            "2022-03-29 04:49:42.837469 Epoch 22, Training loss 1.0008359894423229\n",
            "2022-03-29 04:49:59.143774 Epoch 23, Training loss 0.9899806528140211\n",
            "2022-03-29 04:50:15.395515 Epoch 24, Training loss 0.983061324528721\n",
            "2022-03-29 04:50:31.467212 Epoch 25, Training loss 0.9753647975604552\n",
            "2022-03-29 04:50:47.616223 Epoch 26, Training loss 0.9652918141211391\n",
            "2022-03-29 04:51:03.793097 Epoch 27, Training loss 0.9572945996318631\n",
            "2022-03-29 04:51:19.913082 Epoch 28, Training loss 0.9496668057368539\n",
            "2022-03-29 04:51:36.168776 Epoch 29, Training loss 0.9435373862533618\n",
            "2022-03-29 04:51:52.424330 Epoch 30, Training loss 0.9379475910187988\n",
            "2022-03-29 04:52:08.480048 Epoch 31, Training loss 0.9292231115233868\n",
            "2022-03-29 04:52:24.584962 Epoch 32, Training loss 0.9227231974187105\n",
            "2022-03-29 04:52:40.716858 Epoch 33, Training loss 0.9179274650943249\n",
            "2022-03-29 04:52:56.813626 Epoch 34, Training loss 0.9113619358795683\n",
            "2022-03-29 04:53:12.841020 Epoch 35, Training loss 0.9053229826795476\n",
            "2022-03-29 04:53:28.915168 Epoch 36, Training loss 0.8979318544382939\n",
            "2022-03-29 04:53:45.009522 Epoch 37, Training loss 0.8927240303867613\n",
            "2022-03-29 04:54:01.120619 Epoch 38, Training loss 0.8870264258226166\n",
            "2022-03-29 04:54:17.348029 Epoch 39, Training loss 0.8825809098113223\n",
            "2022-03-29 04:54:33.453362 Epoch 40, Training loss 0.8777755876178936\n",
            "2022-03-29 04:54:49.608349 Epoch 41, Training loss 0.8725566612104015\n",
            "2022-03-29 04:55:05.648131 Epoch 42, Training loss 0.8694420047580739\n",
            "2022-03-29 04:55:21.677865 Epoch 43, Training loss 0.8627901143201476\n",
            "2022-03-29 04:55:37.576346 Epoch 44, Training loss 0.8583850465772097\n",
            "2022-03-29 04:55:53.654063 Epoch 45, Training loss 0.8550797089591355\n",
            "2022-03-29 04:56:09.656316 Epoch 46, Training loss 0.8503186014835792\n",
            "2022-03-29 04:56:25.815512 Epoch 47, Training loss 0.8456568033493999\n",
            "2022-03-29 04:56:42.247932 Epoch 48, Training loss 0.8401629247552599\n",
            "2022-03-29 04:56:58.715357 Epoch 49, Training loss 0.8372280460489375\n",
            "2022-03-29 04:57:15.110087 Epoch 50, Training loss 0.8336238739892955\n",
            "2022-03-29 04:57:31.316966 Epoch 51, Training loss 0.8293508624710391\n",
            "2022-03-29 04:57:47.436771 Epoch 52, Training loss 0.8264960185874759\n",
            "2022-03-29 04:58:03.560021 Epoch 53, Training loss 0.8213632603740448\n",
            "2022-03-29 04:58:19.741156 Epoch 54, Training loss 0.8182226753676943\n",
            "2022-03-29 04:58:35.759712 Epoch 55, Training loss 0.8164703217918611\n",
            "2022-03-29 04:58:51.725288 Epoch 56, Training loss 0.813756138086319\n",
            "2022-03-29 04:59:07.667156 Epoch 57, Training loss 0.8101419415848944\n",
            "2022-03-29 04:59:23.743279 Epoch 58, Training loss 0.8061054366476396\n",
            "2022-03-29 04:59:39.870130 Epoch 59, Training loss 0.8010984800203377\n",
            "2022-03-29 04:59:56.045550 Epoch 60, Training loss 0.7984991505399079\n",
            "2022-03-29 05:00:12.214356 Epoch 61, Training loss 0.7957620618059812\n",
            "2022-03-29 05:00:28.442052 Epoch 62, Training loss 0.7933025333811256\n",
            "2022-03-29 05:00:44.664034 Epoch 63, Training loss 0.7895444822890679\n",
            "2022-03-29 05:01:00.785191 Epoch 64, Training loss 0.7865593171180667\n",
            "2022-03-29 05:01:16.990419 Epoch 65, Training loss 0.7829491484653005\n",
            "2022-03-29 05:01:33.413290 Epoch 66, Training loss 0.7814795731202416\n",
            "2022-03-29 05:01:49.731702 Epoch 67, Training loss 0.7781799213431985\n",
            "2022-03-29 05:02:06.157976 Epoch 68, Training loss 0.774701689972597\n",
            "2022-03-29 05:02:22.423843 Epoch 69, Training loss 0.7735352105725452\n",
            "2022-03-29 05:02:38.794065 Epoch 70, Training loss 0.7709909326127727\n",
            "2022-03-29 05:02:55.147945 Epoch 71, Training loss 0.7671134419300977\n",
            "2022-03-29 05:03:11.499643 Epoch 72, Training loss 0.7651188479131444\n",
            "2022-03-29 05:03:27.855662 Epoch 73, Training loss 0.7624215733669603\n",
            "2022-03-29 05:03:44.222196 Epoch 74, Training loss 0.7620118894159337\n",
            "2022-03-29 05:04:00.500132 Epoch 75, Training loss 0.758224579608044\n",
            "2022-03-29 05:04:16.985626 Epoch 76, Training loss 0.7559856083005896\n",
            "2022-03-29 05:04:33.470785 Epoch 77, Training loss 0.7530599034503292\n",
            "2022-03-29 05:04:49.796572 Epoch 78, Training loss 0.7514124558785992\n",
            "2022-03-29 05:05:06.192316 Epoch 79, Training loss 0.7475944198001071\n",
            "2022-03-29 05:05:22.538414 Epoch 80, Training loss 0.7468442001077525\n",
            "2022-03-29 05:05:38.848238 Epoch 81, Training loss 0.7458301747165372\n",
            "2022-03-29 05:05:55.187759 Epoch 82, Training loss 0.7420474498168282\n",
            "2022-03-29 05:06:11.521995 Epoch 83, Training loss 0.7395091111702687\n",
            "2022-03-29 05:06:27.740027 Epoch 84, Training loss 0.7389296510868975\n",
            "2022-03-29 05:06:44.020594 Epoch 85, Training loss 0.7363012990225917\n",
            "2022-03-29 05:07:00.295477 Epoch 86, Training loss 0.7337375617850467\n",
            "2022-03-29 05:07:16.686984 Epoch 87, Training loss 0.7311524321203646\n",
            "2022-03-29 05:07:33.061906 Epoch 88, Training loss 0.7298841092668836\n",
            "2022-03-29 05:07:49.451343 Epoch 89, Training loss 0.7256599239376195\n",
            "2022-03-29 05:08:05.828317 Epoch 90, Training loss 0.7261931668309605\n",
            "2022-03-29 05:08:22.186371 Epoch 91, Training loss 0.7236102866699629\n",
            "2022-03-29 05:08:38.367131 Epoch 92, Training loss 0.722677390898585\n",
            "2022-03-29 05:08:54.456804 Epoch 93, Training loss 0.7207249682730116\n",
            "2022-03-29 05:09:10.619273 Epoch 94, Training loss 0.7184768036350875\n",
            "2022-03-29 05:09:26.924395 Epoch 95, Training loss 0.7165889101641257\n",
            "2022-03-29 05:09:43.060503 Epoch 96, Training loss 0.7152631839218042\n",
            "2022-03-29 05:09:59.216285 Epoch 97, Training loss 0.7126118961883627\n",
            "2022-03-29 05:10:15.469308 Epoch 98, Training loss 0.7101365947891074\n",
            "2022-03-29 05:10:31.759729 Epoch 99, Training loss 0.7079432414239629\n",
            "2022-03-29 05:10:48.063360 Epoch 100, Training loss 0.7068493322795614\n",
            "2022-03-29 05:11:04.418476 Epoch 101, Training loss 0.7042073158885512\n",
            "2022-03-29 05:11:20.856285 Epoch 102, Training loss 0.7033774850847166\n",
            "2022-03-29 05:11:37.208417 Epoch 103, Training loss 0.7010753852174715\n",
            "2022-03-29 05:11:53.521291 Epoch 104, Training loss 0.7004518805409942\n",
            "2022-03-29 05:12:09.939656 Epoch 105, Training loss 0.6983554868213356\n",
            "2022-03-29 05:12:26.340926 Epoch 106, Training loss 0.6975895401157076\n",
            "2022-03-29 05:12:42.832253 Epoch 107, Training loss 0.6949667753008626\n",
            "2022-03-29 05:12:59.269283 Epoch 108, Training loss 0.6944683361083955\n",
            "2022-03-29 05:13:15.746763 Epoch 109, Training loss 0.6921649193748489\n",
            "2022-03-29 05:13:32.203806 Epoch 110, Training loss 0.6913974325522743\n",
            "2022-03-29 05:13:48.619939 Epoch 111, Training loss 0.6892339372269028\n",
            "2022-03-29 05:14:04.971653 Epoch 112, Training loss 0.6873663065149961\n",
            "2022-03-29 05:14:21.389775 Epoch 113, Training loss 0.6857754708555959\n",
            "2022-03-29 05:14:37.712599 Epoch 114, Training loss 0.6843395745738998\n",
            "2022-03-29 05:14:54.050575 Epoch 115, Training loss 0.6821757610267996\n",
            "2022-03-29 05:15:10.366049 Epoch 116, Training loss 0.6809194871150624\n",
            "2022-03-29 05:15:26.686725 Epoch 117, Training loss 0.6802813976698214\n",
            "2022-03-29 05:15:42.966501 Epoch 118, Training loss 0.6793984975427618\n",
            "2022-03-29 05:15:59.296655 Epoch 119, Training loss 0.6775040169201239\n",
            "2022-03-29 05:16:15.471908 Epoch 120, Training loss 0.674044982780276\n",
            "2022-03-29 05:16:31.517647 Epoch 121, Training loss 0.6749519488924299\n",
            "2022-03-29 05:16:47.606049 Epoch 122, Training loss 0.6733487541108485\n",
            "2022-03-29 05:17:03.647727 Epoch 123, Training loss 0.671272263121422\n",
            "2022-03-29 05:17:19.702097 Epoch 124, Training loss 0.6704850475044202\n",
            "2022-03-29 05:17:35.853652 Epoch 125, Training loss 0.6690996259527133\n",
            "2022-03-29 05:17:52.021799 Epoch 126, Training loss 0.6680172080617122\n",
            "2022-03-29 05:18:08.037349 Epoch 127, Training loss 0.6677329477751651\n",
            "2022-03-29 05:18:24.190863 Epoch 128, Training loss 0.665081941258267\n",
            "2022-03-29 05:18:40.253606 Epoch 129, Training loss 0.6628265112181149\n",
            "2022-03-29 05:18:56.426214 Epoch 130, Training loss 0.661802287411202\n",
            "2022-03-29 05:19:12.456304 Epoch 131, Training loss 0.6612380690053296\n",
            "2022-03-29 05:19:28.446199 Epoch 132, Training loss 0.6594282483200893\n",
            "2022-03-29 05:19:44.456573 Epoch 133, Training loss 0.6575920630598922\n",
            "2022-03-29 05:20:00.410742 Epoch 134, Training loss 0.6563628097171978\n",
            "2022-03-29 05:20:16.528157 Epoch 135, Training loss 0.6555610448884233\n",
            "2022-03-29 05:20:32.657211 Epoch 136, Training loss 0.6530215984110332\n",
            "2022-03-29 05:20:48.735671 Epoch 137, Training loss 0.6545678715571723\n",
            "2022-03-29 05:21:04.839201 Epoch 138, Training loss 0.6511830487824461\n",
            "2022-03-29 05:21:20.902755 Epoch 139, Training loss 0.6506876624987253\n",
            "2022-03-29 05:21:36.937062 Epoch 140, Training loss 0.6511643521316216\n",
            "2022-03-29 05:21:53.108963 Epoch 141, Training loss 0.6479902057848927\n",
            "2022-03-29 05:22:09.214874 Epoch 142, Training loss 0.6464533220662181\n",
            "2022-03-29 05:22:25.455116 Epoch 143, Training loss 0.6460484148520033\n",
            "2022-03-29 05:22:41.705958 Epoch 144, Training loss 0.6438361210057802\n",
            "2022-03-29 05:22:57.795727 Epoch 145, Training loss 0.6438819955453239\n",
            "2022-03-29 05:23:13.870513 Epoch 146, Training loss 0.6428414567199814\n",
            "2022-03-29 05:23:29.983650 Epoch 147, Training loss 0.6409613485912533\n",
            "2022-03-29 05:23:46.161258 Epoch 148, Training loss 0.6416370103045193\n",
            "2022-03-29 05:24:02.229203 Epoch 149, Training loss 0.639247388257395\n",
            "2022-03-29 05:24:18.287381 Epoch 150, Training loss 0.6376032215135786\n",
            "2022-03-29 05:24:34.379023 Epoch 151, Training loss 0.6372024007236866\n",
            "2022-03-29 05:24:50.485346 Epoch 152, Training loss 0.6348268769281294\n",
            "2022-03-29 05:25:06.497002 Epoch 153, Training loss 0.6343780055908901\n",
            "2022-03-29 05:25:22.687432 Epoch 154, Training loss 0.6337571932607905\n",
            "2022-03-29 05:25:38.702976 Epoch 155, Training loss 0.6308617931802559\n",
            "2022-03-29 05:25:54.814919 Epoch 156, Training loss 0.6329276029716062\n",
            "2022-03-29 05:26:10.959511 Epoch 157, Training loss 0.630887910609355\n",
            "2022-03-29 05:26:27.117345 Epoch 158, Training loss 0.6293063723217801\n",
            "2022-03-29 05:26:43.141965 Epoch 159, Training loss 0.6294451465692057\n",
            "2022-03-29 05:26:59.157347 Epoch 160, Training loss 0.626173249767412\n",
            "2022-03-29 05:27:15.255347 Epoch 161, Training loss 0.6252308866328291\n",
            "2022-03-29 05:27:31.187492 Epoch 162, Training loss 0.6256789554034352\n",
            "2022-03-29 05:27:47.320684 Epoch 163, Training loss 0.6237681304554805\n",
            "2022-03-29 05:28:03.377052 Epoch 164, Training loss 0.622890898829226\n",
            "2022-03-29 05:28:19.499390 Epoch 165, Training loss 0.6215645253963178\n",
            "2022-03-29 05:28:35.531413 Epoch 166, Training loss 0.6214041505628229\n",
            "2022-03-29 05:28:51.787685 Epoch 167, Training loss 0.6207436998100841\n",
            "2022-03-29 05:29:08.101760 Epoch 168, Training loss 0.6188998521898713\n",
            "2022-03-29 05:29:24.514632 Epoch 169, Training loss 0.6184129768320362\n",
            "2022-03-29 05:29:40.866583 Epoch 170, Training loss 0.6176199038772632\n",
            "2022-03-29 05:29:57.114353 Epoch 171, Training loss 0.6175713888214677\n",
            "2022-03-29 05:30:13.146666 Epoch 172, Training loss 0.6166385512446504\n",
            "2022-03-29 05:30:29.234584 Epoch 173, Training loss 0.6158214133337635\n",
            "2022-03-29 05:30:45.418376 Epoch 174, Training loss 0.6133546421823599\n",
            "2022-03-29 05:31:01.494542 Epoch 175, Training loss 0.6126716159798605\n",
            "2022-03-29 05:31:17.516209 Epoch 176, Training loss 0.6112935559447769\n",
            "2022-03-29 05:31:33.610674 Epoch 177, Training loss 0.6096830413774457\n",
            "2022-03-29 05:31:49.739009 Epoch 178, Training loss 0.6101785112372444\n",
            "2022-03-29 05:32:05.787317 Epoch 179, Training loss 0.6093615380394489\n",
            "2022-03-29 05:32:21.927724 Epoch 180, Training loss 0.6093639831637483\n",
            "2022-03-29 05:32:38.068341 Epoch 181, Training loss 0.6085558655621756\n",
            "2022-03-29 05:32:54.189526 Epoch 182, Training loss 0.6040799456560398\n",
            "2022-03-29 05:33:10.332434 Epoch 183, Training loss 0.6052494557464824\n",
            "2022-03-29 05:33:26.530624 Epoch 184, Training loss 0.6057535899264733\n",
            "2022-03-29 05:33:42.485823 Epoch 185, Training loss 0.6033020370146808\n",
            "2022-03-29 05:33:58.563049 Epoch 186, Training loss 0.6031048941185407\n",
            "2022-03-29 05:34:14.515393 Epoch 187, Training loss 0.601732166869866\n",
            "2022-03-29 05:34:30.620482 Epoch 188, Training loss 0.601836519358713\n",
            "2022-03-29 05:34:46.659798 Epoch 189, Training loss 0.59973214802992\n",
            "2022-03-29 05:35:02.928655 Epoch 190, Training loss 0.6000208354667019\n",
            "2022-03-29 05:35:19.188494 Epoch 191, Training loss 0.5999509330524508\n",
            "2022-03-29 05:35:35.436675 Epoch 192, Training loss 0.5991985428211329\n",
            "2022-03-29 05:35:51.639259 Epoch 193, Training loss 0.5971212307434253\n",
            "2022-03-29 05:36:07.706111 Epoch 194, Training loss 0.5994096344617932\n",
            "2022-03-29 05:36:23.743776 Epoch 195, Training loss 0.5950735055881998\n",
            "2022-03-29 05:36:39.823846 Epoch 196, Training loss 0.5961785111433405\n",
            "2022-03-29 05:36:55.819530 Epoch 197, Training loss 0.5939326539368885\n",
            "2022-03-29 05:37:11.845541 Epoch 198, Training loss 0.5926487493469282\n",
            "2022-03-29 05:37:28.086380 Epoch 199, Training loss 0.5929422341572964\n",
            "2022-03-29 05:37:44.106796 Epoch 200, Training loss 0.5931231443915526\n",
            "2022-03-29 05:38:00.113996 Epoch 201, Training loss 0.5915830689470482\n",
            "2022-03-29 05:38:16.135141 Epoch 202, Training loss 0.5925029303754688\n",
            "2022-03-29 05:38:32.133376 Epoch 203, Training loss 0.5900381991015676\n",
            "2022-03-29 05:38:48.213617 Epoch 204, Training loss 0.5896815969358624\n",
            "2022-03-29 05:39:04.311751 Epoch 205, Training loss 0.5893683549769394\n",
            "2022-03-29 05:39:20.207990 Epoch 206, Training loss 0.5876794257356078\n",
            "2022-03-29 05:39:36.216644 Epoch 207, Training loss 0.5883955053813622\n",
            "2022-03-29 05:39:52.405756 Epoch 208, Training loss 0.5880877391990188\n",
            "2022-03-29 05:40:08.634349 Epoch 209, Training loss 0.5850999141516893\n",
            "2022-03-29 05:40:24.804732 Epoch 210, Training loss 0.5843361791061319\n",
            "2022-03-29 05:40:40.982028 Epoch 211, Training loss 0.5852850006364495\n",
            "2022-03-29 05:40:57.196278 Epoch 212, Training loss 0.5833929780765873\n",
            "2022-03-29 05:41:13.479314 Epoch 213, Training loss 0.5825844594203603\n",
            "2022-03-29 05:41:29.743034 Epoch 214, Training loss 0.5820154455845313\n",
            "2022-03-29 05:41:45.993389 Epoch 215, Training loss 0.5822297081236949\n",
            "2022-03-29 05:42:02.233400 Epoch 216, Training loss 0.5809567806970738\n",
            "2022-03-29 05:42:18.338782 Epoch 217, Training loss 0.5802055150270462\n",
            "2022-03-29 05:42:34.399547 Epoch 218, Training loss 0.5793245681716354\n",
            "2022-03-29 05:42:50.435410 Epoch 219, Training loss 0.5788457201950995\n",
            "2022-03-29 05:43:06.544666 Epoch 220, Training loss 0.5793919356354057\n",
            "2022-03-29 05:43:22.653640 Epoch 221, Training loss 0.5777945587854556\n",
            "2022-03-29 05:43:38.720293 Epoch 222, Training loss 0.5760615874281929\n",
            "2022-03-29 05:43:54.842643 Epoch 223, Training loss 0.5773381447167043\n",
            "2022-03-29 05:44:10.925067 Epoch 224, Training loss 0.5762601769565011\n",
            "2022-03-29 05:44:26.946253 Epoch 225, Training loss 0.5771259006560611\n",
            "2022-03-29 05:44:43.032335 Epoch 226, Training loss 0.574509388764801\n",
            "2022-03-29 05:44:59.197569 Epoch 227, Training loss 0.5761210400887462\n",
            "2022-03-29 05:45:15.510501 Epoch 228, Training loss 0.573408106365777\n",
            "2022-03-29 05:45:31.888448 Epoch 229, Training loss 0.5739665261619841\n",
            "2022-03-29 05:45:48.213769 Epoch 230, Training loss 0.5732763382556189\n",
            "2022-03-29 05:46:04.604953 Epoch 231, Training loss 0.571650506056788\n",
            "2022-03-29 05:46:20.845319 Epoch 232, Training loss 0.5730152896336277\n",
            "2022-03-29 05:46:37.115541 Epoch 233, Training loss 0.5704127281828\n",
            "2022-03-29 05:46:53.172184 Epoch 234, Training loss 0.5702229269096614\n",
            "2022-03-29 05:47:09.406281 Epoch 235, Training loss 0.5684887075515659\n",
            "2022-03-29 05:47:25.684814 Epoch 236, Training loss 0.5682201350436491\n",
            "2022-03-29 05:47:41.955706 Epoch 237, Training loss 0.5684937039375915\n",
            "2022-03-29 05:47:58.215829 Epoch 238, Training loss 0.5682620364229393\n",
            "2022-03-29 05:48:14.318140 Epoch 239, Training loss 0.5646616827953806\n",
            "2022-03-29 05:48:30.490021 Epoch 240, Training loss 0.5663297228572314\n",
            "2022-03-29 05:48:46.683819 Epoch 241, Training loss 0.5669150487388797\n",
            "2022-03-29 05:49:02.935714 Epoch 242, Training loss 0.5647468808895487\n",
            "2022-03-29 05:49:19.202445 Epoch 243, Training loss 0.564939545014935\n",
            "2022-03-29 05:49:35.274191 Epoch 244, Training loss 0.5638445688940376\n",
            "2022-03-29 05:49:51.452371 Epoch 245, Training loss 0.5623786911139708\n",
            "2022-03-29 05:50:07.693791 Epoch 246, Training loss 0.5635920532447908\n",
            "2022-03-29 05:50:23.894362 Epoch 247, Training loss 0.5614409918904\n",
            "2022-03-29 05:50:40.042547 Epoch 248, Training loss 0.5628163662103131\n",
            "2022-03-29 05:50:56.270880 Epoch 249, Training loss 0.5607632660423704\n",
            "2022-03-29 05:51:12.537540 Epoch 250, Training loss 0.561782169734578\n",
            "2022-03-29 05:51:28.877567 Epoch 251, Training loss 0.5597875880844453\n",
            "2022-03-29 05:51:45.117232 Epoch 252, Training loss 0.559905415110271\n",
            "2022-03-29 05:52:01.277122 Epoch 253, Training loss 0.5597169877737379\n",
            "2022-03-29 05:52:17.455724 Epoch 254, Training loss 0.5582519879807597\n",
            "2022-03-29 05:52:33.685264 Epoch 255, Training loss 0.5581415732345922\n",
            "2022-03-29 05:52:49.855193 Epoch 256, Training loss 0.5568709260667376\n",
            "2022-03-29 05:53:05.975169 Epoch 257, Training loss 0.5573793476271203\n",
            "2022-03-29 05:53:22.056290 Epoch 258, Training loss 0.5588511617287345\n",
            "2022-03-29 05:53:38.077305 Epoch 259, Training loss 0.5561833486456396\n",
            "2022-03-29 05:53:54.263310 Epoch 260, Training loss 0.5568616594881048\n",
            "2022-03-29 05:54:10.361367 Epoch 261, Training loss 0.5541683801299776\n",
            "2022-03-29 05:54:26.548321 Epoch 262, Training loss 0.5554663963101404\n",
            "2022-03-29 05:54:42.774011 Epoch 263, Training loss 0.5567390577643728\n",
            "2022-03-29 05:54:58.993755 Epoch 264, Training loss 0.552203243383971\n",
            "2022-03-29 05:55:15.174742 Epoch 265, Training loss 0.5535541249967902\n",
            "2022-03-29 05:55:31.299695 Epoch 266, Training loss 0.5533432598271029\n",
            "2022-03-29 05:55:47.486214 Epoch 267, Training loss 0.5528468256411345\n",
            "2022-03-29 05:56:03.484897 Epoch 268, Training loss 0.551628080989851\n",
            "2022-03-29 05:56:19.605460 Epoch 269, Training loss 0.5517893302471132\n",
            "2022-03-29 05:56:35.745257 Epoch 270, Training loss 0.5508224909072337\n",
            "2022-03-29 05:56:51.928130 Epoch 271, Training loss 0.550318009046185\n",
            "2022-03-29 05:57:08.052271 Epoch 272, Training loss 0.549383996316539\n",
            "2022-03-29 05:57:24.343829 Epoch 273, Training loss 0.5505934296666509\n",
            "2022-03-29 05:57:40.547338 Epoch 274, Training loss 0.5479320099058054\n",
            "2022-03-29 05:57:56.654853 Epoch 275, Training loss 0.5486197203702634\n",
            "2022-03-29 05:58:12.695719 Epoch 276, Training loss 0.5487419070718843\n",
            "2022-03-29 05:58:28.770874 Epoch 277, Training loss 0.5481776609597608\n",
            "2022-03-29 05:58:44.986519 Epoch 278, Training loss 0.5477987147505631\n",
            "2022-03-29 05:59:01.174360 Epoch 279, Training loss 0.5484573876156527\n",
            "2022-03-29 05:59:17.433993 Epoch 280, Training loss 0.5467811137285379\n",
            "2022-03-29 05:59:33.538126 Epoch 281, Training loss 0.5468302696867062\n",
            "2022-03-29 05:59:49.616379 Epoch 282, Training loss 0.5471786831689003\n",
            "2022-03-29 06:00:05.767420 Epoch 283, Training loss 0.544730999128288\n",
            "2022-03-29 06:00:21.902019 Epoch 284, Training loss 0.5428585512635044\n",
            "2022-03-29 06:00:38.023452 Epoch 285, Training loss 0.5427479487855721\n",
            "2022-03-29 06:00:54.413099 Epoch 286, Training loss 0.5445341820378438\n",
            "2022-03-29 06:01:10.846222 Epoch 287, Training loss 0.5450218313795221\n",
            "2022-03-29 06:01:27.284173 Epoch 288, Training loss 0.5419838310355116\n",
            "2022-03-29 06:01:43.692445 Epoch 289, Training loss 0.5409448249527561\n",
            "2022-03-29 06:02:00.135070 Epoch 290, Training loss 0.5410751961838559\n",
            "2022-03-29 06:02:16.534394 Epoch 291, Training loss 0.5388203115795579\n",
            "2022-03-29 06:02:32.890203 Epoch 292, Training loss 0.5426470129310018\n",
            "2022-03-29 06:02:49.289490 Epoch 293, Training loss 0.5402962298458799\n",
            "2022-03-29 06:03:05.617042 Epoch 294, Training loss 0.5409918176319898\n",
            "2022-03-29 06:03:21.854583 Epoch 295, Training loss 0.5388028282872246\n",
            "2022-03-29 06:03:38.040983 Epoch 296, Training loss 0.5398437237305105\n",
            "2022-03-29 06:03:54.287460 Epoch 297, Training loss 0.5371148009281939\n",
            "2022-03-29 06:04:10.494629 Epoch 298, Training loss 0.5383988131037758\n",
            "2022-03-29 06:04:26.734491 Epoch 299, Training loss 0.539555762201319\n",
            "2022-03-29 06:04:42.855158 Epoch 300, Training loss 0.5370135453274792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plHXrzAf-lIh",
        "outputId": "c3bc3e53-2bcb-4d82-ecb8-3abb6a3fb381"
      },
      "id": "plHXrzAf-lIh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.81\n",
            "Accuracy val: 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################################################################\n",
        "#########Problem 2 PART 1#########\n",
        "##############################################################################################################################"
      ],
      "metadata": {
        "id": "lFNmsT_3nmy-"
      },
      "id": "lFNmsT_3nmy-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetRes(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
        "        out1 = out\n",
        "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
        "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "AfyMoNCOnpr5"
      },
      "id": "AfyMoNCOnpr5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "model = NetRes(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwuuv38Jwvd7",
        "outputId": "3b06f713-70ff-4fcb-d965-d5d3c0fd0592"
      },
      "id": "Uwuuv38Jwvd7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-30 14:56:48.653678 Epoch 1, Training loss 2.118773611915081\n",
            "2022-03-30 14:56:59.732190 Epoch 2, Training loss 1.7503244863141834\n",
            "2022-03-30 14:57:10.799833 Epoch 3, Training loss 1.5545217981728752\n",
            "2022-03-30 14:57:21.884237 Epoch 4, Training loss 1.4413532382996797\n",
            "2022-03-30 14:57:32.987184 Epoch 5, Training loss 1.3589966778864946\n",
            "2022-03-30 14:57:43.981476 Epoch 6, Training loss 1.2896170318126678\n",
            "2022-03-30 14:57:55.056163 Epoch 7, Training loss 1.2293713762022345\n",
            "2022-03-30 14:58:06.258686 Epoch 8, Training loss 1.180406075151985\n",
            "2022-03-30 14:58:17.535626 Epoch 9, Training loss 1.1376883311344839\n",
            "2022-03-30 14:58:28.579210 Epoch 10, Training loss 1.0984396485568921\n",
            "2022-03-30 14:58:39.589001 Epoch 11, Training loss 1.0637750482315298\n",
            "2022-03-30 14:58:50.588554 Epoch 12, Training loss 1.034006581205846\n",
            "2022-03-30 14:59:01.586133 Epoch 13, Training loss 1.00559051735017\n",
            "2022-03-30 14:59:12.436030 Epoch 14, Training loss 0.9804429912658603\n",
            "2022-03-30 14:59:23.493625 Epoch 15, Training loss 0.9606560496875393\n",
            "2022-03-30 14:59:34.460573 Epoch 16, Training loss 0.9414867086483695\n",
            "2022-03-30 14:59:45.386743 Epoch 17, Training loss 0.9202986186575097\n",
            "2022-03-30 14:59:56.301743 Epoch 18, Training loss 0.906030728765156\n",
            "2022-03-30 15:00:07.325005 Epoch 19, Training loss 0.8909446919513175\n",
            "2022-03-30 15:00:18.233233 Epoch 20, Training loss 0.8786013121037837\n",
            "2022-03-30 15:00:29.576311 Epoch 21, Training loss 0.8657008769262172\n",
            "2022-03-30 15:00:40.807120 Epoch 22, Training loss 0.8546449686865063\n",
            "2022-03-30 15:00:51.999426 Epoch 23, Training loss 0.8469206514718283\n",
            "2022-03-30 15:01:03.136457 Epoch 24, Training loss 0.83377184347271\n",
            "2022-03-30 15:01:14.353072 Epoch 25, Training loss 0.8233626543560906\n",
            "2022-03-30 15:01:25.477599 Epoch 26, Training loss 0.8129425452036017\n",
            "2022-03-30 15:01:36.680657 Epoch 27, Training loss 0.8074885737484373\n",
            "2022-03-30 15:01:47.855700 Epoch 28, Training loss 0.8019598277709673\n",
            "2022-03-30 15:01:59.190579 Epoch 29, Training loss 0.7942344972392177\n",
            "2022-03-30 15:02:10.265281 Epoch 30, Training loss 0.7817112526015553\n",
            "2022-03-30 15:02:21.344459 Epoch 31, Training loss 0.7764894690964838\n",
            "2022-03-30 15:02:32.554715 Epoch 32, Training loss 0.7712509962527648\n",
            "2022-03-30 15:02:43.697700 Epoch 33, Training loss 0.7665930837773911\n",
            "2022-03-30 15:02:54.736251 Epoch 34, Training loss 0.7599880527276213\n",
            "2022-03-30 15:03:06.061257 Epoch 35, Training loss 0.7539466165215768\n",
            "2022-03-30 15:03:17.093969 Epoch 36, Training loss 0.7494864399399599\n",
            "2022-03-30 15:03:28.257615 Epoch 37, Training loss 0.7437976384940355\n",
            "2022-03-30 15:03:39.371032 Epoch 38, Training loss 0.7400852395293048\n",
            "2022-03-30 15:03:50.415480 Epoch 39, Training loss 0.7323405106201806\n",
            "2022-03-30 15:04:01.350094 Epoch 40, Training loss 0.7275295770153061\n",
            "2022-03-30 15:04:12.416143 Epoch 41, Training loss 0.7233181361804533\n",
            "2022-03-30 15:04:23.433728 Epoch 42, Training loss 0.7209753567148047\n",
            "2022-03-30 15:04:34.624781 Epoch 43, Training loss 0.7133496946767163\n",
            "2022-03-30 15:04:45.862508 Epoch 44, Training loss 0.7090958898024791\n",
            "2022-03-30 15:04:56.930527 Epoch 45, Training loss 0.7072001125120446\n",
            "2022-03-30 15:05:09.196214 Epoch 46, Training loss 0.7008611272515544\n",
            "2022-03-30 15:05:20.318082 Epoch 47, Training loss 0.6984078295700386\n",
            "2022-03-30 15:05:31.353560 Epoch 48, Training loss 0.6952729395131017\n",
            "2022-03-30 15:05:42.474122 Epoch 49, Training loss 0.6891982193721835\n",
            "2022-03-30 15:05:53.409145 Epoch 50, Training loss 0.6850337057238649\n",
            "2022-03-30 15:06:04.315678 Epoch 51, Training loss 0.683263395753358\n",
            "2022-03-30 15:06:15.308884 Epoch 52, Training loss 0.6783122840287436\n",
            "2022-03-30 15:06:26.312892 Epoch 53, Training loss 0.6762096203882676\n",
            "2022-03-30 15:06:37.451548 Epoch 54, Training loss 0.6704352251861406\n",
            "2022-03-30 15:06:48.542838 Epoch 55, Training loss 0.6694576023408519\n",
            "2022-03-30 15:06:59.384434 Epoch 56, Training loss 0.6637631957716954\n",
            "2022-03-30 15:07:10.470068 Epoch 57, Training loss 0.6609728534508239\n",
            "2022-03-30 15:07:21.685284 Epoch 58, Training loss 0.6571571895914614\n",
            "2022-03-30 15:07:32.716202 Epoch 59, Training loss 0.6549206197338031\n",
            "2022-03-30 15:07:43.753298 Epoch 60, Training loss 0.6509967686803749\n",
            "2022-03-30 15:07:54.783990 Epoch 61, Training loss 0.6502219287254621\n",
            "2022-03-30 15:08:05.768174 Epoch 62, Training loss 0.6446007296176213\n",
            "2022-03-30 15:08:16.873612 Epoch 63, Training loss 0.6407300718605061\n",
            "2022-03-30 15:08:27.894933 Epoch 64, Training loss 0.6383016051919869\n",
            "2022-03-30 15:08:38.989679 Epoch 65, Training loss 0.6370272739692722\n",
            "2022-03-30 15:08:50.146091 Epoch 66, Training loss 0.6322703903822033\n",
            "2022-03-30 15:09:01.077391 Epoch 67, Training loss 0.6303918932557411\n",
            "2022-03-30 15:09:12.006553 Epoch 68, Training loss 0.626948209789098\n",
            "2022-03-30 15:09:22.878199 Epoch 69, Training loss 0.6271358498222078\n",
            "2022-03-30 15:09:33.720884 Epoch 70, Training loss 0.6228158662996024\n",
            "2022-03-30 15:09:44.727353 Epoch 71, Training loss 0.6175048470954456\n",
            "2022-03-30 15:09:55.581009 Epoch 72, Training loss 0.6166377479158094\n",
            "2022-03-30 15:10:06.410522 Epoch 73, Training loss 0.6153290785487046\n",
            "2022-03-30 15:10:17.318899 Epoch 74, Training loss 0.6090281739106873\n",
            "2022-03-30 15:10:28.259080 Epoch 75, Training loss 0.6097492148046908\n",
            "2022-03-30 15:10:39.226238 Epoch 76, Training loss 0.6062295513079904\n",
            "2022-03-30 15:10:50.134548 Epoch 77, Training loss 0.6061411221771289\n",
            "2022-03-30 15:11:00.939786 Epoch 78, Training loss 0.6038710573292754\n",
            "2022-03-30 15:11:11.792339 Epoch 79, Training loss 0.5991707745644138\n",
            "2022-03-30 15:11:22.637893 Epoch 80, Training loss 0.5978181346526841\n",
            "2022-03-30 15:11:33.569004 Epoch 81, Training loss 0.5947306451895048\n",
            "2022-03-30 15:11:44.443878 Epoch 82, Training loss 0.5926392013993105\n",
            "2022-03-30 15:11:55.414098 Epoch 83, Training loss 0.5906372314218975\n",
            "2022-03-30 15:12:06.418497 Epoch 84, Training loss 0.5868694292919715\n",
            "2022-03-30 15:12:17.285003 Epoch 85, Training loss 0.5826910506467076\n",
            "2022-03-30 15:12:28.129777 Epoch 86, Training loss 0.5825334746971764\n",
            "2022-03-30 15:12:39.080031 Epoch 87, Training loss 0.5799855024689604\n",
            "2022-03-30 15:12:50.286774 Epoch 88, Training loss 0.5801700477481193\n",
            "2022-03-30 15:13:01.220421 Epoch 89, Training loss 0.5767086303371298\n",
            "2022-03-30 15:13:12.136970 Epoch 90, Training loss 0.5768162899691126\n",
            "2022-03-30 15:13:23.146245 Epoch 91, Training loss 0.5734181473855777\n",
            "2022-03-30 15:13:34.038456 Epoch 92, Training loss 0.5696832796992244\n",
            "2022-03-30 15:13:45.002477 Epoch 93, Training loss 0.5666767947585382\n",
            "2022-03-30 15:13:55.942469 Epoch 94, Training loss 0.5690761014933476\n",
            "2022-03-30 15:14:06.937755 Epoch 95, Training loss 0.5665140563188611\n",
            "2022-03-30 15:14:17.804669 Epoch 96, Training loss 0.5605231926150029\n",
            "2022-03-30 15:14:28.706450 Epoch 97, Training loss 0.5623006741790211\n",
            "2022-03-30 15:14:39.722316 Epoch 98, Training loss 0.5586738145869711\n",
            "2022-03-30 15:14:50.672315 Epoch 99, Training loss 0.5563509542390209\n",
            "2022-03-30 15:15:01.529433 Epoch 100, Training loss 0.5547352012465981\n",
            "2022-03-30 15:15:12.407329 Epoch 101, Training loss 0.5567993900126509\n",
            "2022-03-30 15:15:23.352286 Epoch 102, Training loss 0.5532470455826701\n",
            "2022-03-30 15:15:34.229757 Epoch 103, Training loss 0.5505331079368396\n",
            "2022-03-30 15:15:45.109187 Epoch 104, Training loss 0.5492302125219799\n",
            "2022-03-30 15:15:55.929113 Epoch 105, Training loss 0.5468060968972533\n",
            "2022-03-30 15:16:06.776664 Epoch 106, Training loss 0.5459140317747965\n",
            "2022-03-30 15:16:17.625458 Epoch 107, Training loss 0.5453130624559529\n",
            "2022-03-30 15:16:28.406476 Epoch 108, Training loss 0.543751213823438\n",
            "2022-03-30 15:16:39.274975 Epoch 109, Training loss 0.5420519996177205\n",
            "2022-03-30 15:16:50.259897 Epoch 110, Training loss 0.5388397763071158\n",
            "2022-03-30 15:17:01.236701 Epoch 111, Training loss 0.5380001382335372\n",
            "2022-03-30 15:17:11.992770 Epoch 112, Training loss 0.5353419974141413\n",
            "2022-03-30 15:17:22.880520 Epoch 113, Training loss 0.5365340115545351\n",
            "2022-03-30 15:17:33.726874 Epoch 114, Training loss 0.534332173578727\n",
            "2022-03-30 15:17:44.604969 Epoch 115, Training loss 0.5306421275181539\n",
            "2022-03-30 15:17:55.644203 Epoch 116, Training loss 0.5306580975994735\n",
            "2022-03-30 15:18:06.729855 Epoch 117, Training loss 0.5297476486743563\n",
            "2022-03-30 15:18:17.677868 Epoch 118, Training loss 0.526362227928608\n",
            "2022-03-30 15:18:28.671279 Epoch 119, Training loss 0.525345313770082\n",
            "2022-03-30 15:18:39.524044 Epoch 120, Training loss 0.52429675394693\n",
            "2022-03-30 15:18:50.647322 Epoch 121, Training loss 0.5250685135155078\n",
            "2022-03-30 15:19:01.490875 Epoch 122, Training loss 0.5209017728105225\n",
            "2022-03-30 15:19:12.321242 Epoch 123, Training loss 0.5205220824388592\n",
            "2022-03-30 15:19:23.226531 Epoch 124, Training loss 0.5174020377685652\n",
            "2022-03-30 15:19:34.057823 Epoch 125, Training loss 0.5184927915825563\n",
            "2022-03-30 15:19:44.921720 Epoch 126, Training loss 0.5156420591999503\n",
            "2022-03-30 15:19:55.822156 Epoch 127, Training loss 0.5145645917814863\n",
            "2022-03-30 15:20:06.755285 Epoch 128, Training loss 0.5118953644695794\n",
            "2022-03-30 15:20:17.678954 Epoch 129, Training loss 0.514681896895094\n",
            "2022-03-30 15:20:28.455404 Epoch 130, Training loss 0.5110891578561815\n",
            "2022-03-30 15:20:39.352950 Epoch 131, Training loss 0.5115310431593825\n",
            "2022-03-30 15:20:50.216916 Epoch 132, Training loss 0.5075635847723697\n",
            "2022-03-30 15:21:01.159231 Epoch 133, Training loss 0.5057346445062886\n",
            "2022-03-30 15:21:12.034735 Epoch 134, Training loss 0.5074940892817724\n",
            "2022-03-30 15:21:23.085064 Epoch 135, Training loss 0.5056312002070115\n",
            "2022-03-30 15:21:33.933191 Epoch 136, Training loss 0.5041699914638039\n",
            "2022-03-30 15:21:44.743716 Epoch 137, Training loss 0.5023256753526075\n",
            "2022-03-30 15:21:55.605758 Epoch 138, Training loss 0.49966210176420334\n",
            "2022-03-30 15:22:06.525076 Epoch 139, Training loss 0.5017446729799976\n",
            "2022-03-30 15:22:17.517424 Epoch 140, Training loss 0.5008128264447307\n",
            "2022-03-30 15:22:28.465611 Epoch 141, Training loss 0.49660163365132975\n",
            "2022-03-30 15:22:39.342308 Epoch 142, Training loss 0.49868984526151894\n",
            "2022-03-30 15:22:50.295691 Epoch 143, Training loss 0.49834863321326883\n",
            "2022-03-30 15:23:01.203659 Epoch 144, Training loss 0.4953119467438944\n",
            "2022-03-30 15:23:12.058858 Epoch 145, Training loss 0.4936095202708488\n",
            "2022-03-30 15:23:22.980109 Epoch 146, Training loss 0.4942217179385902\n",
            "2022-03-30 15:23:33.925319 Epoch 147, Training loss 0.490988834834922\n",
            "2022-03-30 15:23:44.911236 Epoch 148, Training loss 0.49263167482279147\n",
            "2022-03-30 15:23:55.951063 Epoch 149, Training loss 0.48943269576715387\n",
            "2022-03-30 15:24:06.940766 Epoch 150, Training loss 0.48841089930604487\n",
            "2022-03-30 15:24:17.792006 Epoch 151, Training loss 0.4868215649481625\n",
            "2022-03-30 15:24:28.667909 Epoch 152, Training loss 0.4849152695530516\n",
            "2022-03-30 15:24:39.637101 Epoch 153, Training loss 0.4835874158746141\n",
            "2022-03-30 15:24:50.911242 Epoch 154, Training loss 0.4850989770896904\n",
            "2022-03-30 15:25:01.932550 Epoch 155, Training loss 0.4829429281718286\n",
            "2022-03-30 15:25:12.984948 Epoch 156, Training loss 0.4800314438122008\n",
            "2022-03-30 15:25:23.954164 Epoch 157, Training loss 0.4792122710353273\n",
            "2022-03-30 15:25:34.841177 Epoch 158, Training loss 0.4832553399531433\n",
            "2022-03-30 15:25:45.633814 Epoch 159, Training loss 0.4793611015467083\n",
            "2022-03-30 15:25:56.534692 Epoch 160, Training loss 0.479492769178832\n",
            "2022-03-30 15:26:07.390589 Epoch 161, Training loss 0.4754373475318522\n",
            "2022-03-30 15:26:18.263341 Epoch 162, Training loss 0.47635048361080684\n",
            "2022-03-30 15:26:29.264799 Epoch 163, Training loss 0.47499521292002916\n",
            "2022-03-30 15:26:40.134748 Epoch 164, Training loss 0.47649312046025416\n",
            "2022-03-30 15:26:51.054065 Epoch 165, Training loss 0.473335251993383\n",
            "2022-03-30 15:27:01.991515 Epoch 166, Training loss 0.47051277068798497\n",
            "2022-03-30 15:27:12.896901 Epoch 167, Training loss 0.472720676885389\n",
            "2022-03-30 15:27:23.726778 Epoch 168, Training loss 0.4742358882942468\n",
            "2022-03-30 15:27:34.541976 Epoch 169, Training loss 0.4700844716233061\n",
            "2022-03-30 15:27:45.393247 Epoch 170, Training loss 0.469180548625529\n",
            "2022-03-30 15:27:56.222212 Epoch 171, Training loss 0.46942392997729504\n",
            "2022-03-30 15:28:07.217989 Epoch 172, Training loss 0.4681767937929734\n",
            "2022-03-30 15:28:18.006216 Epoch 173, Training loss 0.4677243512457289\n",
            "2022-03-30 15:28:28.775566 Epoch 174, Training loss 0.462929460170019\n",
            "2022-03-30 15:28:39.616483 Epoch 175, Training loss 0.4651122914479517\n",
            "2022-03-30 15:28:50.463743 Epoch 176, Training loss 0.4639074515618022\n",
            "2022-03-30 15:29:01.445217 Epoch 177, Training loss 0.4644551017057255\n",
            "2022-03-30 15:29:12.343246 Epoch 178, Training loss 0.4630651156920606\n",
            "2022-03-30 15:29:23.115292 Epoch 179, Training loss 0.46108863009211354\n",
            "2022-03-30 15:29:34.159277 Epoch 180, Training loss 0.46233543227700624\n",
            "2022-03-30 15:29:44.940636 Epoch 181, Training loss 0.4595619693322255\n",
            "2022-03-30 15:29:55.782196 Epoch 182, Training loss 0.4598546034234869\n",
            "2022-03-30 15:30:06.721788 Epoch 183, Training loss 0.4555978546743198\n",
            "2022-03-30 15:30:17.583071 Epoch 184, Training loss 0.45870780786666115\n",
            "2022-03-30 15:30:28.462701 Epoch 185, Training loss 0.45706891152255064\n",
            "2022-03-30 15:30:39.375236 Epoch 186, Training loss 0.4563245953577559\n",
            "2022-03-30 15:30:50.372181 Epoch 187, Training loss 0.454702249992534\n",
            "2022-03-30 15:31:01.263604 Epoch 188, Training loss 0.4536730021695652\n",
            "2022-03-30 15:31:12.100721 Epoch 189, Training loss 0.45591826470154323\n",
            "2022-03-30 15:31:22.856847 Epoch 190, Training loss 0.4538505960760824\n",
            "2022-03-30 15:31:33.694770 Epoch 191, Training loss 0.45361843545113684\n",
            "2022-03-30 15:31:44.543788 Epoch 192, Training loss 0.45266908437699616\n",
            "2022-03-30 15:31:55.394568 Epoch 193, Training loss 0.45076696729035026\n",
            "2022-03-30 15:32:06.303704 Epoch 194, Training loss 0.4514723455966891\n",
            "2022-03-30 15:32:17.112114 Epoch 195, Training loss 0.4503453919649734\n",
            "2022-03-30 15:32:27.901060 Epoch 196, Training loss 0.45027617300334183\n",
            "2022-03-30 15:32:39.215725 Epoch 197, Training loss 0.4485063199192057\n",
            "2022-03-30 15:32:50.112517 Epoch 198, Training loss 0.446231642209203\n",
            "2022-03-30 15:33:01.042326 Epoch 199, Training loss 0.44326288565574096\n",
            "2022-03-30 15:33:11.952508 Epoch 200, Training loss 0.44664115508270386\n",
            "2022-03-30 15:33:22.815525 Epoch 201, Training loss 0.4450011896469709\n",
            "2022-03-30 15:33:33.719720 Epoch 202, Training loss 0.44168465300594145\n",
            "2022-03-30 15:33:44.665666 Epoch 203, Training loss 0.44531438093813486\n",
            "2022-03-30 15:33:55.547195 Epoch 204, Training loss 0.44204171339188086\n",
            "2022-03-30 15:34:06.468299 Epoch 205, Training loss 0.43993044377821483\n",
            "2022-03-30 15:34:17.378279 Epoch 206, Training loss 0.4400289304497297\n",
            "2022-03-30 15:34:28.269509 Epoch 207, Training loss 0.4415342857313278\n",
            "2022-03-30 15:34:39.114389 Epoch 208, Training loss 0.44272521822272665\n",
            "2022-03-30 15:34:50.058850 Epoch 209, Training loss 0.4377758234281979\n",
            "2022-03-30 15:35:00.968931 Epoch 210, Training loss 0.4385080543225226\n",
            "2022-03-30 15:35:11.878450 Epoch 211, Training loss 0.435715993404236\n",
            "2022-03-30 15:35:22.917216 Epoch 212, Training loss 0.43749552391602864\n",
            "2022-03-30 15:35:33.734948 Epoch 213, Training loss 0.4401140428717484\n",
            "2022-03-30 15:35:44.571628 Epoch 214, Training loss 0.43655158270655386\n",
            "2022-03-30 15:35:55.497013 Epoch 215, Training loss 0.43470692468802336\n",
            "2022-03-30 15:36:06.512541 Epoch 216, Training loss 0.4384071064536529\n",
            "2022-03-30 15:36:17.485523 Epoch 217, Training loss 0.43443859448594513\n",
            "2022-03-30 15:36:28.216320 Epoch 218, Training loss 0.43522119581165825\n",
            "2022-03-30 15:36:39.164128 Epoch 219, Training loss 0.4345229767129549\n",
            "2022-03-30 15:36:50.414015 Epoch 220, Training loss 0.43299230302462494\n",
            "2022-03-30 15:37:01.430861 Epoch 221, Training loss 0.429966358691835\n",
            "2022-03-30 15:37:12.471100 Epoch 222, Training loss 0.428240743763459\n",
            "2022-03-30 15:37:23.482233 Epoch 223, Training loss 0.431328100240444\n",
            "2022-03-30 15:37:34.469260 Epoch 224, Training loss 0.42969809154339156\n",
            "2022-03-30 15:37:45.400909 Epoch 225, Training loss 0.429902494068036\n",
            "2022-03-30 15:37:56.182498 Epoch 226, Training loss 0.4306029723314068\n",
            "2022-03-30 15:38:07.372381 Epoch 227, Training loss 0.4292566171463798\n",
            "2022-03-30 15:38:18.399736 Epoch 228, Training loss 0.4276312422912444\n",
            "2022-03-30 15:38:29.388588 Epoch 229, Training loss 0.4281836811386411\n",
            "2022-03-30 15:38:40.277830 Epoch 230, Training loss 0.4250755681444312\n",
            "2022-03-30 15:38:51.325230 Epoch 231, Training loss 0.42968253901852366\n",
            "2022-03-30 15:39:02.207846 Epoch 232, Training loss 0.4248770464716665\n",
            "2022-03-30 15:39:13.016949 Epoch 233, Training loss 0.42591882714301427\n",
            "2022-03-30 15:39:23.939883 Epoch 234, Training loss 0.42469264233432463\n",
            "2022-03-30 15:39:34.965061 Epoch 235, Training loss 0.4244040156931371\n",
            "2022-03-30 15:39:45.916472 Epoch 236, Training loss 0.42542772408565294\n",
            "2022-03-30 15:39:56.804316 Epoch 237, Training loss 0.42694942799904156\n",
            "2022-03-30 15:40:07.805512 Epoch 238, Training loss 0.4228128826869723\n",
            "2022-03-30 15:40:18.844252 Epoch 239, Training loss 0.4204356085766307\n",
            "2022-03-30 15:40:29.671421 Epoch 240, Training loss 0.42360079330404093\n",
            "2022-03-30 15:40:40.570910 Epoch 241, Training loss 0.42052356386199935\n",
            "2022-03-30 15:40:51.544546 Epoch 242, Training loss 0.4198182348323905\n",
            "2022-03-30 15:41:02.574654 Epoch 243, Training loss 0.42124697444079173\n",
            "2022-03-30 15:41:13.489590 Epoch 244, Training loss 0.41878683929858\n",
            "2022-03-30 15:41:24.466514 Epoch 245, Training loss 0.41881117037952403\n",
            "2022-03-30 15:41:35.394609 Epoch 246, Training loss 0.4141507193903484\n",
            "2022-03-30 15:41:46.206397 Epoch 247, Training loss 0.4189677463506189\n",
            "2022-03-30 15:41:57.005977 Epoch 248, Training loss 0.4170384672672852\n",
            "2022-03-30 15:42:07.873509 Epoch 249, Training loss 0.415291260823112\n",
            "2022-03-30 15:42:18.943740 Epoch 250, Training loss 0.4155821524503286\n",
            "2022-03-30 15:42:29.983757 Epoch 251, Training loss 0.42038866052465973\n",
            "2022-03-30 15:42:41.108983 Epoch 252, Training loss 0.4148252708146639\n",
            "2022-03-30 15:42:52.047719 Epoch 253, Training loss 0.4168295511389937\n",
            "2022-03-30 15:43:03.099939 Epoch 254, Training loss 0.41319514599526325\n",
            "2022-03-30 15:43:13.811264 Epoch 255, Training loss 0.41553192465658995\n",
            "2022-03-30 15:43:24.823539 Epoch 256, Training loss 0.4124287036450013\n",
            "2022-03-30 15:43:35.757537 Epoch 257, Training loss 0.4129027687108425\n",
            "2022-03-30 15:43:46.711778 Epoch 258, Training loss 0.41555345045102526\n",
            "2022-03-30 15:43:57.620280 Epoch 259, Training loss 0.4125206110918004\n",
            "2022-03-30 15:44:08.543654 Epoch 260, Training loss 0.4085564224425789\n",
            "2022-03-30 15:44:19.432587 Epoch 261, Training loss 0.41192191101782155\n",
            "2022-03-30 15:44:30.316189 Epoch 262, Training loss 0.4089831098952257\n",
            "2022-03-30 15:44:40.892077 Epoch 263, Training loss 0.4069897540466255\n",
            "2022-03-30 15:44:51.686451 Epoch 264, Training loss 0.41011442221186656\n",
            "2022-03-30 15:45:02.821348 Epoch 265, Training loss 0.41107847499649236\n",
            "2022-03-30 15:45:13.623630 Epoch 266, Training loss 0.4115015898862153\n",
            "2022-03-30 15:45:24.354050 Epoch 267, Training loss 0.40735170210871247\n",
            "2022-03-30 15:45:35.232687 Epoch 268, Training loss 0.40914286143334627\n",
            "2022-03-30 15:45:46.138689 Epoch 269, Training loss 0.40963484870884426\n",
            "2022-03-30 15:45:56.943060 Epoch 270, Training loss 0.4077986040726647\n",
            "2022-03-30 15:46:08.088250 Epoch 271, Training loss 0.40786972386605297\n",
            "2022-03-30 15:46:18.913021 Epoch 272, Training loss 0.40815827848813724\n",
            "2022-03-30 15:46:30.057202 Epoch 273, Training loss 0.4044764209205232\n",
            "2022-03-30 15:46:41.003772 Epoch 274, Training loss 0.40348490348557375\n",
            "2022-03-30 15:46:51.945415 Epoch 275, Training loss 0.404296759887577\n",
            "2022-03-30 15:47:02.770629 Epoch 276, Training loss 0.4022756964539933\n",
            "2022-03-30 15:47:13.434221 Epoch 277, Training loss 0.4064341043610402\n",
            "2022-03-30 15:47:24.315070 Epoch 278, Training loss 0.40171761601172445\n",
            "2022-03-30 15:47:35.368345 Epoch 279, Training loss 0.4054197574896581\n",
            "2022-03-30 15:47:46.299979 Epoch 280, Training loss 0.40123326739158166\n",
            "2022-03-30 15:47:57.101316 Epoch 281, Training loss 0.40124095938242305\n",
            "2022-03-30 15:48:08.008192 Epoch 282, Training loss 0.4057898214825279\n",
            "2022-03-30 15:48:18.967009 Epoch 283, Training loss 0.4028562074884429\n",
            "2022-03-30 15:48:29.792898 Epoch 284, Training loss 0.4024388769741558\n",
            "2022-03-30 15:48:40.600907 Epoch 285, Training loss 0.4011030084527362\n",
            "2022-03-30 15:48:51.458719 Epoch 286, Training loss 0.39901595654161387\n",
            "2022-03-30 15:49:02.643182 Epoch 287, Training loss 0.4002588281164999\n",
            "2022-03-30 15:49:13.700757 Epoch 288, Training loss 0.40131128072509986\n",
            "2022-03-30 15:49:24.845284 Epoch 289, Training loss 0.4037212198957458\n",
            "2022-03-30 15:49:35.804115 Epoch 290, Training loss 0.3981615993029931\n",
            "2022-03-30 15:49:46.824682 Epoch 291, Training loss 0.39880313395577316\n",
            "2022-03-30 15:49:57.562957 Epoch 292, Training loss 0.3959845649388135\n",
            "2022-03-30 15:50:08.510906 Epoch 293, Training loss 0.39768407934004696\n",
            "2022-03-30 15:50:19.507076 Epoch 294, Training loss 0.39597193502327976\n",
            "2022-03-30 15:50:30.597775 Epoch 295, Training loss 0.3961890802035094\n",
            "2022-03-30 15:50:41.555955 Epoch 296, Training loss 0.3948050790735523\n",
            "2022-03-30 15:50:52.509779 Epoch 297, Training loss 0.39773797836450053\n",
            "2022-03-30 15:51:03.417405 Epoch 298, Training loss 0.3985724752135289\n",
            "2022-03-30 15:51:14.467871 Epoch 299, Training loss 0.39597976272520813\n",
            "2022-03-30 15:51:25.786951 Epoch 300, Training loss 0.3975446016511039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRpeE77JxsJj",
        "outputId": "aa6208dc-7f2a-4aac-b513-7a037dc5c1a9"
      },
      "id": "nRpeE77JxsJj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.76\n",
            "Accuracy val: 0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################################################################\n",
        "#########Problem 2 PART 2######### 1/3 (BATCH NORMALIZATION)\n",
        "##############################################################################################################################"
      ],
      "metadata": {
        "id": "i5h3pTW4l7jJ"
      },
      "id": "i5h3pTW4l7jJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetRes(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "          super(NetRes, self).__init__()\n",
        "          self.conv = nn.Conv2d(n_chans1, n_chans1, \n",
        "            kernel_size=3, padding=1, bias=False)\n",
        "          self.batch_norm = nn.BatchNorm2d(num_features=\n",
        "                                     n_chans1)\n",
        "          torch.nn.init.kaiming_normal_(self.conv.weight, \n",
        "                            nonlinearity='relu')\n",
        "          torch.nn.init.constant_(self.batch_norm.weight, \n",
        "                            0.5)\n",
        "          torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "        \n",
        "    def forward(self, x):\n",
        "     out = self.conv(x)\n",
        "     out = torch.relu(out)\n",
        "     return out + x"
      ],
      "metadata": {
        "id": "cqVvRy67_Hy1"
      },
      "id": "cqVvRy67_Hy1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, \n",
        "                        batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to('cuda:0')\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs =300,\n",
        "    optimizer= optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcCrjQoG_5MJ",
        "outputId": "11a4adc7-6cf8-4f56-dfe3-6568cf850e3a"
      },
      "id": "zcCrjQoG_5MJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-30 22:43:00.080416 Epoch 1, Training loss 2.0433139910783304\n",
            "2022-03-30 22:43:09.056253 Epoch 2, Training loss 1.7821431681323234\n",
            "2022-03-30 22:43:18.483246 Epoch 3, Training loss 1.6077832204606526\n",
            "2022-03-30 22:43:27.734464 Epoch 4, Training loss 1.5257196385232383\n",
            "2022-03-30 22:43:37.226082 Epoch 5, Training loss 1.4694439521073686\n",
            "2022-03-30 22:43:46.696314 Epoch 6, Training loss 1.4247727144099867\n",
            "2022-03-30 22:43:56.168772 Epoch 7, Training loss 1.385382987196793\n",
            "2022-03-30 22:44:05.417752 Epoch 8, Training loss 1.345282492735197\n",
            "2022-03-30 22:44:14.700885 Epoch 9, Training loss 1.3018881581780855\n",
            "2022-03-30 22:44:23.814807 Epoch 10, Training loss 1.2568906674452145\n",
            "2022-03-30 22:44:33.032870 Epoch 11, Training loss 1.2209365199441495\n",
            "2022-03-30 22:44:42.461687 Epoch 12, Training loss 1.1908568870990783\n",
            "2022-03-30 22:44:51.618654 Epoch 13, Training loss 1.1631897738979906\n",
            "2022-03-30 22:45:00.702190 Epoch 14, Training loss 1.1378452889907085\n",
            "2022-03-30 22:45:10.113566 Epoch 15, Training loss 1.1176696303098097\n",
            "2022-03-30 22:45:19.350377 Epoch 16, Training loss 1.0978669314585683\n",
            "2022-03-30 22:45:28.530513 Epoch 17, Training loss 1.0796291674951763\n",
            "2022-03-30 22:45:37.844822 Epoch 18, Training loss 1.0625739611323228\n",
            "2022-03-30 22:45:46.943128 Epoch 19, Training loss 1.0493576762926242\n",
            "2022-03-30 22:45:56.133728 Epoch 20, Training loss 1.0355397427021085\n",
            "2022-03-30 22:46:05.113787 Epoch 21, Training loss 1.0240520783854872\n",
            "2022-03-30 22:46:14.596762 Epoch 22, Training loss 1.0116074271214284\n",
            "2022-03-30 22:46:23.654390 Epoch 23, Training loss 1.0023854921388504\n",
            "2022-03-30 22:46:32.900251 Epoch 24, Training loss 0.9945218564604249\n",
            "2022-03-30 22:46:42.210421 Epoch 25, Training loss 0.9858207536475433\n",
            "2022-03-30 22:46:51.324182 Epoch 26, Training loss 0.9765059474636527\n",
            "2022-03-30 22:47:00.604026 Epoch 27, Training loss 0.9687950844350068\n",
            "2022-03-30 22:47:09.578107 Epoch 28, Training loss 0.9626021958373087\n",
            "2022-03-30 22:47:18.623721 Epoch 29, Training loss 0.9549488700411813\n",
            "2022-03-30 22:47:27.815689 Epoch 30, Training loss 0.9483764814141461\n",
            "2022-03-30 22:47:37.001699 Epoch 31, Training loss 0.9409043338445141\n",
            "2022-03-30 22:47:46.146490 Epoch 32, Training loss 0.9335385696662356\n",
            "2022-03-30 22:47:55.261943 Epoch 33, Training loss 0.9281366084847609\n",
            "2022-03-30 22:48:04.506132 Epoch 34, Training loss 0.9213916096845856\n",
            "2022-03-30 22:48:13.778300 Epoch 35, Training loss 0.9161236245004113\n",
            "2022-03-30 22:48:22.846052 Epoch 36, Training loss 0.9103046411748432\n",
            "2022-03-30 22:48:31.769517 Epoch 37, Training loss 0.9036654845985306\n",
            "2022-03-30 22:48:41.018708 Epoch 38, Training loss 0.8990452277767079\n",
            "2022-03-30 22:48:50.306903 Epoch 39, Training loss 0.8942073728422375\n",
            "2022-03-30 22:48:59.597068 Epoch 40, Training loss 0.8886247247533725\n",
            "2022-03-30 22:49:08.720037 Epoch 41, Training loss 0.8837815003321908\n",
            "2022-03-30 22:49:18.083312 Epoch 42, Training loss 0.8786501556711124\n",
            "2022-03-30 22:49:27.208097 Epoch 43, Training loss 0.8728969801798501\n",
            "2022-03-30 22:49:36.590625 Epoch 44, Training loss 0.8692923372663806\n",
            "2022-03-30 22:49:45.943763 Epoch 45, Training loss 0.8651089654173083\n",
            "2022-03-30 22:49:55.092791 Epoch 46, Training loss 0.860038573937038\n",
            "2022-03-30 22:50:04.700714 Epoch 47, Training loss 0.8547497316242179\n",
            "2022-03-30 22:50:13.899370 Epoch 48, Training loss 0.8512534232395689\n",
            "2022-03-30 22:50:23.237794 Epoch 49, Training loss 0.847132809745991\n",
            "2022-03-30 22:50:32.451571 Epoch 50, Training loss 0.8436875078455567\n",
            "2022-03-30 22:50:41.611113 Epoch 51, Training loss 0.8378376843755507\n",
            "2022-03-30 22:50:50.881679 Epoch 52, Training loss 0.8340969494236704\n",
            "2022-03-30 22:51:00.215010 Epoch 53, Training loss 0.8309336472350313\n",
            "2022-03-30 22:51:09.145886 Epoch 54, Training loss 0.8274258290943892\n",
            "2022-03-30 22:51:18.366420 Epoch 55, Training loss 0.8236568352145612\n",
            "2022-03-30 22:51:27.719700 Epoch 56, Training loss 0.8204166014176195\n",
            "2022-03-30 22:51:36.919833 Epoch 57, Training loss 0.8161967308320048\n",
            "2022-03-30 22:51:46.128193 Epoch 58, Training loss 0.8126945015033493\n",
            "2022-03-30 22:51:55.479887 Epoch 59, Training loss 0.8101605705135618\n",
            "2022-03-30 22:52:04.755038 Epoch 60, Training loss 0.8062197367858399\n",
            "2022-03-30 22:52:14.135680 Epoch 61, Training loss 0.8029446828243373\n",
            "2022-03-30 22:52:23.435584 Epoch 62, Training loss 0.7986018473992262\n",
            "2022-03-30 22:52:32.427472 Epoch 63, Training loss 0.7963255248639894\n",
            "2022-03-30 22:52:41.524592 Epoch 64, Training loss 0.7947031071652537\n",
            "2022-03-30 22:52:51.223371 Epoch 65, Training loss 0.7908895333938282\n",
            "2022-03-30 22:53:00.330837 Epoch 66, Training loss 0.7875865857162134\n",
            "2022-03-30 22:53:09.640786 Epoch 67, Training loss 0.7848909802144141\n",
            "2022-03-30 22:53:18.766025 Epoch 68, Training loss 0.7809438404753385\n",
            "2022-03-30 22:53:27.915698 Epoch 69, Training loss 0.7788766341288681\n",
            "2022-03-30 22:53:37.358969 Epoch 70, Training loss 0.7750676541072329\n",
            "2022-03-30 22:53:46.493613 Epoch 71, Training loss 0.7752228687181497\n",
            "2022-03-30 22:53:55.475758 Epoch 72, Training loss 0.7703852459140446\n",
            "2022-03-30 22:54:04.679953 Epoch 73, Training loss 0.7691448444448163\n",
            "2022-03-30 22:54:13.709731 Epoch 74, Training loss 0.7639342988543498\n",
            "2022-03-30 22:54:22.916699 Epoch 75, Training loss 0.7631786786534293\n",
            "2022-03-30 22:54:32.094254 Epoch 76, Training loss 0.7601490673963981\n",
            "2022-03-30 22:54:41.457917 Epoch 77, Training loss 0.7574696513393041\n",
            "2022-03-30 22:54:50.732058 Epoch 78, Training loss 0.7557284774454048\n",
            "2022-03-30 22:54:59.988893 Epoch 79, Training loss 0.7532423489996235\n",
            "2022-03-30 22:55:09.204720 Epoch 80, Training loss 0.7510043770989494\n",
            "2022-03-30 22:55:18.282957 Epoch 81, Training loss 0.7471775234584004\n",
            "2022-03-30 22:55:27.654391 Epoch 82, Training loss 0.7452909616786806\n",
            "2022-03-30 22:55:37.115008 Epoch 83, Training loss 0.7441483934593323\n",
            "2022-03-30 22:55:46.123200 Epoch 84, Training loss 0.7424836707923114\n",
            "2022-03-30 22:55:55.312747 Epoch 85, Training loss 0.7400352948385737\n",
            "2022-03-30 22:56:04.737999 Epoch 86, Training loss 0.7361532463442029\n",
            "2022-03-30 22:56:14.042589 Epoch 87, Training loss 0.7352834117915624\n",
            "2022-03-30 22:56:23.478480 Epoch 88, Training loss 0.7325031606818709\n",
            "2022-03-30 22:56:32.577496 Epoch 89, Training loss 0.731102618072039\n",
            "2022-03-30 22:56:41.606502 Epoch 90, Training loss 0.7280676654156517\n",
            "2022-03-30 22:56:51.017957 Epoch 91, Training loss 0.7279561369696541\n",
            "2022-03-30 22:57:00.319020 Epoch 92, Training loss 0.7252229848100097\n",
            "2022-03-30 22:57:09.715076 Epoch 93, Training loss 0.7242615827742744\n",
            "2022-03-30 22:57:19.020494 Epoch 94, Training loss 0.7217815388041688\n",
            "2022-03-30 22:57:28.163505 Epoch 95, Training loss 0.7174776816916892\n",
            "2022-03-30 22:57:37.491222 Epoch 96, Training loss 0.7171816953536495\n",
            "2022-03-30 22:57:46.865073 Epoch 97, Training loss 0.7142757502815608\n",
            "2022-03-30 22:57:55.789020 Epoch 98, Training loss 0.7137278524201239\n",
            "2022-03-30 22:58:05.052771 Epoch 99, Training loss 0.7108313805230743\n",
            "2022-03-30 22:58:14.290933 Epoch 100, Training loss 0.7094357596791309\n",
            "2022-03-30 22:58:23.739718 Epoch 101, Training loss 0.7073342303180938\n",
            "2022-03-30 22:58:33.269925 Epoch 102, Training loss 0.7064153584830292\n",
            "2022-03-30 22:58:42.521796 Epoch 103, Training loss 0.7040979778751388\n",
            "2022-03-30 22:58:51.809997 Epoch 104, Training loss 0.701848077880757\n",
            "2022-03-30 22:59:01.177838 Epoch 105, Training loss 0.6985227936292853\n",
            "2022-03-30 22:59:10.297083 Epoch 106, Training loss 0.6997520603868358\n",
            "2022-03-30 22:59:19.280521 Epoch 107, Training loss 0.6987103859863013\n",
            "2022-03-30 22:59:28.453067 Epoch 108, Training loss 0.6948021045502495\n",
            "2022-03-30 22:59:37.760202 Epoch 109, Training loss 0.693694171393314\n",
            "2022-03-30 22:59:47.297715 Epoch 110, Training loss 0.6909396781793335\n",
            "2022-03-30 22:59:56.808172 Epoch 111, Training loss 0.6903244415893579\n",
            "2022-03-30 23:00:06.311354 Epoch 112, Training loss 0.6884206080680613\n",
            "2022-03-30 23:00:15.630185 Epoch 113, Training loss 0.6864704547635735\n",
            "2022-03-30 23:00:25.217912 Epoch 114, Training loss 0.6855922324197067\n",
            "2022-03-30 23:00:34.369699 Epoch 115, Training loss 0.6846609506994257\n",
            "2022-03-30 23:00:43.725171 Epoch 116, Training loss 0.6837098292072715\n",
            "2022-03-30 23:00:53.261332 Epoch 117, Training loss 0.6818309549404227\n",
            "2022-03-30 23:01:02.822727 Epoch 118, Training loss 0.679618110780216\n",
            "2022-03-30 23:01:12.286369 Epoch 119, Training loss 0.6778591521027143\n",
            "2022-03-30 23:01:21.761415 Epoch 120, Training loss 0.6757374839938205\n",
            "2022-03-30 23:01:31.489234 Epoch 121, Training loss 0.6746902779087691\n",
            "2022-03-30 23:01:40.903354 Epoch 122, Training loss 0.6734041365439934\n",
            "2022-03-30 23:01:50.281539 Epoch 123, Training loss 0.6708694379729079\n",
            "2022-03-30 23:01:59.289142 Epoch 124, Training loss 0.6691963270954464\n",
            "2022-03-30 23:02:08.843166 Epoch 125, Training loss 0.667723171927435\n",
            "2022-03-30 23:02:18.120944 Epoch 126, Training loss 0.6666456272306345\n",
            "2022-03-30 23:02:27.333792 Epoch 127, Training loss 0.6657290686960415\n",
            "2022-03-30 23:02:36.655677 Epoch 128, Training loss 0.665571184414427\n",
            "2022-03-30 23:02:45.780466 Epoch 129, Training loss 0.6643440270286691\n",
            "2022-03-30 23:02:54.795083 Epoch 130, Training loss 0.6619411431005239\n",
            "2022-03-30 23:03:04.236758 Epoch 131, Training loss 0.6613091792901764\n",
            "2022-03-30 23:03:13.431529 Epoch 132, Training loss 0.6605622855293781\n",
            "2022-03-30 23:03:22.491390 Epoch 133, Training loss 0.657961355946253\n",
            "2022-03-30 23:03:31.900766 Epoch 134, Training loss 0.658692389650418\n",
            "2022-03-30 23:03:41.221391 Epoch 135, Training loss 0.6539594350797137\n",
            "2022-03-30 23:03:50.281007 Epoch 136, Training loss 0.6557859211702786\n",
            "2022-03-30 23:03:59.526884 Epoch 137, Training loss 0.6521818451869213\n",
            "2022-03-30 23:04:08.876017 Epoch 138, Training loss 0.6518422460083462\n",
            "2022-03-30 23:04:18.414210 Epoch 139, Training loss 0.6495376244149245\n",
            "2022-03-30 23:04:27.649114 Epoch 140, Training loss 0.6485660897039086\n",
            "2022-03-30 23:04:36.634161 Epoch 141, Training loss 0.6478255587389402\n",
            "2022-03-30 23:04:45.891578 Epoch 142, Training loss 0.6458351155528632\n",
            "2022-03-30 23:04:55.315617 Epoch 143, Training loss 0.6470191897181294\n",
            "2022-03-30 23:05:04.801143 Epoch 144, Training loss 0.6436109455954998\n",
            "2022-03-30 23:05:14.110273 Epoch 145, Training loss 0.6419577825328578\n",
            "2022-03-30 23:05:23.296836 Epoch 146, Training loss 0.6402618114448264\n",
            "2022-03-30 23:05:32.539559 Epoch 147, Training loss 0.6382840116081945\n",
            "2022-03-30 23:05:41.697909 Epoch 148, Training loss 0.6401568356987155\n",
            "2022-03-30 23:05:50.844915 Epoch 149, Training loss 0.6383799130235182\n",
            "2022-03-30 23:05:59.751625 Epoch 150, Training loss 0.6371194199299264\n",
            "2022-03-30 23:06:09.074615 Epoch 151, Training loss 0.6345545250131651\n",
            "2022-03-30 23:06:18.165725 Epoch 152, Training loss 0.6350425514952301\n",
            "2022-03-30 23:06:27.625637 Epoch 153, Training loss 0.6343670748460019\n",
            "2022-03-30 23:06:36.913747 Epoch 154, Training loss 0.631985672370857\n",
            "2022-03-30 23:06:46.155073 Epoch 155, Training loss 0.6322033633585171\n",
            "2022-03-30 23:06:55.429092 Epoch 156, Training loss 0.6320968296216882\n",
            "2022-03-30 23:07:04.441035 Epoch 157, Training loss 0.6315436700116033\n",
            "2022-03-30 23:07:13.442192 Epoch 158, Training loss 0.6279991752351336\n",
            "2022-03-30 23:07:22.390538 Epoch 159, Training loss 0.6288218169718447\n",
            "2022-03-30 23:07:31.817397 Epoch 160, Training loss 0.6263497845291177\n",
            "2022-03-30 23:07:41.095944 Epoch 161, Training loss 0.6247238881905061\n",
            "2022-03-30 23:07:50.376676 Epoch 162, Training loss 0.6240509394032266\n",
            "2022-03-30 23:07:59.299367 Epoch 163, Training loss 0.6236460294641192\n",
            "2022-03-30 23:08:08.543029 Epoch 164, Training loss 0.6214037167904017\n",
            "2022-03-30 23:08:17.784297 Epoch 165, Training loss 0.6219677575637618\n",
            "2022-03-30 23:08:27.058335 Epoch 166, Training loss 0.6225644576808681\n",
            "2022-03-30 23:08:36.054779 Epoch 167, Training loss 0.6206196258058938\n",
            "2022-03-30 23:08:44.976305 Epoch 168, Training loss 0.618597805157037\n",
            "2022-03-30 23:08:54.291795 Epoch 169, Training loss 0.6185371919589884\n",
            "2022-03-30 23:09:03.769563 Epoch 170, Training loss 0.6163179713594334\n",
            "2022-03-30 23:09:12.804395 Epoch 171, Training loss 0.6162333645860253\n",
            "2022-03-30 23:09:22.204828 Epoch 172, Training loss 0.6158811532323013\n",
            "2022-03-30 23:09:31.663557 Epoch 173, Training loss 0.6141559471330984\n",
            "2022-03-30 23:09:40.905576 Epoch 174, Training loss 0.6132172211204343\n",
            "2022-03-30 23:09:50.351788 Epoch 175, Training loss 0.6116385172928691\n",
            "2022-03-30 23:09:59.392315 Epoch 176, Training loss 0.612828500892805\n",
            "2022-03-30 23:10:08.618337 Epoch 177, Training loss 0.609975494120432\n",
            "2022-03-30 23:10:17.941026 Epoch 178, Training loss 0.6073844135569795\n",
            "2022-03-30 23:10:27.225170 Epoch 179, Training loss 0.6080852230948866\n",
            "2022-03-30 23:10:36.228226 Epoch 180, Training loss 0.6071884468235933\n",
            "2022-03-30 23:10:45.579535 Epoch 181, Training loss 0.6079349441220389\n",
            "2022-03-30 23:10:55.033209 Epoch 182, Training loss 0.6067444875340937\n",
            "2022-03-30 23:11:04.447010 Epoch 183, Training loss 0.6055729811834862\n",
            "2022-03-30 23:11:13.805059 Epoch 184, Training loss 0.6048378331200851\n",
            "2022-03-30 23:11:22.738540 Epoch 185, Training loss 0.6042945006162005\n",
            "2022-03-30 23:11:31.943602 Epoch 186, Training loss 0.602525477061796\n",
            "2022-03-30 23:11:41.098096 Epoch 187, Training loss 0.6012481242951835\n",
            "2022-03-30 23:11:50.329885 Epoch 188, Training loss 0.6002007846332267\n",
            "2022-03-30 23:11:59.537652 Epoch 189, Training loss 0.6006655039461067\n",
            "2022-03-30 23:12:08.885932 Epoch 190, Training loss 0.5979652619346634\n",
            "2022-03-30 23:12:18.450803 Epoch 191, Training loss 0.5979774712449144\n",
            "2022-03-30 23:12:27.768493 Epoch 192, Training loss 0.5967068971346712\n",
            "2022-03-30 23:12:36.974221 Epoch 193, Training loss 0.5969646829930718\n",
            "2022-03-30 23:12:45.873749 Epoch 194, Training loss 0.5963860669785448\n",
            "2022-03-30 23:12:55.189119 Epoch 195, Training loss 0.5951364158516954\n",
            "2022-03-30 23:13:04.403230 Epoch 196, Training loss 0.5956324474204837\n",
            "2022-03-30 23:13:13.492355 Epoch 197, Training loss 0.5929741596855471\n",
            "2022-03-30 23:13:22.756330 Epoch 198, Training loss 0.5931024651621919\n",
            "2022-03-30 23:13:31.920815 Epoch 199, Training loss 0.5912405831353439\n",
            "2022-03-30 23:13:41.549971 Epoch 200, Training loss 0.5919203138564859\n",
            "2022-03-30 23:13:51.208087 Epoch 201, Training loss 0.5897567983326095\n",
            "2022-03-30 23:14:00.371740 Epoch 202, Training loss 0.5904217650518393\n",
            "2022-03-30 23:14:09.550716 Epoch 203, Training loss 0.5891862098137131\n",
            "2022-03-30 23:14:19.273338 Epoch 204, Training loss 0.5880626876031041\n",
            "2022-03-30 23:14:28.779333 Epoch 205, Training loss 0.5865559957140242\n",
            "2022-03-30 23:14:38.232608 Epoch 206, Training loss 0.5874489337739432\n",
            "2022-03-30 23:14:47.440619 Epoch 207, Training loss 0.585760813921004\n",
            "2022-03-30 23:14:56.761110 Epoch 208, Training loss 0.5857396920776123\n",
            "2022-03-30 23:15:06.299607 Epoch 209, Training loss 0.583949885690761\n",
            "2022-03-30 23:15:15.763767 Epoch 210, Training loss 0.5818769527060906\n",
            "2022-03-30 23:15:24.846812 Epoch 211, Training loss 0.5828627019434633\n",
            "2022-03-30 23:15:34.272614 Epoch 212, Training loss 0.5818449293865877\n",
            "2022-03-30 23:15:43.512112 Epoch 213, Training loss 0.5818081763775452\n",
            "2022-03-30 23:15:53.102205 Epoch 214, Training loss 0.5800911734628555\n",
            "2022-03-30 23:16:02.383313 Epoch 215, Training loss 0.5793580979566135\n",
            "2022-03-30 23:16:11.665722 Epoch 216, Training loss 0.5804130641167121\n",
            "2022-03-30 23:16:20.937588 Epoch 217, Training loss 0.5778535846096781\n",
            "2022-03-30 23:16:30.241200 Epoch 218, Training loss 0.5770543223756659\n",
            "2022-03-30 23:16:39.550693 Epoch 219, Training loss 0.5796453917727751\n",
            "2022-03-30 23:16:48.769069 Epoch 220, Training loss 0.5758118692147153\n",
            "2022-03-30 23:16:58.002778 Epoch 221, Training loss 0.5762917562518888\n",
            "2022-03-30 23:17:07.557231 Epoch 222, Training loss 0.5731984387959361\n",
            "2022-03-30 23:17:16.681579 Epoch 223, Training loss 0.5734146421827624\n",
            "2022-03-30 23:17:26.093916 Epoch 224, Training loss 0.5730488616639696\n",
            "2022-03-30 23:17:35.538282 Epoch 225, Training loss 0.5737648037693385\n",
            "2022-03-30 23:17:44.754351 Epoch 226, Training loss 0.5725325165540361\n",
            "2022-03-30 23:17:54.217899 Epoch 227, Training loss 0.5727173244709249\n",
            "2022-03-30 23:18:03.728632 Epoch 228, Training loss 0.5715735106898086\n",
            "2022-03-30 23:18:12.856531 Epoch 229, Training loss 0.571348921569717\n",
            "2022-03-30 23:18:22.276724 Epoch 230, Training loss 0.5693457077455033\n",
            "2022-03-30 23:18:31.792916 Epoch 231, Training loss 0.571553928010604\n",
            "2022-03-30 23:18:41.323629 Epoch 232, Training loss 0.5683840891284406\n",
            "2022-03-30 23:18:50.525160 Epoch 233, Training loss 0.5680722864082707\n",
            "2022-03-30 23:18:59.859738 Epoch 234, Training loss 0.567982126894357\n",
            "2022-03-30 23:19:09.540638 Epoch 235, Training loss 0.5676830548916936\n",
            "2022-03-30 23:19:18.965039 Epoch 236, Training loss 0.5664710194787101\n",
            "2022-03-30 23:19:27.978481 Epoch 237, Training loss 0.5672021866835597\n",
            "2022-03-30 23:19:37.117588 Epoch 238, Training loss 0.5652240045997493\n",
            "2022-03-30 23:19:46.455730 Epoch 239, Training loss 0.5651613459029161\n",
            "2022-03-30 23:19:55.638655 Epoch 240, Training loss 0.5656452377891297\n",
            "2022-03-30 23:20:05.461882 Epoch 241, Training loss 0.5619634669607557\n",
            "2022-03-30 23:20:14.910464 Epoch 242, Training loss 0.56164596807164\n",
            "2022-03-30 23:20:24.304486 Epoch 243, Training loss 0.5628333381755882\n",
            "2022-03-30 23:20:33.826646 Epoch 244, Training loss 0.5607728713461201\n",
            "2022-03-30 23:20:43.004579 Epoch 245, Training loss 0.5598784361577704\n",
            "2022-03-30 23:20:52.343190 Epoch 246, Training loss 0.5590540629137507\n",
            "2022-03-30 23:21:01.660615 Epoch 247, Training loss 0.5591104510800003\n",
            "2022-03-30 23:21:10.987873 Epoch 248, Training loss 0.5580162590421984\n",
            "2022-03-30 23:21:20.377136 Epoch 249, Training loss 0.5586595401891967\n",
            "2022-03-30 23:21:29.620882 Epoch 250, Training loss 0.5583848787466889\n",
            "2022-03-30 23:21:39.031957 Epoch 251, Training loss 0.5564345938089253\n",
            "2022-03-30 23:21:48.601535 Epoch 252, Training loss 0.5554804444465491\n",
            "2022-03-30 23:21:58.088458 Epoch 253, Training loss 0.5566776196288941\n",
            "2022-03-30 23:22:07.111549 Epoch 254, Training loss 0.556271286762279\n",
            "2022-03-30 23:22:16.469655 Epoch 255, Training loss 0.5552512849383342\n",
            "2022-03-30 23:22:25.813677 Epoch 256, Training loss 0.5535686547722658\n",
            "2022-03-30 23:22:35.185874 Epoch 257, Training loss 0.5554126422194874\n",
            "2022-03-30 23:22:44.626510 Epoch 258, Training loss 0.5530918349542886\n",
            "2022-03-30 23:22:53.993950 Epoch 259, Training loss 0.5517938669647098\n",
            "2022-03-30 23:23:03.385109 Epoch 260, Training loss 0.5524163420890909\n",
            "2022-03-30 23:23:12.779304 Epoch 261, Training loss 0.5521543891457341\n",
            "2022-03-30 23:23:21.903780 Epoch 262, Training loss 0.5487461045117634\n",
            "2022-03-30 23:23:30.979565 Epoch 263, Training loss 0.5511978336078737\n",
            "2022-03-30 23:23:40.446069 Epoch 264, Training loss 0.5504791845217385\n",
            "2022-03-30 23:23:49.612146 Epoch 265, Training loss 0.5503946815610237\n",
            "2022-03-30 23:23:58.937957 Epoch 266, Training loss 0.5494481503125042\n",
            "2022-03-30 23:24:08.200004 Epoch 267, Training loss 0.5497199339634927\n",
            "2022-03-30 23:24:17.530414 Epoch 268, Training loss 0.5464772621879492\n",
            "2022-03-30 23:24:26.915202 Epoch 269, Training loss 0.5471343129987607\n",
            "2022-03-30 23:24:36.179795 Epoch 270, Training loss 0.5473961013624126\n",
            "2022-03-30 23:24:45.334759 Epoch 271, Training loss 0.5459113548631254\n",
            "2022-03-30 23:24:54.436497 Epoch 272, Training loss 0.5471358493237239\n",
            "2022-03-30 23:25:03.710822 Epoch 273, Training loss 0.5444522787771566\n",
            "2022-03-30 23:25:13.340132 Epoch 274, Training loss 0.5463207022613271\n",
            "2022-03-30 23:25:22.799120 Epoch 275, Training loss 0.5437561671447266\n",
            "2022-03-30 23:25:32.054039 Epoch 276, Training loss 0.5443910781074973\n",
            "2022-03-30 23:25:41.423917 Epoch 277, Training loss 0.5410070780216886\n",
            "2022-03-30 23:25:50.783129 Epoch 278, Training loss 0.5434590305971063\n",
            "2022-03-30 23:26:00.213354 Epoch 279, Training loss 0.5403318071304379\n",
            "2022-03-30 23:26:09.283702 Epoch 280, Training loss 0.5439352125234311\n",
            "2022-03-30 23:26:18.886573 Epoch 281, Training loss 0.542843020080453\n",
            "2022-03-30 23:26:28.413008 Epoch 282, Training loss 0.5427252830523054\n",
            "2022-03-30 23:26:37.775325 Epoch 283, Training loss 0.5408545179325907\n",
            "2022-03-30 23:26:47.108169 Epoch 284, Training loss 0.5403940593990524\n",
            "2022-03-30 23:26:56.492326 Epoch 285, Training loss 0.5391279345430682\n",
            "2022-03-30 23:27:06.082627 Epoch 286, Training loss 0.5390649620834214\n",
            "2022-03-30 23:27:15.380539 Epoch 287, Training loss 0.537678191919461\n",
            "2022-03-30 23:27:24.560825 Epoch 288, Training loss 0.5390832826609502\n",
            "2022-03-30 23:27:33.646818 Epoch 289, Training loss 0.5377432840407047\n",
            "2022-03-30 23:27:42.867400 Epoch 290, Training loss 0.538158214915439\n",
            "2022-03-30 23:27:52.725952 Epoch 291, Training loss 0.5382662101855973\n",
            "2022-03-30 23:28:02.412506 Epoch 292, Training loss 0.535513639716846\n",
            "2022-03-30 23:28:11.627593 Epoch 293, Training loss 0.5368290699046591\n",
            "2022-03-30 23:28:21.195719 Epoch 294, Training loss 0.5356248577156335\n",
            "2022-03-30 23:28:30.358643 Epoch 295, Training loss 0.534610606596598\n",
            "2022-03-30 23:28:39.722728 Epoch 296, Training loss 0.5353069868691437\n",
            "2022-03-30 23:28:48.970181 Epoch 297, Training loss 0.5348970570680126\n",
            "2022-03-30 23:28:58.191555 Epoch 298, Training loss 0.5359941850537839\n",
            "2022-03-30 23:29:07.573335 Epoch 299, Training loss 0.5317887856489252\n",
            "2022-03-30 23:29:16.995635 Epoch 300, Training loss 0.5321382847817048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EixdwMve-07g",
        "outputId": "df156a94-764e-4609-b0d9-14bc2ec2a5e4"
      },
      "id": "EixdwMve-07g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.81\n",
            "Accuracy val: 0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################################################################\n",
        "#########Problem 2 PART 2######### 2/3 (Dropout)\n",
        "##############################################################################################################################"
      ],
      "metadata": {
        "id": "dzhyAuJZYse1"
      },
      "id": "dzhyAuJZYse1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetRes(nn.Module):\n",
        "    def __init__(self, n_chans1=32, n_blocks=10):\n",
        "      super().__init__()\n",
        "      self.n_chans1 = n_chans1\n",
        "      self.conv1 = nn.Sequential(\n",
        "        *(n_blocks * [NetRes(n_chans1=n_chans1)]))\n",
        "      self.conv1_dropout = nn.Dropout2d(p=0.3)\n",
        "      self.fc1 = nn.Linear(8*8*n_chans1 // 2, 32)\n",
        "      self.fc2 = nn.Linear(32,2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "      out = self.conv1_dropout(out)\n",
        "      out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "      out = self.conv2_dropout(out)\n",
        "      out = out.view(-1, 8*8*self.n_chans1 // 2)\n",
        "      out = torch.tanh(self.fc1(out))\n",
        "      out = self.fc2(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "TftwIPWFZ9Tx"
      },
      "id": "TftwIPWFZ9Tx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, \n",
        "                        batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to('cuda:0')\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs =300,\n",
        "    optimizer= optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaMYwDemaZjJ",
        "outputId": "dd5f5e1a-8182-41bc-8a03-1296736fe7ad"
      },
      "id": "LaMYwDemaZjJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-30 21:50:34.646271 Epoch 1, Training loss 2.0466181361461846\n",
            "2022-03-30 21:50:43.828393 Epoch 2, Training loss 1.7477930401597181\n",
            "2022-03-30 21:50:53.122481 Epoch 3, Training loss 1.5780409339748684\n",
            "2022-03-30 21:51:02.230131 Epoch 4, Training loss 1.483592438118537\n",
            "2022-03-30 21:51:11.453534 Epoch 5, Training loss 1.4116625601373365\n",
            "2022-03-30 21:51:21.292715 Epoch 6, Training loss 1.3526329143577829\n",
            "2022-03-30 21:51:30.920935 Epoch 7, Training loss 1.304327110500287\n",
            "2022-03-30 21:51:40.374649 Epoch 8, Training loss 1.263481073443542\n",
            "2022-03-30 21:51:49.802405 Epoch 9, Training loss 1.22599498413103\n",
            "2022-03-30 21:51:58.818066 Epoch 10, Training loss 1.197358668307819\n",
            "2022-03-30 21:52:07.997774 Epoch 11, Training loss 1.1683437597873572\n",
            "2022-03-30 21:52:17.749909 Epoch 12, Training loss 1.145666810405224\n",
            "2022-03-30 21:52:27.297721 Epoch 13, Training loss 1.1223295064228576\n",
            "2022-03-30 21:52:36.651003 Epoch 14, Training loss 1.1002734790525168\n",
            "2022-03-30 21:52:46.172385 Epoch 15, Training loss 1.0807859234492798\n",
            "2022-03-30 21:52:55.367415 Epoch 16, Training loss 1.0640721375222706\n",
            "2022-03-30 21:53:04.682425 Epoch 17, Training loss 1.0468999681722782\n",
            "2022-03-30 21:53:13.843916 Epoch 18, Training loss 1.0320805885907633\n",
            "2022-03-30 21:53:22.944206 Epoch 19, Training loss 1.0192482408965031\n",
            "2022-03-30 21:53:32.315291 Epoch 20, Training loss 1.0058467508581899\n",
            "2022-03-30 21:53:41.579327 Epoch 21, Training loss 0.9944413832538878\n",
            "2022-03-30 21:53:50.801148 Epoch 22, Training loss 0.985778706000589\n",
            "2022-03-30 21:53:59.866317 Epoch 23, Training loss 0.9750423228649228\n",
            "2022-03-30 21:54:09.421339 Epoch 24, Training loss 0.9669450606836383\n",
            "2022-03-30 21:54:19.008446 Epoch 25, Training loss 0.9597495927682618\n",
            "2022-03-30 21:54:28.403750 Epoch 26, Training loss 0.9513164417213186\n",
            "2022-03-30 21:54:37.573179 Epoch 27, Training loss 0.9423051854533613\n",
            "2022-03-30 21:54:46.639550 Epoch 28, Training loss 0.934661391369827\n",
            "2022-03-30 21:54:55.846428 Epoch 29, Training loss 0.9276694244588427\n",
            "2022-03-30 21:55:05.213269 Epoch 30, Training loss 0.9211221900589935\n",
            "2022-03-30 21:55:14.516333 Epoch 31, Training loss 0.9148958493833956\n",
            "2022-03-30 21:55:24.138999 Epoch 32, Training loss 0.9075125051886225\n",
            "2022-03-30 21:55:33.307272 Epoch 33, Training loss 0.9029011789642637\n",
            "2022-03-30 21:55:42.561847 Epoch 34, Training loss 0.8970341651183565\n",
            "2022-03-30 21:55:51.852507 Epoch 35, Training loss 0.8902750725636397\n",
            "2022-03-30 21:56:00.845067 Epoch 36, Training loss 0.8849920716584491\n",
            "2022-03-30 21:56:09.889353 Epoch 37, Training loss 0.88096094847945\n",
            "2022-03-30 21:56:19.190639 Epoch 38, Training loss 0.8729815618766238\n",
            "2022-03-30 21:56:28.426207 Epoch 39, Training loss 0.8694114768139237\n",
            "2022-03-30 21:56:37.499444 Epoch 40, Training loss 0.8642743352581473\n",
            "2022-03-30 21:56:46.738879 Epoch 41, Training loss 0.8599186980206034\n",
            "2022-03-30 21:56:56.081102 Epoch 42, Training loss 0.8550843353314168\n",
            "2022-03-30 21:57:05.329580 Epoch 43, Training loss 0.848140198594469\n",
            "2022-03-30 21:57:14.448640 Epoch 44, Training loss 0.8460431267385897\n",
            "2022-03-30 21:57:23.526521 Epoch 45, Training loss 0.8405207689765775\n",
            "2022-03-30 21:57:32.694841 Epoch 46, Training loss 0.837000262630565\n",
            "2022-03-30 21:57:42.273643 Epoch 47, Training loss 0.8315850359094722\n",
            "2022-03-30 21:57:51.502223 Epoch 48, Training loss 0.8291697154569504\n",
            "2022-03-30 21:58:01.072243 Epoch 49, Training loss 0.8256534788462208\n",
            "2022-03-30 21:58:10.159819 Epoch 50, Training loss 0.8221141230267333\n",
            "2022-03-30 21:58:19.195040 Epoch 51, Training loss 0.818161750960228\n",
            "2022-03-30 21:58:28.388209 Epoch 52, Training loss 0.8128832307313104\n",
            "2022-03-30 21:58:37.680037 Epoch 53, Training loss 0.8097103693524895\n",
            "2022-03-30 21:58:46.741411 Epoch 54, Training loss 0.8074117587960284\n",
            "2022-03-30 21:58:56.634925 Epoch 55, Training loss 0.8034667022850203\n",
            "2022-03-30 21:59:06.147667 Epoch 56, Training loss 0.8001792790639736\n",
            "2022-03-30 21:59:15.382278 Epoch 57, Training loss 0.7952246440341101\n",
            "2022-03-30 21:59:24.645919 Epoch 58, Training loss 0.7917925032508343\n",
            "2022-03-30 21:59:34.452308 Epoch 59, Training loss 0.7892618989167006\n",
            "2022-03-30 21:59:43.778198 Epoch 60, Training loss 0.7863089461308306\n",
            "2022-03-30 21:59:53.060517 Epoch 61, Training loss 0.783544478171012\n",
            "2022-03-30 22:00:02.202088 Epoch 62, Training loss 0.7800508942597967\n",
            "2022-03-30 22:00:11.235516 Epoch 63, Training loss 0.7769570127320107\n",
            "2022-03-30 22:00:20.445365 Epoch 64, Training loss 0.7737365584163105\n",
            "2022-03-30 22:00:29.816024 Epoch 65, Training loss 0.7713703105364309\n",
            "2022-03-30 22:00:39.015998 Epoch 66, Training loss 0.7685936444708149\n",
            "2022-03-30 22:00:48.158333 Epoch 67, Training loss 0.7640456729913916\n",
            "2022-03-30 22:00:57.182193 Epoch 68, Training loss 0.7615712848694428\n",
            "2022-03-30 22:01:06.361974 Epoch 69, Training loss 0.760277019551648\n",
            "2022-03-30 22:01:15.814459 Epoch 70, Training loss 0.7551356663408182\n",
            "2022-03-30 22:01:24.836453 Epoch 71, Training loss 0.7537872861413395\n",
            "2022-03-30 22:01:34.115469 Epoch 72, Training loss 0.7516550925534095\n",
            "2022-03-30 22:01:43.240121 Epoch 73, Training loss 0.7498612377192359\n",
            "2022-03-30 22:01:52.433784 Epoch 74, Training loss 0.7467203959632103\n",
            "2022-03-30 22:02:01.732923 Epoch 75, Training loss 0.7440524386323016\n",
            "2022-03-30 22:02:11.005152 Epoch 76, Training loss 0.7414851036599225\n",
            "2022-03-30 22:02:20.323783 Epoch 77, Training loss 0.7389109292451073\n",
            "2022-03-30 22:02:29.414456 Epoch 78, Training loss 0.737251870391314\n",
            "2022-03-30 22:02:38.674614 Epoch 79, Training loss 0.7359215730748823\n",
            "2022-03-30 22:02:47.793691 Epoch 80, Training loss 0.7336337389543538\n",
            "2022-03-30 22:02:57.176660 Epoch 81, Training loss 0.7306441867442997\n",
            "2022-03-30 22:03:06.711543 Epoch 82, Training loss 0.7286093048656078\n",
            "2022-03-30 22:03:15.970853 Epoch 83, Training loss 0.72722523146883\n",
            "2022-03-30 22:03:25.111061 Epoch 84, Training loss 0.7243884388748032\n",
            "2022-03-30 22:03:34.307279 Epoch 85, Training loss 0.722233283931337\n",
            "2022-03-30 22:03:43.681231 Epoch 86, Training loss 0.7189002955889763\n",
            "2022-03-30 22:03:53.218638 Epoch 87, Training loss 0.7178000415606267\n",
            "2022-03-30 22:04:02.342877 Epoch 88, Training loss 0.717336430650233\n",
            "2022-03-30 22:04:11.527078 Epoch 89, Training loss 0.7144497466819061\n",
            "2022-03-30 22:04:20.840054 Epoch 90, Training loss 0.712302412027898\n",
            "2022-03-30 22:04:30.228266 Epoch 91, Training loss 0.7103819566614488\n",
            "2022-03-30 22:04:39.383594 Epoch 92, Training loss 0.7075284641721974\n",
            "2022-03-30 22:04:48.675991 Epoch 93, Training loss 0.7062840495649201\n",
            "2022-03-30 22:04:58.001358 Epoch 94, Training loss 0.7059025866601168\n",
            "2022-03-30 22:05:07.105134 Epoch 95, Training loss 0.7019181195503611\n",
            "2022-03-30 22:05:16.110435 Epoch 96, Training loss 0.7007015725154706\n",
            "2022-03-30 22:05:25.049942 Epoch 97, Training loss 0.6986944350363958\n",
            "2022-03-30 22:05:34.352422 Epoch 98, Training loss 0.6985298495006074\n",
            "2022-03-30 22:05:43.595474 Epoch 99, Training loss 0.6953401012570047\n",
            "2022-03-30 22:05:53.202126 Epoch 100, Training loss 0.6934693844803154\n",
            "2022-03-30 22:06:02.163642 Epoch 101, Training loss 0.6924204133127047\n",
            "2022-03-30 22:06:11.128614 Epoch 102, Training loss 0.6903570583638023\n",
            "2022-03-30 22:06:20.478882 Epoch 103, Training loss 0.6880107248378227\n",
            "2022-03-30 22:06:30.119694 Epoch 104, Training loss 0.6855561853293568\n",
            "2022-03-30 22:06:39.354846 Epoch 105, Training loss 0.6863525494971239\n",
            "2022-03-30 22:06:48.346394 Epoch 106, Training loss 0.6852656459183339\n",
            "2022-03-30 22:06:57.403274 Epoch 107, Training loss 0.6829958540170699\n",
            "2022-03-30 22:07:06.483781 Epoch 108, Training loss 0.6802224276010947\n",
            "2022-03-30 22:07:15.986227 Epoch 109, Training loss 0.6785900513153247\n",
            "2022-03-30 22:07:25.427368 Epoch 110, Training loss 0.6764271107628522\n",
            "2022-03-30 22:07:34.800869 Epoch 111, Training loss 0.6750663827980876\n",
            "2022-03-30 22:07:43.834297 Epoch 112, Training loss 0.6742061402105615\n",
            "2022-03-30 22:07:53.015952 Epoch 113, Training loss 0.6725555982659844\n",
            "2022-03-30 22:08:02.108438 Epoch 114, Training loss 0.671075109097049\n",
            "2022-03-30 22:08:11.141724 Epoch 115, Training loss 0.6686356569571263\n",
            "2022-03-30 22:08:20.736221 Epoch 116, Training loss 0.668853121111765\n",
            "2022-03-30 22:08:29.949220 Epoch 117, Training loss 0.6672705390950298\n",
            "2022-03-30 22:08:39.008492 Epoch 118, Training loss 0.6636504391422662\n",
            "2022-03-30 22:08:48.124547 Epoch 119, Training loss 0.6635437899309656\n",
            "2022-03-30 22:08:57.349304 Epoch 120, Training loss 0.6624484817923793\n",
            "2022-03-30 22:09:06.674721 Epoch 121, Training loss 0.6597274037654443\n",
            "2022-03-30 22:09:16.004558 Epoch 122, Training loss 0.6602132171392441\n",
            "2022-03-30 22:09:25.040560 Epoch 123, Training loss 0.6574857485721178\n",
            "2022-03-30 22:09:34.069709 Epoch 124, Training loss 0.6558408390759202\n",
            "2022-03-30 22:09:43.268808 Epoch 125, Training loss 0.6556795852644669\n",
            "2022-03-30 22:09:52.351448 Epoch 126, Training loss 0.655331818725142\n",
            "2022-03-30 22:10:01.735724 Epoch 127, Training loss 0.6523446056162915\n",
            "2022-03-30 22:10:10.902844 Epoch 128, Training loss 0.6509995203646247\n",
            "2022-03-30 22:10:20.167422 Epoch 129, Training loss 0.6489537016052724\n",
            "2022-03-30 22:10:29.424891 Epoch 130, Training loss 0.6468728536077778\n",
            "2022-03-30 22:10:38.400052 Epoch 131, Training loss 0.64756554948247\n",
            "2022-03-30 22:10:47.373549 Epoch 132, Training loss 0.6448004805218533\n",
            "2022-03-30 22:10:56.667077 Epoch 133, Training loss 0.6438032427178625\n",
            "2022-03-30 22:11:05.834464 Epoch 134, Training loss 0.6432562174699495\n",
            "2022-03-30 22:11:15.060612 Epoch 135, Training loss 0.6409410761902704\n",
            "2022-03-30 22:11:24.242770 Epoch 136, Training loss 0.641224520476273\n",
            "2022-03-30 22:11:33.514368 Epoch 137, Training loss 0.6394911735030391\n",
            "2022-03-30 22:11:42.888036 Epoch 138, Training loss 0.6398106212429988\n",
            "2022-03-30 22:11:52.428739 Epoch 139, Training loss 0.6383180107606952\n",
            "2022-03-30 22:12:01.741816 Epoch 140, Training loss 0.6353934128647265\n",
            "2022-03-30 22:12:10.791751 Epoch 141, Training loss 0.6361407419604719\n",
            "2022-03-30 22:12:19.871948 Epoch 142, Training loss 0.6355939750247599\n",
            "2022-03-30 22:12:29.300525 Epoch 143, Training loss 0.632771538575287\n",
            "2022-03-30 22:12:38.673931 Epoch 144, Training loss 0.6315103134383326\n",
            "2022-03-30 22:12:47.831238 Epoch 145, Training loss 0.6311813866924447\n",
            "2022-03-30 22:12:57.283697 Epoch 146, Training loss 0.6276447433035087\n",
            "2022-03-30 22:13:06.479290 Epoch 147, Training loss 0.6273789360852497\n",
            "2022-03-30 22:13:15.554244 Epoch 148, Training loss 0.6277551002362195\n",
            "2022-03-30 22:13:24.792849 Epoch 149, Training loss 0.6252391067764643\n",
            "2022-03-30 22:13:34.157089 Epoch 150, Training loss 0.6249541742417514\n",
            "2022-03-30 22:13:43.821873 Epoch 151, Training loss 0.6228288931919791\n",
            "2022-03-30 22:13:52.894537 Epoch 152, Training loss 0.6225252835570699\n",
            "2022-03-30 22:14:02.047290 Epoch 153, Training loss 0.620763855517063\n",
            "2022-03-30 22:14:11.083985 Epoch 154, Training loss 0.6200937451151631\n",
            "2022-03-30 22:14:20.266579 Epoch 155, Training loss 0.6207668937151999\n",
            "2022-03-30 22:14:29.705444 Epoch 156, Training loss 0.6182331841086488\n",
            "2022-03-30 22:14:39.024820 Epoch 157, Training loss 0.618890442659178\n",
            "2022-03-30 22:14:48.166743 Epoch 158, Training loss 0.6161579999624921\n",
            "2022-03-30 22:14:57.150828 Epoch 159, Training loss 0.61512206376666\n",
            "2022-03-30 22:15:06.286106 Epoch 160, Training loss 0.6149908573654912\n",
            "2022-03-30 22:15:15.690303 Epoch 161, Training loss 0.6134531970524117\n",
            "2022-03-30 22:15:25.047403 Epoch 162, Training loss 0.6117871884266128\n",
            "2022-03-30 22:15:34.469508 Epoch 163, Training loss 0.612565343756505\n",
            "2022-03-30 22:15:43.508038 Epoch 164, Training loss 0.6102185692552411\n",
            "2022-03-30 22:15:52.499089 Epoch 165, Training loss 0.6102405014397848\n",
            "2022-03-30 22:16:01.444153 Epoch 166, Training loss 0.6080317448091019\n",
            "2022-03-30 22:16:10.459958 Epoch 167, Training loss 0.6079085508499609\n",
            "2022-03-30 22:16:19.445427 Epoch 168, Training loss 0.6065516466527339\n",
            "2022-03-30 22:16:28.671011 Epoch 169, Training loss 0.6057027549389988\n",
            "2022-03-30 22:16:37.999205 Epoch 170, Training loss 0.6041058005045747\n",
            "2022-03-30 22:16:47.135158 Epoch 171, Training loss 0.603434867966358\n",
            "2022-03-30 22:16:56.312694 Epoch 172, Training loss 0.601862353818191\n",
            "2022-03-30 22:17:05.544737 Epoch 173, Training loss 0.6015464480575698\n",
            "2022-03-30 22:17:14.745802 Epoch 174, Training loss 0.6021945894984029\n",
            "2022-03-30 22:17:23.700070 Epoch 175, Training loss 0.6007774815230114\n",
            "2022-03-30 22:17:32.764737 Epoch 176, Training loss 0.5988488271641914\n",
            "2022-03-30 22:17:41.942800 Epoch 177, Training loss 0.5987988253078802\n",
            "2022-03-30 22:17:51.375993 Epoch 178, Training loss 0.5980256445648725\n",
            "2022-03-30 22:18:00.665045 Epoch 179, Training loss 0.5963589980474213\n",
            "2022-03-30 22:18:09.876037 Epoch 180, Training loss 0.595264792556653\n",
            "2022-03-30 22:18:19.059286 Epoch 181, Training loss 0.5960601915407668\n",
            "2022-03-30 22:18:28.370759 Epoch 182, Training loss 0.5951664182536133\n",
            "2022-03-30 22:18:37.710909 Epoch 183, Training loss 0.5941253723314656\n",
            "2022-03-30 22:18:46.860588 Epoch 184, Training loss 0.5929439265824035\n",
            "2022-03-30 22:18:55.849740 Epoch 185, Training loss 0.590596507958439\n",
            "2022-03-30 22:19:05.180751 Epoch 186, Training loss 0.5904126460366237\n",
            "2022-03-30 22:19:14.424089 Epoch 187, Training loss 0.5908770759392272\n",
            "2022-03-30 22:19:23.603806 Epoch 188, Training loss 0.5896340073526972\n",
            "2022-03-30 22:19:32.718711 Epoch 189, Training loss 0.5890325949624982\n",
            "2022-03-30 22:19:42.061055 Epoch 190, Training loss 0.5877727309380041\n",
            "2022-03-30 22:19:51.252897 Epoch 191, Training loss 0.5843707954563448\n",
            "2022-03-30 22:20:00.415485 Epoch 192, Training loss 0.5866158345852361\n",
            "2022-03-30 22:20:09.484933 Epoch 193, Training loss 0.5859564804207639\n",
            "2022-03-30 22:20:18.587504 Epoch 194, Training loss 0.5840524685047471\n",
            "2022-03-30 22:20:27.777481 Epoch 195, Training loss 0.5830505990311313\n",
            "2022-03-30 22:20:36.987657 Epoch 196, Training loss 0.5830254667936383\n",
            "2022-03-30 22:20:46.474528 Epoch 197, Training loss 0.583013659967181\n",
            "2022-03-30 22:20:55.688326 Epoch 198, Training loss 0.5823015945646769\n",
            "2022-03-30 22:21:04.929220 Epoch 199, Training loss 0.581307690047547\n",
            "2022-03-30 22:21:14.087585 Epoch 200, Training loss 0.5808020277553813\n",
            "2022-03-30 22:21:23.247535 Epoch 201, Training loss 0.5791175698151674\n",
            "2022-03-30 22:21:32.401758 Epoch 202, Training loss 0.5782750707376948\n",
            "2022-03-30 22:21:41.411977 Epoch 203, Training loss 0.5771224143941079\n",
            "2022-03-30 22:21:50.696531 Epoch 204, Training loss 0.5772079635230477\n",
            "2022-03-30 22:21:59.967700 Epoch 205, Training loss 0.5754631779459126\n",
            "2022-03-30 22:22:09.187256 Epoch 206, Training loss 0.5767562616892787\n",
            "2022-03-30 22:22:18.616897 Epoch 207, Training loss 0.5746470902429517\n",
            "2022-03-30 22:22:27.851489 Epoch 208, Training loss 0.575638769990038\n",
            "2022-03-30 22:22:37.268262 Epoch 209, Training loss 0.5747662093252173\n",
            "2022-03-30 22:22:46.281598 Epoch 210, Training loss 0.5730643422173722\n",
            "2022-03-30 22:22:55.500148 Epoch 211, Training loss 0.5718499983058256\n",
            "2022-03-30 22:23:05.168174 Epoch 212, Training loss 0.5720381693690634\n",
            "2022-03-30 22:23:14.498714 Epoch 213, Training loss 0.5715089830977228\n",
            "2022-03-30 22:23:23.865675 Epoch 214, Training loss 0.5705257739176226\n",
            "2022-03-30 22:23:33.010625 Epoch 215, Training loss 0.5682115675238393\n",
            "2022-03-30 22:23:42.053656 Epoch 216, Training loss 0.569437882250837\n",
            "2022-03-30 22:23:51.074389 Epoch 217, Training loss 0.5685722938431498\n",
            "2022-03-30 22:24:00.285233 Epoch 218, Training loss 0.5681128810205118\n",
            "2022-03-30 22:24:09.420105 Epoch 219, Training loss 0.5657234415221397\n",
            "2022-03-30 22:24:18.381725 Epoch 220, Training loss 0.5665664188468548\n",
            "2022-03-30 22:24:27.423929 Epoch 221, Training loss 0.5668996638425475\n",
            "2022-03-30 22:24:36.700465 Epoch 222, Training loss 0.5660095667595144\n",
            "2022-03-30 22:24:46.045506 Epoch 223, Training loss 0.5650847771817156\n",
            "2022-03-30 22:24:55.486066 Epoch 224, Training loss 0.5661559283657147\n",
            "2022-03-30 22:25:04.621233 Epoch 225, Training loss 0.5632374667755479\n",
            "2022-03-30 22:25:13.787362 Epoch 226, Training loss 0.5621518882186821\n",
            "2022-03-30 22:25:22.966750 Epoch 227, Training loss 0.5623804516804493\n",
            "2022-03-30 22:25:32.190808 Epoch 228, Training loss 0.5626861387887574\n",
            "2022-03-30 22:25:41.195706 Epoch 229, Training loss 0.5614985685671687\n",
            "2022-03-30 22:25:50.293432 Epoch 230, Training loss 0.5591538511883573\n",
            "2022-03-30 22:25:59.382086 Epoch 231, Training loss 0.5584787173420572\n",
            "2022-03-30 22:26:08.558696 Epoch 232, Training loss 0.5610517755036464\n",
            "2022-03-30 22:26:17.881965 Epoch 233, Training loss 0.5569802189574522\n",
            "2022-03-30 22:26:27.178874 Epoch 234, Training loss 0.5589845425942365\n",
            "2022-03-30 22:26:36.387993 Epoch 235, Training loss 0.5570936244924355\n",
            "2022-03-30 22:26:45.520934 Epoch 236, Training loss 0.5552876491833221\n",
            "2022-03-30 22:26:54.658503 Epoch 237, Training loss 0.5566684197053275\n",
            "2022-03-30 22:27:03.559115 Epoch 238, Training loss 0.5566939577422179\n",
            "2022-03-30 22:27:12.985894 Epoch 239, Training loss 0.5546846840806934\n",
            "2022-03-30 22:27:22.134765 Epoch 240, Training loss 0.5534478684368036\n",
            "2022-03-30 22:27:31.143317 Epoch 241, Training loss 0.5542183465817395\n",
            "2022-03-30 22:27:40.230905 Epoch 242, Training loss 0.5535078859695083\n",
            "2022-03-30 22:27:49.355970 Epoch 243, Training loss 0.5512051640645318\n",
            "2022-03-30 22:27:58.569709 Epoch 244, Training loss 0.5523991140029619\n",
            "2022-03-30 22:28:07.923035 Epoch 245, Training loss 0.5516812087172438\n",
            "2022-03-30 22:28:16.916186 Epoch 246, Training loss 0.5497242220108162\n",
            "2022-03-30 22:28:25.906878 Epoch 247, Training loss 0.5498421264578924\n",
            "2022-03-30 22:28:35.241719 Epoch 248, Training loss 0.5499643864076765\n",
            "2022-03-30 22:28:44.421088 Epoch 249, Training loss 0.547778082396978\n",
            "2022-03-30 22:28:53.557171 Epoch 250, Training loss 0.5482259956390961\n",
            "2022-03-30 22:29:02.976952 Epoch 251, Training loss 0.5488626916161583\n",
            "2022-03-30 22:29:12.085961 Epoch 252, Training loss 0.5469979613905063\n",
            "2022-03-30 22:29:21.222900 Epoch 253, Training loss 0.5469885640551367\n",
            "2022-03-30 22:29:30.426694 Epoch 254, Training loss 0.5471197256194357\n",
            "2022-03-30 22:29:39.394121 Epoch 255, Training loss 0.5470906138191443\n",
            "2022-03-30 22:29:48.620720 Epoch 256, Training loss 0.546164262172816\n",
            "2022-03-30 22:29:57.831337 Epoch 257, Training loss 0.5442578400987799\n",
            "2022-03-30 22:30:06.854493 Epoch 258, Training loss 0.544722447462399\n",
            "2022-03-30 22:30:15.999037 Epoch 259, Training loss 0.5434504943468687\n",
            "2022-03-30 22:30:25.222805 Epoch 260, Training loss 0.5439252596148445\n",
            "2022-03-30 22:30:34.442963 Epoch 261, Training loss 0.5432759502812115\n",
            "2022-03-30 22:30:43.649223 Epoch 262, Training loss 0.5413198813872264\n",
            "2022-03-30 22:30:52.743034 Epoch 263, Training loss 0.5424991878668975\n",
            "2022-03-30 22:31:01.706315 Epoch 264, Training loss 0.5418448513731018\n",
            "2022-03-30 22:31:10.816615 Epoch 265, Training loss 0.5396242622676712\n",
            "2022-03-30 22:31:19.823079 Epoch 266, Training loss 0.540044711373956\n",
            "2022-03-30 22:31:29.200605 Epoch 267, Training loss 0.5398534813424205\n",
            "2022-03-30 22:31:38.494328 Epoch 268, Training loss 0.5393453235821346\n",
            "2022-03-30 22:31:47.679113 Epoch 269, Training loss 0.5396714285206612\n",
            "2022-03-30 22:31:56.811587 Epoch 270, Training loss 0.5379915311742012\n",
            "2022-03-30 22:32:05.971637 Epoch 271, Training loss 0.538887656634421\n",
            "2022-03-30 22:32:15.181256 Epoch 272, Training loss 0.5363999548013253\n",
            "2022-03-30 22:32:24.121174 Epoch 273, Training loss 0.5371906188747767\n",
            "2022-03-30 22:32:33.343817 Epoch 274, Training loss 0.5368553251409165\n",
            "2022-03-30 22:32:42.503140 Epoch 275, Training loss 0.5353480086607092\n",
            "2022-03-30 22:32:51.800701 Epoch 276, Training loss 0.5345817915999981\n",
            "2022-03-30 22:33:01.086614 Epoch 277, Training loss 0.5357385088148934\n",
            "2022-03-30 22:33:10.264317 Epoch 278, Training loss 0.5336719725061866\n",
            "2022-03-30 22:33:19.389644 Epoch 279, Training loss 0.533338315079889\n",
            "2022-03-30 22:33:28.573084 Epoch 280, Training loss 0.5339034188662648\n",
            "2022-03-30 22:33:37.613669 Epoch 281, Training loss 0.5345911618769931\n",
            "2022-03-30 22:33:46.692555 Epoch 282, Training loss 0.5328000721800358\n",
            "2022-03-30 22:33:56.065879 Epoch 283, Training loss 0.5306786385643513\n",
            "2022-03-30 22:34:05.460236 Epoch 284, Training loss 0.5314817252899985\n",
            "2022-03-30 22:34:14.623633 Epoch 285, Training loss 0.531600386094864\n",
            "2022-03-30 22:34:23.904015 Epoch 286, Training loss 0.5310857643937821\n",
            "2022-03-30 22:34:33.056129 Epoch 287, Training loss 0.5298091613346964\n",
            "2022-03-30 22:34:42.311640 Epoch 288, Training loss 0.529304342044284\n",
            "2022-03-30 22:34:51.536346 Epoch 289, Training loss 0.5294281205977015\n",
            "2022-03-30 22:35:00.567243 Epoch 290, Training loss 0.5273676527392529\n",
            "2022-03-30 22:35:09.789405 Epoch 291, Training loss 0.5302051728605615\n",
            "2022-03-30 22:35:18.946229 Epoch 292, Training loss 0.5300124439094073\n",
            "2022-03-30 22:35:28.241903 Epoch 293, Training loss 0.5278941139464488\n",
            "2022-03-30 22:35:37.666156 Epoch 294, Training loss 0.5267389620966314\n",
            "2022-03-30 22:35:46.774673 Epoch 295, Training loss 0.527898856021864\n",
            "2022-03-30 22:35:56.017501 Epoch 296, Training loss 0.5265592464133907\n",
            "2022-03-30 22:36:05.170451 Epoch 297, Training loss 0.524987024724331\n",
            "2022-03-30 22:36:14.442613 Epoch 298, Training loss 0.5261732253348431\n",
            "2022-03-30 22:36:23.354770 Epoch 299, Training loss 0.5245524141413477\n",
            "2022-03-30 22:36:32.510412 Epoch 300, Training loss 0.5247340846015974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md1rDBEm-vuN",
        "outputId": "3c8ab3f1-d54e-4a06-8935-815eac39f8f6"
      },
      "id": "md1rDBEm-vuN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.81\n",
            "Accuracy val: 0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################################################################\n",
        "#########Problem 2 PART 3######### 3/3 (Weight Decay)\n",
        "##############################################################################################################################"
      ],
      "metadata": {
        "id": "99fUAhXM-n8r"
      },
      "id": "99fUAhXM-n8r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, \n",
        "                  train_loader):\n",
        "  for epoch in range(1, n_epochs +1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      outputs = model(imgs.to('cuda:0'))\n",
        "      loss = loss_fn(outputs.to('cuda:0'), \n",
        "                     labels.to('cuda:0'))\n",
        "      \n",
        "      ambda = 0.001\n",
        "      norm = sum(p.pow(2.0).sum()\n",
        "                    for p in model.parameters())\n",
        "    \n",
        "      loss = loss + ambda*norm\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_train += loss.item()\n",
        "     \n",
        "    print('{} Epoch {}, Training Loss {}'.format(datetime.datetime.now(),\n",
        "                                    epoch, loss_train / len(train_loader)))"
      ],
      "metadata": {
        "id": "ZkkyoWWc-r3J"
      },
      "id": "ZkkyoWWc-r3J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NetRes()\n",
        "model.to('cuda:0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ObH5bAUPNU9",
        "outputId": "fb6a0e5a-5f3e-4371-c875-31d6fd6f2481"
      },
      "id": "7ObH5bAUPNU9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NetRes(\n",
              "  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, \n",
        "                    batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to('cuda:0')\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs =300,\n",
        "    optimizer= optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi57ZM8UPZ7Q",
        "outputId": "0fdc6257-d3f1-4936-eee4-a0884f5977d5"
      },
      "id": "Xi57ZM8UPZ7Q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-30 23:52:50.730233 Epoch 1, Training Loss 2.072790256394145\n",
            "2022-03-30 23:53:00.858566 Epoch 2, Training Loss 1.7770396222543838\n",
            "2022-03-30 23:53:10.865084 Epoch 3, Training Loss 1.616920314176613\n",
            "2022-03-30 23:53:20.822348 Epoch 4, Training Loss 1.5275759218293992\n",
            "2022-03-30 23:53:30.979104 Epoch 5, Training Loss 1.4489904951561443\n",
            "2022-03-30 23:53:40.945324 Epoch 6, Training Loss 1.3861161008515321\n",
            "2022-03-30 23:53:50.742502 Epoch 7, Training Loss 1.3383906567493058\n",
            "2022-03-30 23:54:00.764503 Epoch 8, Training Loss 1.2991474222039323\n",
            "2022-03-30 23:54:10.673615 Epoch 9, Training Loss 1.2664397182062155\n",
            "2022-03-30 23:54:20.610602 Epoch 10, Training Loss 1.239240956047307\n",
            "2022-03-30 23:54:30.505538 Epoch 11, Training Loss 1.2165156798746886\n",
            "2022-03-30 23:54:40.277176 Epoch 12, Training Loss 1.1966671775216642\n",
            "2022-03-30 23:54:50.195609 Epoch 13, Training Loss 1.1801723683124308\n",
            "2022-03-30 23:55:00.284385 Epoch 14, Training Loss 1.165393960216771\n",
            "2022-03-30 23:55:10.238828 Epoch 15, Training Loss 1.1519162623626191\n",
            "2022-03-30 23:55:20.302087 Epoch 16, Training Loss 1.1367849352414652\n",
            "2022-03-30 23:55:30.551902 Epoch 17, Training Loss 1.123984235098295\n",
            "2022-03-30 23:55:40.300715 Epoch 18, Training Loss 1.1115116794091051\n",
            "2022-03-30 23:55:50.095627 Epoch 19, Training Loss 1.1007485869900344\n",
            "2022-03-30 23:55:59.891636 Epoch 20, Training Loss 1.0900108157216435\n",
            "2022-03-30 23:56:10.020655 Epoch 21, Training Loss 1.0802677043563569\n",
            "2022-03-30 23:56:19.817506 Epoch 22, Training Loss 1.071541487103533\n",
            "2022-03-30 23:56:29.667581 Epoch 23, Training Loss 1.0639133059307742\n",
            "2022-03-30 23:56:39.740825 Epoch 24, Training Loss 1.0552966865279791\n",
            "2022-03-30 23:56:50.008884 Epoch 25, Training Loss 1.0465075906432804\n",
            "2022-03-30 23:56:59.919013 Epoch 26, Training Loss 1.0430185349701007\n",
            "2022-03-30 23:57:09.774764 Epoch 27, Training Loss 1.03576119080224\n",
            "2022-03-30 23:57:19.829740 Epoch 28, Training Loss 1.028756222304176\n",
            "2022-03-30 23:57:29.914087 Epoch 29, Training Loss 1.0251606336182646\n",
            "2022-03-30 23:57:40.068619 Epoch 30, Training Loss 1.0193311702412413\n",
            "2022-03-30 23:57:50.190732 Epoch 31, Training Loss 1.014902003890718\n",
            "2022-03-30 23:58:00.377648 Epoch 32, Training Loss 1.0111856990305663\n",
            "2022-03-30 23:58:10.538127 Epoch 33, Training Loss 1.0078801789399607\n",
            "2022-03-30 23:58:20.810029 Epoch 34, Training Loss 1.0043534481769327\n",
            "2022-03-30 23:58:30.868603 Epoch 35, Training Loss 1.002543562589704\n",
            "2022-03-30 23:58:41.554765 Epoch 36, Training Loss 0.9973879259108277\n",
            "2022-03-30 23:58:52.070288 Epoch 37, Training Loss 0.995697972948289\n",
            "2022-03-30 23:59:02.395631 Epoch 38, Training Loss 0.9912538332554995\n",
            "2022-03-30 23:59:12.407913 Epoch 39, Training Loss 0.9888417290147308\n",
            "2022-03-30 23:59:22.563033 Epoch 40, Training Loss 0.9874817946225481\n",
            "2022-03-30 23:59:32.735873 Epoch 41, Training Loss 0.9841301771228576\n",
            "2022-03-30 23:59:42.850938 Epoch 42, Training Loss 0.981818057844401\n",
            "2022-03-30 23:59:52.636331 Epoch 43, Training Loss 0.9800842674187077\n",
            "2022-03-31 00:00:02.672191 Epoch 44, Training Loss 0.9777239945995838\n",
            "2022-03-31 00:00:12.669177 Epoch 45, Training Loss 0.9763469676989729\n",
            "2022-03-31 00:00:22.945441 Epoch 46, Training Loss 0.9716293780547579\n",
            "2022-03-31 00:00:33.056150 Epoch 47, Training Loss 0.9709076955342841\n",
            "2022-03-31 00:00:43.115747 Epoch 48, Training Loss 0.970457361634735\n",
            "2022-03-31 00:00:53.199357 Epoch 49, Training Loss 0.9686591333288062\n",
            "2022-03-31 00:01:02.965935 Epoch 50, Training Loss 0.9665103185817104\n",
            "2022-03-31 00:01:12.849518 Epoch 51, Training Loss 0.964869138987168\n",
            "2022-03-31 00:01:23.304220 Epoch 52, Training Loss 0.9626221329812199\n",
            "2022-03-31 00:01:33.341031 Epoch 53, Training Loss 0.9620676787612993\n",
            "2022-03-31 00:01:43.393058 Epoch 54, Training Loss 0.9582256341681761\n",
            "2022-03-31 00:01:53.516355 Epoch 55, Training Loss 0.9589466225460667\n",
            "2022-03-31 00:02:03.285714 Epoch 56, Training Loss 0.9588488091135878\n",
            "2022-03-31 00:02:13.489662 Epoch 57, Training Loss 0.9572395310377526\n",
            "2022-03-31 00:02:23.445542 Epoch 58, Training Loss 0.9558405305266076\n",
            "2022-03-31 00:02:33.347401 Epoch 59, Training Loss 0.955150911951309\n",
            "2022-03-31 00:02:43.232184 Epoch 60, Training Loss 0.9519351311504384\n",
            "2022-03-31 00:02:53.246667 Epoch 61, Training Loss 0.9516926677635563\n",
            "2022-03-31 00:03:03.275081 Epoch 62, Training Loss 0.9487277927910885\n",
            "2022-03-31 00:03:13.230453 Epoch 63, Training Loss 0.9497313532987823\n",
            "2022-03-31 00:03:23.401469 Epoch 64, Training Loss 0.9475793870513701\n",
            "2022-03-31 00:03:33.460936 Epoch 65, Training Loss 0.9464902302340779\n",
            "2022-03-31 00:03:43.258991 Epoch 66, Training Loss 0.9484669918294453\n",
            "2022-03-31 00:03:53.011397 Epoch 67, Training Loss 0.9451663279167527\n",
            "2022-03-31 00:04:03.046706 Epoch 68, Training Loss 0.9450112162038798\n",
            "2022-03-31 00:04:13.226536 Epoch 69, Training Loss 0.9439281411183155\n",
            "2022-03-31 00:04:23.324643 Epoch 70, Training Loss 0.9431036720647836\n",
            "2022-03-31 00:04:33.294605 Epoch 71, Training Loss 0.9413433890513447\n",
            "2022-03-31 00:04:43.438215 Epoch 72, Training Loss 0.942349399523357\n",
            "2022-03-31 00:04:53.633628 Epoch 73, Training Loss 0.9402721316155875\n",
            "2022-03-31 00:05:03.653202 Epoch 74, Training Loss 0.9390495400447065\n",
            "2022-03-31 00:05:13.340131 Epoch 75, Training Loss 0.9401209985508638\n",
            "2022-03-31 00:05:23.286079 Epoch 76, Training Loss 0.9385844216017467\n",
            "2022-03-31 00:05:33.051670 Epoch 77, Training Loss 0.9381687145708771\n",
            "2022-03-31 00:05:43.153583 Epoch 78, Training Loss 0.9360883078154396\n",
            "2022-03-31 00:05:53.381890 Epoch 79, Training Loss 0.9356299168465997\n",
            "2022-03-31 00:06:03.329030 Epoch 80, Training Loss 0.9334030141458487\n",
            "2022-03-31 00:06:13.295449 Epoch 81, Training Loss 0.9339141900582082\n",
            "2022-03-31 00:06:23.309465 Epoch 82, Training Loss 0.9327401173541613\n",
            "2022-03-31 00:06:33.040620 Epoch 83, Training Loss 0.9329837892976258\n",
            "2022-03-31 00:06:42.889469 Epoch 84, Training Loss 0.9325700916750047\n",
            "2022-03-31 00:06:52.675920 Epoch 85, Training Loss 0.9326877373716106\n",
            "2022-03-31 00:07:02.878614 Epoch 86, Training Loss 0.9306378248707413\n",
            "2022-03-31 00:07:12.965641 Epoch 87, Training Loss 0.9317557469505788\n",
            "2022-03-31 00:07:22.988198 Epoch 88, Training Loss 0.9310443216882398\n",
            "2022-03-31 00:07:33.267079 Epoch 89, Training Loss 0.9299712735978539\n",
            "2022-03-31 00:07:43.126151 Epoch 90, Training Loss 0.9291197415965292\n",
            "2022-03-31 00:07:52.858134 Epoch 91, Training Loss 0.9275216157631496\n",
            "2022-03-31 00:08:02.933812 Epoch 92, Training Loss 0.9269325750715592\n",
            "2022-03-31 00:08:12.990268 Epoch 93, Training Loss 0.9275558729610784\n",
            "2022-03-31 00:08:22.877265 Epoch 94, Training Loss 0.9268880715150662\n",
            "2022-03-31 00:08:33.141325 Epoch 95, Training Loss 0.925042478019929\n",
            "2022-03-31 00:08:43.127928 Epoch 96, Training Loss 0.9258355394653652\n",
            "2022-03-31 00:08:52.934010 Epoch 97, Training Loss 0.9252837993909636\n",
            "2022-03-31 00:09:02.953740 Epoch 98, Training Loss 0.923510310899876\n",
            "2022-03-31 00:09:12.721065 Epoch 99, Training Loss 0.9250770289727184\n",
            "2022-03-31 00:09:22.788165 Epoch 100, Training Loss 0.9226008623914645\n",
            "2022-03-31 00:09:32.806099 Epoch 101, Training Loss 0.9230402951197856\n",
            "2022-03-31 00:09:42.638079 Epoch 102, Training Loss 0.9208985178366952\n",
            "2022-03-31 00:09:52.551488 Epoch 103, Training Loss 0.9213555440725878\n",
            "2022-03-31 00:10:02.697549 Epoch 104, Training Loss 0.9201101937409862\n",
            "2022-03-31 00:10:12.856789 Epoch 105, Training Loss 0.9194933396318684\n",
            "2022-03-31 00:10:23.059987 Epoch 106, Training Loss 0.9195312929275395\n",
            "2022-03-31 00:10:32.911835 Epoch 107, Training Loss 0.9211107219576531\n",
            "2022-03-31 00:10:42.786442 Epoch 108, Training Loss 0.9203609443838944\n",
            "2022-03-31 00:10:52.524419 Epoch 109, Training Loss 0.9183585920449718\n",
            "2022-03-31 00:11:02.801614 Epoch 110, Training Loss 0.917692399771927\n",
            "2022-03-31 00:11:13.153424 Epoch 111, Training Loss 0.9170024843929369\n",
            "2022-03-31 00:11:23.026777 Epoch 112, Training Loss 0.9181287761996774\n",
            "2022-03-31 00:11:33.190155 Epoch 113, Training Loss 0.9168079373476755\n",
            "2022-03-31 00:11:42.983923 Epoch 114, Training Loss 0.9162839247899897\n",
            "2022-03-31 00:11:52.887700 Epoch 115, Training Loss 0.9174867469788818\n",
            "2022-03-31 00:12:02.927422 Epoch 116, Training Loss 0.9154917272307989\n",
            "2022-03-31 00:12:12.883687 Epoch 117, Training Loss 0.9143209397945258\n",
            "2022-03-31 00:12:22.815706 Epoch 118, Training Loss 0.9159069714491325\n",
            "2022-03-31 00:12:33.155432 Epoch 119, Training Loss 0.9163485500208862\n",
            "2022-03-31 00:12:43.259428 Epoch 120, Training Loss 0.9140511098725107\n",
            "2022-03-31 00:12:53.644649 Epoch 121, Training Loss 0.9144733213555173\n",
            "2022-03-31 00:13:03.659225 Epoch 122, Training Loss 0.9148272076226256\n",
            "2022-03-31 00:13:13.523578 Epoch 123, Training Loss 0.9135623885237653\n",
            "2022-03-31 00:13:23.238918 Epoch 124, Training Loss 0.9122886291855131\n",
            "2022-03-31 00:13:33.354974 Epoch 125, Training Loss 0.9132429753118159\n",
            "2022-03-31 00:13:43.514750 Epoch 126, Training Loss 0.911548171964143\n",
            "2022-03-31 00:13:53.529631 Epoch 127, Training Loss 0.9119843780384649\n",
            "2022-03-31 00:14:03.426021 Epoch 128, Training Loss 0.9127025114148474\n",
            "2022-03-31 00:14:13.604878 Epoch 129, Training Loss 0.9106621080652222\n",
            "2022-03-31 00:14:23.643393 Epoch 130, Training Loss 0.9111171961592897\n",
            "2022-03-31 00:14:33.513624 Epoch 131, Training Loss 0.9106149954716568\n",
            "2022-03-31 00:14:43.493735 Epoch 132, Training Loss 0.9118292846185777\n",
            "2022-03-31 00:14:53.581458 Epoch 133, Training Loss 0.9088113134169518\n",
            "2022-03-31 00:15:03.948287 Epoch 134, Training Loss 0.9111629069766121\n",
            "2022-03-31 00:15:14.075854 Epoch 135, Training Loss 0.909654602980065\n",
            "2022-03-31 00:15:24.149291 Epoch 136, Training Loss 0.9089712312306895\n",
            "2022-03-31 00:15:34.340281 Epoch 137, Training Loss 0.9087162994210373\n",
            "2022-03-31 00:15:44.551382 Epoch 138, Training Loss 0.910290999028384\n",
            "2022-03-31 00:15:54.236853 Epoch 139, Training Loss 0.9087748655577754\n",
            "2022-03-31 00:16:04.173651 Epoch 140, Training Loss 0.907261351718927\n",
            "2022-03-31 00:16:14.428804 Epoch 141, Training Loss 0.9092824504808392\n",
            "2022-03-31 00:16:24.210382 Epoch 142, Training Loss 0.9085707156097188\n",
            "2022-03-31 00:16:34.660279 Epoch 143, Training Loss 0.9076492234569071\n",
            "2022-03-31 00:16:44.692397 Epoch 144, Training Loss 0.9078733550618067\n",
            "2022-03-31 00:16:54.668228 Epoch 145, Training Loss 0.9066194677749253\n",
            "2022-03-31 00:17:04.570170 Epoch 146, Training Loss 0.9076542403844311\n",
            "2022-03-31 00:17:14.298493 Epoch 147, Training Loss 0.9066464706607487\n",
            "2022-03-31 00:17:24.304714 Epoch 148, Training Loss 0.9059945710784639\n",
            "2022-03-31 00:17:34.443591 Epoch 149, Training Loss 0.9059447963981677\n",
            "2022-03-31 00:17:44.645913 Epoch 150, Training Loss 0.9062748078799918\n",
            "2022-03-31 00:17:54.673059 Epoch 151, Training Loss 0.90489797701921\n",
            "2022-03-31 00:18:04.796465 Epoch 152, Training Loss 0.9055104516351314\n",
            "2022-03-31 00:18:14.914445 Epoch 153, Training Loss 0.9052306099620926\n",
            "2022-03-31 00:18:24.913023 Epoch 154, Training Loss 0.9063349325028832\n",
            "2022-03-31 00:18:34.740727 Epoch 155, Training Loss 0.9046214573904682\n",
            "2022-03-31 00:18:44.596535 Epoch 156, Training Loss 0.9052130903886713\n",
            "2022-03-31 00:18:54.824802 Epoch 157, Training Loss 0.905457682088208\n",
            "2022-03-31 00:19:04.850341 Epoch 158, Training Loss 0.9041823084701968\n",
            "2022-03-31 00:19:15.103469 Epoch 159, Training Loss 0.903430252657522\n",
            "2022-03-31 00:19:25.156188 Epoch 160, Training Loss 0.9043879551655801\n",
            "2022-03-31 00:19:35.286025 Epoch 161, Training Loss 0.9033786445627432\n",
            "2022-03-31 00:19:45.270233 Epoch 162, Training Loss 0.9041034310217708\n",
            "2022-03-31 00:19:55.232012 Epoch 163, Training Loss 0.9030992091464265\n",
            "2022-03-31 00:20:05.126225 Epoch 164, Training Loss 0.901424647215992\n",
            "2022-03-31 00:20:15.571539 Epoch 165, Training Loss 0.9023187865534097\n",
            "2022-03-31 00:20:25.638915 Epoch 166, Training Loss 0.9015819915115376\n",
            "2022-03-31 00:20:35.577259 Epoch 167, Training Loss 0.902921345456482\n",
            "2022-03-31 00:20:45.624031 Epoch 168, Training Loss 0.903178207785882\n",
            "2022-03-31 00:20:55.555047 Epoch 169, Training Loss 0.90262085290821\n",
            "2022-03-31 00:21:05.655917 Epoch 170, Training Loss 0.9029350771623499\n",
            "2022-03-31 00:21:15.650354 Epoch 171, Training Loss 0.9017889815218308\n",
            "2022-03-31 00:21:25.642173 Epoch 172, Training Loss 0.9033202695877046\n",
            "2022-03-31 00:21:35.841269 Epoch 173, Training Loss 0.9017139205237483\n",
            "2022-03-31 00:21:45.808316 Epoch 174, Training Loss 0.9016218092435461\n",
            "2022-03-31 00:21:55.666031 Epoch 175, Training Loss 0.9019965460843138\n",
            "2022-03-31 00:22:05.902050 Epoch 176, Training Loss 0.9010052629139113\n",
            "2022-03-31 00:22:15.916389 Epoch 177, Training Loss 0.9022290016074315\n",
            "2022-03-31 00:22:26.217262 Epoch 178, Training Loss 0.9024042394917334\n",
            "2022-03-31 00:22:36.209716 Epoch 179, Training Loss 0.9012339721860179\n",
            "2022-03-31 00:22:46.021243 Epoch 180, Training Loss 0.9005255978125746\n",
            "2022-03-31 00:22:55.895271 Epoch 181, Training Loss 0.8985794991483469\n",
            "2022-03-31 00:23:06.008235 Epoch 182, Training Loss 0.9014220932865387\n",
            "2022-03-31 00:23:15.938781 Epoch 183, Training Loss 0.8998581122254472\n",
            "2022-03-31 00:23:25.938767 Epoch 184, Training Loss 0.9007512714399402\n",
            "2022-03-31 00:23:36.033409 Epoch 185, Training Loss 0.900496391384193\n",
            "2022-03-31 00:23:45.923311 Epoch 186, Training Loss 0.9002728114652512\n",
            "2022-03-31 00:23:56.052877 Epoch 187, Training Loss 0.8994619553656225\n",
            "2022-03-31 00:24:06.085343 Epoch 188, Training Loss 0.8991505147703468\n",
            "2022-03-31 00:24:16.474402 Epoch 189, Training Loss 0.899685035962278\n",
            "2022-03-31 00:24:26.823483 Epoch 190, Training Loss 0.8994347063629219\n",
            "2022-03-31 00:24:36.921761 Epoch 191, Training Loss 0.8995876111795226\n",
            "2022-03-31 00:24:46.818201 Epoch 192, Training Loss 0.8984100304143813\n",
            "2022-03-31 00:24:56.758935 Epoch 193, Training Loss 0.9005761414080324\n",
            "2022-03-31 00:25:06.856678 Epoch 194, Training Loss 0.8989797602681553\n",
            "2022-03-31 00:25:16.766206 Epoch 195, Training Loss 0.8992872241208011\n",
            "2022-03-31 00:25:26.482214 Epoch 196, Training Loss 0.8997745801268331\n",
            "2022-03-31 00:25:36.596710 Epoch 197, Training Loss 0.8998521587732807\n",
            "2022-03-31 00:25:46.852752 Epoch 198, Training Loss 0.8977543444127378\n",
            "2022-03-31 00:25:56.876670 Epoch 199, Training Loss 0.9000371958288695\n",
            "2022-03-31 00:26:07.220904 Epoch 200, Training Loss 0.8988322044729882\n",
            "2022-03-31 00:26:17.231456 Epoch 201, Training Loss 0.8980205828119117\n",
            "2022-03-31 00:26:27.343853 Epoch 202, Training Loss 0.8975629637308438\n",
            "2022-03-31 00:26:37.437965 Epoch 203, Training Loss 0.8994561100707335\n",
            "2022-03-31 00:26:47.244468 Epoch 204, Training Loss 0.8968771215899826\n",
            "2022-03-31 00:26:57.752006 Epoch 205, Training Loss 0.898305431503774\n",
            "2022-03-31 00:27:07.910289 Epoch 206, Training Loss 0.897592908128753\n",
            "2022-03-31 00:27:17.998884 Epoch 207, Training Loss 0.8971960736662531\n",
            "2022-03-31 00:27:27.918531 Epoch 208, Training Loss 0.8968964547604856\n",
            "2022-03-31 00:27:38.047627 Epoch 209, Training Loss 0.8962233935475654\n",
            "2022-03-31 00:27:48.211690 Epoch 210, Training Loss 0.8963303745097821\n",
            "2022-03-31 00:27:58.224203 Epoch 211, Training Loss 0.897425563057975\n",
            "2022-03-31 00:28:08.176669 Epoch 212, Training Loss 0.8976098299026489\n",
            "2022-03-31 00:28:18.046989 Epoch 213, Training Loss 0.8975226719056248\n",
            "2022-03-31 00:28:27.974683 Epoch 214, Training Loss 0.8967327967171779\n",
            "2022-03-31 00:28:38.020005 Epoch 215, Training Loss 0.8952715753594322\n",
            "2022-03-31 00:28:48.228987 Epoch 216, Training Loss 0.897986747953288\n",
            "2022-03-31 00:28:58.268056 Epoch 217, Training Loss 0.8964077212926372\n",
            "2022-03-31 00:29:08.264768 Epoch 218, Training Loss 0.8967142204951752\n",
            "2022-03-31 00:29:18.427756 Epoch 219, Training Loss 0.8956496755180456\n",
            "2022-03-31 00:29:28.262140 Epoch 220, Training Loss 0.8976309124923423\n",
            "2022-03-31 00:29:38.459716 Epoch 221, Training Loss 0.8962723666902088\n",
            "2022-03-31 00:29:48.338804 Epoch 222, Training Loss 0.8951454809712022\n",
            "2022-03-31 00:29:58.238607 Epoch 223, Training Loss 0.8970003621200161\n",
            "2022-03-31 00:30:08.320072 Epoch 224, Training Loss 0.8954247546470379\n",
            "2022-03-31 00:30:18.484728 Epoch 225, Training Loss 0.8966586377919482\n",
            "2022-03-31 00:30:28.556721 Epoch 226, Training Loss 0.8955391378658811\n",
            "2022-03-31 00:30:38.507301 Epoch 227, Training Loss 0.8954672596186323\n",
            "2022-03-31 00:30:48.301012 Epoch 228, Training Loss 0.8960868370197618\n",
            "2022-03-31 00:30:58.483496 Epoch 229, Training Loss 0.8940774990469599\n",
            "2022-03-31 00:31:08.557694 Epoch 230, Training Loss 0.8941231382929761\n",
            "2022-03-31 00:31:18.682961 Epoch 231, Training Loss 0.8945410522201177\n",
            "2022-03-31 00:31:28.882446 Epoch 232, Training Loss 0.8948511140578238\n",
            "2022-03-31 00:31:39.121413 Epoch 233, Training Loss 0.8935695383554835\n",
            "2022-03-31 00:31:49.230787 Epoch 234, Training Loss 0.8942437527124839\n",
            "2022-03-31 00:31:59.185146 Epoch 235, Training Loss 0.8946155805874358\n",
            "2022-03-31 00:32:08.974837 Epoch 236, Training Loss 0.8959925669385954\n",
            "2022-03-31 00:32:19.266956 Epoch 237, Training Loss 0.8953226170576442\n",
            "2022-03-31 00:32:29.263524 Epoch 238, Training Loss 0.894816233983735\n",
            "2022-03-31 00:32:39.325458 Epoch 239, Training Loss 0.8955994573090692\n",
            "2022-03-31 00:32:49.296822 Epoch 240, Training Loss 0.8948912772985981\n",
            "2022-03-31 00:32:59.576086 Epoch 241, Training Loss 0.8944559632359869\n",
            "2022-03-31 00:33:09.524648 Epoch 242, Training Loss 0.8959603007797086\n",
            "2022-03-31 00:33:19.564500 Epoch 243, Training Loss 0.8933085927268123\n",
            "2022-03-31 00:33:29.380316 Epoch 244, Training Loss 0.8947374737628585\n",
            "2022-03-31 00:33:39.334392 Epoch 245, Training Loss 0.8949700318029165\n",
            "2022-03-31 00:33:49.277217 Epoch 246, Training Loss 0.8938152531681158\n",
            "2022-03-31 00:33:59.413303 Epoch 247, Training Loss 0.8950894465836723\n",
            "2022-03-31 00:34:09.677543 Epoch 248, Training Loss 0.8931762903090328\n",
            "2022-03-31 00:34:19.713586 Epoch 249, Training Loss 0.8948369852417265\n",
            "2022-03-31 00:34:30.057542 Epoch 250, Training Loss 0.8930609711753134\n",
            "2022-03-31 00:34:40.249436 Epoch 251, Training Loss 0.8945036374242105\n",
            "2022-03-31 00:34:50.137280 Epoch 252, Training Loss 0.8934216134231109\n",
            "2022-03-31 00:35:00.393903 Epoch 253, Training Loss 0.89368299053758\n",
            "2022-03-31 00:35:10.658388 Epoch 254, Training Loss 0.8937369611714502\n",
            "2022-03-31 00:35:20.647747 Epoch 255, Training Loss 0.8926742293340776\n",
            "2022-03-31 00:35:30.895372 Epoch 256, Training Loss 0.8919240553360765\n",
            "2022-03-31 00:35:41.069801 Epoch 257, Training Loss 0.893206091792992\n",
            "2022-03-31 00:35:50.902755 Epoch 258, Training Loss 0.893706711356902\n",
            "2022-03-31 00:36:00.954430 Epoch 259, Training Loss 0.893447499445942\n",
            "2022-03-31 00:36:10.744140 Epoch 260, Training Loss 0.8908499566947713\n",
            "2022-03-31 00:36:20.984081 Epoch 261, Training Loss 0.8930097376294148\n",
            "2022-03-31 00:36:31.212972 Epoch 262, Training Loss 0.8924789476730025\n",
            "2022-03-31 00:36:41.222975 Epoch 263, Training Loss 0.8941543336262179\n",
            "2022-03-31 00:36:51.349767 Epoch 264, Training Loss 0.8931605969853413\n",
            "2022-03-31 00:37:01.433372 Epoch 265, Training Loss 0.8926585487392552\n",
            "2022-03-31 00:37:11.478897 Epoch 266, Training Loss 0.8932521865343499\n",
            "2022-03-31 00:37:21.480642 Epoch 267, Training Loss 0.8923538539873059\n",
            "2022-03-31 00:37:31.256292 Epoch 268, Training Loss 0.8908439831203206\n",
            "2022-03-31 00:37:41.345951 Epoch 269, Training Loss 0.8939023303711201\n",
            "2022-03-31 00:37:51.396059 Epoch 270, Training Loss 0.8922295167165644\n",
            "2022-03-31 00:38:01.361818 Epoch 271, Training Loss 0.891935316955342\n",
            "2022-03-31 00:38:11.327164 Epoch 272, Training Loss 0.892634422044315\n",
            "2022-03-31 00:38:21.589144 Epoch 273, Training Loss 0.8911592250742266\n",
            "2022-03-31 00:38:31.636244 Epoch 274, Training Loss 0.8912556878746013\n",
            "2022-03-31 00:38:41.810104 Epoch 275, Training Loss 0.8910994762197479\n",
            "2022-03-31 00:38:51.579036 Epoch 276, Training Loss 0.8922644184373528\n",
            "2022-03-31 00:39:01.751749 Epoch 277, Training Loss 0.8903360815761644\n",
            "2022-03-31 00:39:11.941337 Epoch 278, Training Loss 0.8902863309054119\n",
            "2022-03-31 00:39:22.229755 Epoch 279, Training Loss 0.8925778808648629\n",
            "2022-03-31 00:39:32.363024 Epoch 280, Training Loss 0.8899129244220226\n",
            "2022-03-31 00:39:42.529180 Epoch 281, Training Loss 0.8905080316774071\n",
            "2022-03-31 00:39:52.623017 Epoch 282, Training Loss 0.8915369993890337\n",
            "2022-03-31 00:40:02.380967 Epoch 283, Training Loss 0.8903735719068581\n",
            "2022-03-31 00:40:12.158311 Epoch 284, Training Loss 0.8917891153746553\n",
            "2022-03-31 00:40:22.220405 Epoch 285, Training Loss 0.8895762771596689\n",
            "2022-03-31 00:40:32.541934 Epoch 286, Training Loss 0.8898698830086252\n",
            "2022-03-31 00:40:42.708815 Epoch 287, Training Loss 0.890647632844003\n",
            "2022-03-31 00:40:52.593301 Epoch 288, Training Loss 0.8907781150334936\n",
            "2022-03-31 00:41:02.545541 Epoch 289, Training Loss 0.8914753888421656\n",
            "2022-03-31 00:41:12.657908 Epoch 290, Training Loss 0.8909936108247704\n",
            "2022-03-31 00:41:22.757650 Epoch 291, Training Loss 0.8898434346289281\n",
            "2022-03-31 00:41:32.530624 Epoch 292, Training Loss 0.8918340681763866\n",
            "2022-03-31 00:41:42.507600 Epoch 293, Training Loss 0.8895071889738293\n",
            "2022-03-31 00:41:52.590089 Epoch 294, Training Loss 0.8916421886295309\n",
            "2022-03-31 00:42:02.738304 Epoch 295, Training Loss 0.8903873085670764\n",
            "2022-03-31 00:42:12.744106 Epoch 296, Training Loss 0.8906133078095858\n",
            "2022-03-31 00:42:22.850176 Epoch 297, Training Loss 0.8901986143625605\n",
            "2022-03-31 00:42:32.820286 Epoch 298, Training Loss 0.8906465310727238\n",
            "2022-03-31 00:42:42.714096 Epoch 299, Training Loss 0.8909456539336983\n",
            "2022-03-31 00:42:52.490267 Epoch 300, Training Loss 0.8896733473633867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRzYE_nQbVCR",
        "outputId": "dc0b92bd-e421-41a9-dc83-06db4e293b30"
      },
      "id": "ZRzYE_nQbVCR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.77\n",
            "Accuracy val: 0.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U73T-Fy0mA6J"
      },
      "id": "U73T-Fy0mA6J"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Homework-3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "55fda10dd22042d08466bb4ee0e516b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46ef21000ec94c45856f39dceb4b8aef",
              "IPY_MODEL_1b2c4106dd88477cbbd39ac3654de724",
              "IPY_MODEL_b33fe9614f534ef28c002a2b09ebbb78"
            ],
            "layout": "IPY_MODEL_c524a45ee05846eb9018ae78add1b0cf"
          }
        },
        "46ef21000ec94c45856f39dceb4b8aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f6544e89cbd444bbb843d26ca18a045",
            "placeholder": "​",
            "style": "IPY_MODEL_c3bca4c09b6e4ed0a2d519bea7550e42",
            "value": ""
          }
        },
        "1b2c4106dd88477cbbd39ac3654de724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_372c6a058c3a484c9af8acbbe5738862",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ce5a74ea39d4c469f54d0bfb8746023",
            "value": 170498071
          }
        },
        "b33fe9614f534ef28c002a2b09ebbb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c18b4b364924a5d913a19afa3f54413",
            "placeholder": "​",
            "style": "IPY_MODEL_e88e584e07af4187a81e1c4973d99b79",
            "value": " 170499072/? [00:03&lt;00:00, 53393772.71it/s]"
          }
        },
        "c524a45ee05846eb9018ae78add1b0cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f6544e89cbd444bbb843d26ca18a045": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3bca4c09b6e4ed0a2d519bea7550e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "372c6a058c3a484c9af8acbbe5738862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ce5a74ea39d4c469f54d0bfb8746023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c18b4b364924a5d913a19afa3f54413": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e88e584e07af4187a81e1c4973d99b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}